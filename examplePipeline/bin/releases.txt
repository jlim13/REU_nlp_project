info@plataformasinc.esAn intelligent system avoids forgetting thingsA team of researchers from the University of Granada (UGR) has created a system with Artificial Intelligence techniques which notifies elderly people or people with special needs of the forgetting of certain everyday tasks. This system uses sensors distributed in the environment in order to detect their actions and mobile devices which remind them, for example, to take their keys before they leave home.  An elderly lady is about to go to bed. She goes into her room, sits down on the bed, takes off her slippers and turns off the light. Suddenly, before getting into bed, a small alarm goes off and a mobile device reminds her that she has not taken her tablets.  This is how the new intelligent system developed by researchers from the Department of Computer Science and Artificial Intelligence of the UGR works. María Ros Izquierdo is from the Higher Technical School of Computer Engineering of the UGR and the co-author of a study which is published this month in the Expert Systems with Applications magazine. "It is a prototype which, in a non-intrusive manner, facilitates the control of the activity of people with special needs and increases their independence", she explained to SINC.The system recognizes the everyday actions of the users by means of RFID (Radio Frequency Identification) labels. These labels are discreetly placed on the objects that the individuals touch most often, in such a way that, when they do so, a signal is sent to a computer or mobile device situated in the house itself or at an assistance centre some distance away.  The activities of the people are assessed with Artificial Intelligence techniques (data mining and formal grammar) in order to compile a list of actions such as remembering to take the keys or the mobile phone before leaving home. "It is not necessary to use cameras or microphones, and the devices which are used do not entail any technological complications for users, nor do they modify their daily routines", clarified Ros.  In order to evaluate the system, the scientists have designed a Tagged World, an intelligent space which simulates the rooms of a house, with sensors embedded in the environment which help to recognize the behaviour of its occupants. The researchers monitored each user so as to obtain an individualized database. They later verified with a test the reliability of the system and the degree of intrusion felt by the participants.  "The system does not modify the life of the users, but does positively modify that of the people who look after them", indicated Ros, who recalled that elderly people or those with special needs often reject the aid of others and demand more independence. The new system may help to achieve this objective.  References:Miguel Delgado, María Ros, M. Amparo Vila. "Correct behavior identification system in a Tagged World". Expert Systems with Applications 36 (6): 9899-9906, 2009.
pressoffice@esrc.ac.ukSecurity versus privacy: How do we get the balance right?Published by the Economic and Social Research Council (ESRC), Assessing Privacy Impact summarises key presentations and open discussion involving a wide range of participants, at a special seminar organised in conjunction with the Cyber Security Knowledge Transfer Network (KTN) - the focal point for UK expertise in this area of activity.Advances in computer technology have brought fingertip access to personal information, ranging from our age and where we live, to our financial, educational, medical and other records - even our DNA.   A number of high profile and potentially serious examples of the loss of personal data with, as the booklet shows, a dramatic impact on levels of public trust. There have been a series of regulatory and legislative requirements that have been set for business and state departments, with initiatives such as the Government's 2008 Data Handling Review, mandating the carrying out of privacy impact assessments (PIAs), which help organisations assess and identify potential concerns.In Assessing Privacy Impact, Jonathan Bamford, Assistant Commissioner and Director of Data Protection Development at the Information Commissioner's Office (ICO), explains the thinking behind advice given in a new version of that body's PIA handbook.Launched in June this year, it is designed to be a practical and comprehensive guide for organisations which are developing projects that might have implications for people's privacy. Mr Bamford says that a PIA is not a regulatory assessment involving box-ticking, but "a process where you sit down at an early stage and think about what you are doing, and what you need to know."In a foreword to Assessing Privacy Impact, Professor Ian Diamond, Chief Executive Officer of the ESRC says: "Whilst guidance from the Information Commissioner's Office is a good starting point on how to make assessments and meet the requirements of legislation, a range of questions still remains for businesses, large and small - whether data holders, or vendors who consult on security and privacy or supply privacy enhancing technologies. "That is why the ESRC and the Cyber Security KTN organised a seminar to bring together academics, the ICO, consultants specialising in privacy assessments, and other organisations for which this is all very much a live issue." Tony Dyhouse, Director of the Cyber Security KTN, said: "Privacy is one of the most challenging aspects of security, because it affects everyone but is viewed in many conflicting ways. When handling sensitive data, it is vital to consider all possible issues at the outset and reach an informed decision about what data needs to be held, and the most appropriate safeguards. "The publication of the Assessing Privacy Impact booklet is an important step in helping organisations to achieve this." FOR FURTHER INFORMATION, CONTACT: ESRC Press Office:  Danielle Moore (Tel: 01793 413122, e-mail: danielle.moore@esrc.ac.uk)  Jeanine Woolley (Tel: 01793 413119, e-mail: jeanine.woolley@esrc.ac.uk) (Out of office hours number, Tel: 07554333336)NOTES FOR EDITORS:1. 'Assessing Privacy Impact', published by the ESRC, is based on presentations and open discussion at a seminar held in London in June, 2009, in collaboration with the Cyber Security Knowledge Transfer Network (KTN). If you would like to receive a free copy of the publication please email knowledgetransfer@esrc.ac.uk2. The ESRC Public Policy Seminar Series aims to bring the best social science concepts and evidence into the policy arena and stimulate a discussion of how in the light of these insights, policy can be developed. The goal is to encourage evidence-based policy through an exchange between researchers and policy-makers. For forthcoming public policy seminars, please email knowledgetransfer@esrc.ac.uk3. The Cyber Security Knowledge Transfer Network (KTN) provides a single focal point for UK Cyber Security expertise, to collaboratively identify universal challenges and develop effective response, influence UK investment strategy and Government policy, accelerate innovation and education, harness and promote UK capability internationally and help improve the UK security baseline. Funded by The Technology Strategy Board, the Cyber Security KTN works with regional development agencies, devolved administrations and the research councils. It is managed by QinetiQ, a leading international defence and security technology company. More at: http://www.cybersecurity-ktn.com/4. The Economic and Social Research Council (ESRC) is the UK's largest organisation for funding research on economic and social issues. It supports independent, high quality research which has an impact on business, the public sector and the third sector. The ESRC's planned total expenditure in 2009/10 is £204 million.  At any one time the ESRC supports over 4,000 researchers and postgraduate students in academic institutions and independent research institutes.   More at http://www.esrcsocietytoday.ac.uk  
media@rand.orgNew direction for NATO must make alliance relevant in current security environmentRAND study findsNATO is rethinking its future direction for the first time since the Sept. 11 terrorist attacks, a process that could redirect the Cold War alliance toward contemporary security issues like cyberthreats and piracy, and strengthen its commitment to fragile states like Afghanistan, according to a new RAND Corporation study.The study examines five directions the alliance might take as it revises its strategic concept to make the organization relevant in today's security environment. The directions are a refocus on Europe, a new focus on the greater Middle East, a focus on fragile states, a focus on nonstate threats and a global alliance of liberal democracies. The study also examines the possibility of combining two or more of the options."NATO will need to stay strong in Europe, but it will have to do more than that," said Christopher Chivvis, the study's author and an international security policy analyst at RAND, a nonprofit research organization. "The alliance is in a fight for its existence and will have to combine various roles to stay relevant to both Europe and the United States, but it must also be cautious not to overextend itself." From its inception in 1949, NATO has offered its members a measure of protection against their common security problems. While those security concerns have changed drastically and become much more complex over time, Chivvis says NATO's members continue to face common threats. These include hybrid threats such as terrorism, cyberthreats, piracy and environmental disasters. The alliance could be focused regionally, since most threats to allied security originate in the Middle East, or functionally toward operations such as its current effort in Afghanistan, since failed states are "the wellspring of several of the forces that threaten allied security today," Chivvis said.NATO's members face a host of common security challenges from the Middle East, making the region a logical focal point for the alliance. However, Chivvis says NATO could be an effective tool against nonstate threats if it were willing to commit the resources. Or the alliance could transform itself into an effective tool for the military aspects of coping with state failure. This is an important point when one considers the long-term repercussions if its efforts in Afghanistan should fail. The report says that NATO's revised strategic concept, a draft of which is expected in spring 2010, offers a chance to rebuild the alliance and create a viable security option for its members for at least the next 10 years to 15 years.  "The differences in strategic vision among its members will make building a common vision for the alliance extremely challenging and some combination of missions will be necessary," Chivvis said. "The optimal strategic concept will allow the organization to be flexible, yet encourage concrete commitments that will diminish further strategic drift."The study, "Recasting NATO's Strategic Concept: Possible Directions for the United States,"  can be found at www.rand.orgThe study was prepared by RAND Project AIR FORCE, a federally funded research and development center for studies and analysis aimed at providing independent policy alternatives for the U.S. Air Force.The RAND Corporation is a nonprofit research organization providing objective analysis and effective solutions that address the challenges facing the public and private sectors around the world. To sign up for RAND e-mail alerts: http://www.rand.org/publications/email.htmlRAND is a registered trademark
tilmann.sander-thoemmes@ptb.deMagnetic field measurements of the human heart at room temperatureJoint project between PTB and its US counterpart NIST might advance simple and cost effective diagnostical methodsThis release is available in German.The "magnetically best shielded room on earth" has  the size of an apartment block and is located on the site of the Physikalisch-Technische Bundesanstalt (PTB), Institute Berlin. Magnetic fields such as that of the earth are kept out here as effective as nowhere else. Such ideal conditions allow to measure the tiny magnetic fields of, e.g.,  the human heart. This was the motivation for the American National Institute of Standards and Technology (NIST) to ask PTB to jointly test a newly developed optical magnetic field sensor. It is based on a physical principle very different from SQUIDs, which are usually applied for biomagnetic field measurements. The optical sensor does not need advanced cooling and has the size of a lump of sugar. A high-quality measurement of the human heart signal was demonstrated using this optical sensor. The sensor's suitability was thus proven for biomagnetic measurements in the picotesla range. In future magnetocardiographic measurement devices  to be used as a supplement or an alternative to the ECG  could become simpler and less expensive.Up until now one had to cool as much as one could for biomagnetic measurements! This was necessary as SQUIDs, superconducting quantum interference devices, work optimally at -269 degrees Celsius and can only then fulfil their purpose of measuring tiny magnetic fields. SQUIDs are the best suited sensors to record the magnetic fields arising during the electrical activity of the human heart. A magnetocardiogram (MCG) can be compiled supplementing a conventional electrocardiogram (ECG). (The same applies to the magnetoencephalogram, MEG, which is a recording of the magnetic field of the brain.) Yet to use SQUIDs requires well-shielded rooms and complicated cooling systems. The latter might become obsolete in the future if the optical magnetometer developed by NIST continues to fulfil expectations.The optical sensor is a byproduct of the development of miniaturized atomic clocks, one of NIST's current key research areas. As biomagnetic research is not established at NIST, the scientists turned to their colleagues at PTB in Berlin. PTB is one of the few metrological state institutes in the world with a large scale biomedical research program. The combination of highly advanced equipment such as the magnetically shielded room (BMSR-2) with the experience in testing new sensors on human subjects made PTB the ideal partner for NIST.Using the optical magnetometer PTB experts measured the magnetic field of the human heart and relaxation curves of magnetic nanoparticles. Both are important routine laboratory measurements normally applying SQUIDs. For a direct comparison of the results, SQUID reference data were recorded simultaneously with the optical magnetometer using the multichannel SQUID device installed in the BMSR-2.The impressive quality of the data obtained confirms the suitability of these optical micromagnetometers in the picotesla range. This is the range for magnetic field measurements of the heart, but further improvements are needed to measure the even tinier fields of the human brain. Compared to SQUIDs, the optical sensors exhibited  as expected  a significantly higher noise level. The distinct advantage of these sensors lies, however, in their small design (Contact:Dr. Tilman Sander-Thömmes, PTB Working Group 8.21 Biomagnetism,Tel.: +49 30 3481-7436, e-mail: tilmann.sander-thoemmes@ptb.de
weipeng@exploit-tech.comA*STAR showcases 12 breakthrough infocomm technologies at CommunicAsia 2009Look out for a technology that enables you to control who can view/edit your pictures after someone has downloaded them off, say, your website or social networking page. Also check out a smart image recognition software that can tell you what an image "means". And not to be missed is a smart search technology, which will give users more than just relevant hits. Stop by A*STAR CommunicAsia 2009 booth and experience how infocomm technologies can enhance your lifestyle! Twelve novel ready-for-market technologies, which will improve the quality of life for users in industry verticals such as lifestyle, security, communications and healthcare, have been developed by A*STAR's (Agency for Science, Technology and Research) Institute for Infocomm Research (I2R).  They will be showcased at the upcoming CommunicAsia 2009, from 16 to 19 June 2009 at Singapore Expo Hall 3, Booth 3E2-01.More notable among the group of I2R technologies are KnowleSuite, Snap2Tell / Snap2Play and SUSie (Share & UnShare, it's easy). These technologies seek to alleviate some of the pressing IT issues today that are faced by millions around like world, including professionals, avid social media fans and even tourists.KnowleSuite is a toolbox consisting of applications for automated knowledge search, capture and aggregation. Targeted at knowledge-intensive workers, the suite automates the process of knowledge acquisition and allows users to spend more time in value-adding analysis of the information than its acquisition. Snap2Tell technology is an Image Recognition Engine that recognises images of scenes / objects without the need of labelling or putting barcode on the Point of Interest (POI); and associates it with relevant information. Its small code footprint enables this technology to be implemented into mobile devices, from ultra-mobile computers (UMPC) to regular mobile phones with camera. Snap2Play is an interactive mixed-reality mobile application. It employs Snap2Tell's Image Recognition Engine as well as multi-input modalities (i.e. GPS and accelerometer sensors) to enhance the immersive mobile application. This technology will be well placed to serve the tourism industry, where tourists can simply "snap" pictures with their camera phone and get information on various places of interest.SUSie (Share & UnShare, it's easy) is an efficient de-centralised security key infrastructure technology that is specifically designed to enable users to share and unshare media content (e.g. pictures, video & audio) over social media sites. Sharing refers to enabling content recipients to read and store (encrypted) the media files; while unsharing refers to a complete revocation of media content  even if the recipients have downloaded a copy on their local drives.The other ready-for-market technologies by I2R, including Brain-Computer Interface, Gender Recognition System, iTwin, MobiCare, MPEG-4 SLS, Niometrics, A*STAR Scalable Multimedia Platform, TV White Space sensor and Ultra-Wide Band, will be exhibited at the show. Commercialisation officers from Exploit Technologies will be on hand to discuss licensing and co-development opportunities of the technologies.Additionally, four companies, which are either spin-offs from A*STAR or licensees of A*STAR technologies, will be showcasing their products and services at the A*STAR booth. As part this year's outreach efforts, a CommunicAsia blog has been set up to share with readers about A*STAR technologies and daily exciting developments at the booth.Said Mr Lim Chuan Poh, Chairman of A*STAR: "CommunicAsia 2009 showcases many innovative products that continue to improve our quality of life and productivity.  These innovations are only possible with world class scientific and engineering talent doing cutting edge research and design development working in close collaboration with domain experts to solve real problems.  At the Fusionopolis, Biopolis and beyond, A*STAR has over 2000 scientists who work with both the public sector agencies and nearly 300 companies from the private sector to bring about impactful innovations.  Some of these can be seen in this exhibition in the lifestyle, security, communications and healthcare sectors."On I2R's presence in CommunicAsia 2009, Prof Lye Kin Mun, Deputy Executive Director (Research) added, "The practical solutions from our R&D efforts that will be showcased here could enhance the way we live in the future.  In addition, the myriad of our new Infocomm and media technologies at the exhibition seeks to improve the quality of lifestyles through better healthcare, improved connectivity and information security for the infocomm world."Mr Boon Swan Foo, Executive Chairman of Exploit Technologies said, "Visitors to our CommunicAsia booth can expect to see a robust pool of A*STAR's ready-for-market infocomm technologies, in the areas of lifestyle, security and communications and healthcare, which companies from multiple industry verticals can tap on to in turn create innovative products and services. "We have been actively engaging the industry to leverage A*STAR technologies to provide a competitive edge to their products and services. The use of social media applications like a blog and mobile phone applications this year demonstrates our commitment to reach out to more potential partners through various avenues. To date, we have granted close to 300 licences for our technologies; with more than 70% of our licences going to local SMEs. I am happy that some of these SMEs, like Aksaas, CommonTown, FeRmi and Compex, will be exhibiting at our booth this year. We urge more industry players to partner us to jointly develop A*STAR technologies for use in their businesses. The current economic downturn has in fact provided a good window of opportunity for companies to invest in their IT capabilities, so as to ride the upward economic swing when it happens over the next two years."The pool of ready-for-market technologies can be categorised into four main areas, namely:a.	Lifestyleb.	Securityc.	Communicationsd.	HealthcareAll media representatives and your photographer/film crews are cordially invited to visit A*STAR's booth at CommunicAsia 2009, from 16 to 19 June 2009at Singapore Expo Hall 3, Booth 3E-01. Do also visit the CommunicAsia blog at exploit-tech.webs.com for the latest updates on A*STAR's participation in CommunicAsia 2009.Enc:		Annex A:	Short notes to editors on the technologies to be showcasedAnnex B:	Catalogue of technologies available for licensingAGENCY OF SCIENCE, TECHNOLOGY AND RESEARCH (A*STAR)For media enquiries, please contact:Seeto Wei PengExploit Technologies Pte Ltd (A member of A*STAR)Vice President, Corporate Marketing and Communications DID: (65) 6478 8443   Mobile: (65) 8375 9474Email: weipeng@exploit-tech.com Andrew YapActing Manager, Corporate CommunicationsInstitute for Infocomm Research (I2R)DID: (65) 6874 8003    Fax: (65) 6775 9923Email:  yapjt@scei.a-star.edu.sg  About Exploit Technologies Pte LtdExploit Technologies is the strategic marketing and commercialisation arm of the Agency for Science, Technology and Research (A*STAR). Its mission is to support A*STAR in transforming the economy through commercialising R&D. Exploit Technologies enhances the research output of A*STAR scientists by translating their inventions into marketable products or processes. Through licensing deals and spin-offs with industry partners, Exploit Technologies is a key driver of technology transfer in Singapore. It actively engages industry leaders and players to commercialise A*STAR's technologies and capabilities, bridging the gap from Mind to Market. Exploit Technologies' charter is to identify, protect and exploit promising intellectual property (IP) created by A*STAR's research institutes. For more information, please visit http://www.exploit-tech.com    About Institute for Infocomm ResearchThe Institute for Infocomm Research (I²R pronounced as i-squared-r) is a member of the Agency for Science, Technology and Research (A*STAR) family. Established in 2002, our mission is to be the globally preferred source of innovations in `Interactive Secured Information, Content and Services Anytime Anywhere' through research by passionate people dedicated to Singapore's economic success.  I²R performs R&D in information, communications and media (ICM) technologies to develop holistic solutions across the ICM value chain. Website: www.i2r.a-star.edu.sg ANNEX A: Short notes to editors on the companies and technologies to be showcasedPART 1: COMPANIES1.	CommonTown Pte LtdA provider of e-learning, mobile applications and portal development services that is developing new products in language e-learning with speech processing technologies from A*STAR.  2.	Aksaas Pte LtdAksaas is provider of Software as a Service (SaaS) with leading-edge office automation, knowledge management and document security applications developed with A*STAR's text analytics and information security technologies.3.	FeRmi Pte LtdA fabless semiconductor company that provides integrated RF reader ICs and reference solutions for low cost mobile RFID systems. FeRmi adds value to its partners in the RF reader hardware market segment and enables leading edge solutions to customers for portable applications in various industry segments such as healthcare, pharmaceuticals, logistics, security and other market segments that it serves.4.	Compex Pte LtdCompex specializes in designing on wireless radio, embedded board, antenna and enclosed for OEM/ODM customers. Compex is primarily focusing in OEM/ODM supply of network communication systems and broadband wireless systems (including but not limited to hotspot/hotzone access management systems, outdoor systems for wireless infrastructures, subscriber units and indoor mesh systems) via board-level supplies and complete product builds.PART 2: TECHNOLOGIES1.	Brain-Computer Interface (BCI)Brain-computer interface (BCI) provides a direct communication and interaction channel between the human brain and the computer. I2R focuses on research and development of non-invasive BCI systems using brain signals from the scalp or electroencephalography (EEG), to create novel medical solutions in assistive devices for the physical disabled, neuro-rehabilitation for stroke patients, and attention training for people diagnosed with neurophysiologic disorders.2.	Gender Recognition System This system has the ability to detect frontal human faces on digital video stream quickly and reliably. Based on the extracted face images, the engine is able to simultaneously detects the genders of multiple persons. In addition to that, it can return the statistic of the viewers such as the number of person looking at the screen. In summary, this technology provides the means to measure audience demographics which could be helpful for the advertising industry.3.	iTwinUsing iTwin is like using a physical cable: Plug one Twin into first PC and the other Twin into the second PC and you are ready to go.  Thus, it is like the 2 ends of a cable without the cable  an infinitely long cable. iTwin requires no downloading, installation or configuration of software. The Twin auto-loads the required software and self-configures itself, making file access extremely simple.4.	KnowleSuiteKnowleSuite is a toolbox consisting of applications for automated knowledge search, capture and aggregation. Targeted at knowledge-intensive workers, the suite automates the process of knowledge acquisition and allows users to spend more time in value-adding analysis of the information than its acquisition.5.	MobiCareMobiCare is a wireless sensor used to capture ECG and activity signals.  Handheld devices such as PDA or cell phone are used to process the capture ECG and activity data.  High accuracy and reliable feature extraction and classification methods as well as optimized algorithm are developed and embedded into the mobile devices to automatically process the data. It transmits critical data to hospital when it detects cardiac abnormality. In hospitals, physicians will verify and diagnose if the patient is at high or low risk by examining the received data.  Based on their diagnosis, an ambulance can be dispatched to bring in the patients at high risk; an message can be dispatched informing the patients at low risk to come in for a thorough screening.6.	MPEG-4 SLSAs a lossless audio coding technology, MPEG-4 SLS provides state-of-the-art lossless compression performance for audio signals with sampling rate up to 192 kHz and amplitude resolution up to 24 bits/sample.  MPEG-4 SLS also provides backward compatibility by embedding a MPEG Advanced Audio Coding (AAC) bitstream, a widely adopted lossy audio coder, and Fine Granular Scalability (FGS) from its AAC core data rate all the way to lossless for applications with various Quality of Service (QoS) requirements or varying network conditions. With FGS, "additional compression" is obtained by simply truncating the bit-stream or compressed file without additional encoding or transcoding.7.	NiometricsThe Network Application Fingerprinting technology enables the detection and fingerprinting of network applications including encrypted BitTorrent, Skype and eMule. It can be used to provide Service Providers with visibility of subscriber and application usage patterns as well as malicious traffic on the net.8.	A*STAR Scalable Multimedia PlatformThe A*STAR's Scalable Multimedia Platform (A*SMP) is poised to be among the world's first scalable multimedia platforms that exploits that latest ISO/IEC multimedia codecs / networking and W3C web standards. It revolutionises the way multimedia content is encoded, stored, managed and delivered by content owners and service providers. It also offers a whole new seamlessly connected multimedia experience to consumers, anytime, anywhere and across any device.   A*SMP natively supports state-of-the-art ISO/IEC multimedia codec standards including H.264/MPEG-4 AVC, H.264/SVC, MPEG-4 SLS and AAC.  With a single scalable compressed multimedia file on the server, the platform can now simultaneously serve heterogeneous users across HDTV, PC, mobile phone, and over broadband, wireless and 3G networks.9.	Snap2Tell / Snap2PlaySnap2Tell technology is an Image Recognition Engine developed at I2R. Having this capability, we can recognize images of scenes / objects without the need of labeling or putting barcode on the Point of Interest (POI), and then associate it with relevant information. Its small code footprint enables this technology to be implemented into mobile devices, from ultra-mobile computers (UMPC) to regular mobile phones with camera.Snap2Play is an interactive mixed-reality mobile application. It employs Snap2Tell's Image Recognition Engine as well as multi-input modalities (i.e. GPS and accelerometer sensors) to enhance the immersive mobile application.10.	SUSieSUSie (Share & UnShare, it's easy) is an efficient de-centralized security key infrastructure technology specifically designed to enable software applications that require sharing and unsharing of media content (e.g. pictures, video & audo). Sharing refers to enabling content recipients to read and storage (encrypted) while unsharing refers to complete revocation of media content inadvertently read or exposed to the recipients.11.	TV White Space SensorTV White Space refers to frequencies allocated to a broadcasting service but not utilised in the local spatial context. It has been touted as an untapped resource that has the potential to unleashing a myriad of applications and services, given its attractive propagation characteristics and relatively large frequency blocks.12.	Ultra-Wide BandThe Ultra-Wide Band (UWB) Localisation system can be used to track the position of a moving object with centimeter level accuracy in an indoor environment. It makes use of pulse-based UWB technology. Each tracked item carries a UWB tag. The position of the Tag is monitored by stationary reference. Readers distributed around the tracked zone. By using both Range and Angle-of-Arrival (AOA) information, the position of a Tag can be located using just a single reference Reader.ANNEX B: Catalogue of technologies available for licensing
alexb@intidyn.comResearchers finds hidden sensory system in the skinPersons lacking known nerve receptors can still touch and feel; may shed light on causes of unexplained pain such as fibromyalgiaDecember 15, 2009  (Albany, N.Y., USA)  The human sensory experience is far more complex and nuanced than previously thought, according to a groundbreaking new study published in the December 15 issue of the journal Pain (http://www.painjournalonline.com/article/S0304-3959%2809%2900526-0/abstract).  In the article, researchers at Albany Medical College, the University of Liverpool and Cambridge University report that the human body has an entirely unique and separate sensory system aside from the nerves that give most of us the ability to touch and feel.  Surprisingly, this sensory network is located throughout our blood vessels and sweat glands, and is for most people, largely imperceptible."It's almost like hearing the subtle sound of a single instrument in the midst of a symphony," said senior author Frank Rice, PhD, a Neuroscience Professor at Albany Medical College (AMC), who is a leading authority on the nerve supply to the skin.  "It is only when we shift focus away from the nerve endings associated with normal skin sensation that we can appreciate the sensation hidden in the background."  The research team discovered this hidden sensory system by studying two unique patients who were diagnosed with a previously unknown abnormality by lead author David Bowsher, M.D., Honorary Senior Research Fellow at the University of Liverpool's Pain Research Institute.  These patients had an extremely rare condition called congenital insensitivity to pain, meaning that they were born with very little ability to feel pain.  Other rare individuals with this condition have excessively dry skin, often mutilate themselves accidentally and usually have severe mental handicaps.  "Although they had a few accidents over their lifetimes, what made these two patients unique was that they led normal lives.  Excessive sweating brought them to the clinic, where we discovered their severe lack of pain sensation," said Dr. Bowsher.  "Curiously, our conventional tests with sensitive instruments revealed that all their skin sensation was severely impaired, including their response to different temperatures and mechanical contact.  But, for all intents and purposes, they had adequate sensation for daily living and could tell what is warm and cold, what is touching them, and what is rough and smooth."The mystery deepened when Dr. Bowsher sent skin biopsies across the ocean to Dr. Rice's laboratory, which focuses on multi-molecular microscopic analyses of nerve endings in the skin, especially in relation to chronic pain conditions such as those caused by nerve injuries, diabetes, and shingles.  These unique analyses were pioneered by Dr. Rice at Albany Medical College (AMC) along with collaborators at the Karolinska Institute in Stockholm, Sweden.  "Under normal conditions, the skin contains many different types of nerve endings that distinguish between different temperatures, different types of mechanical contact such as vibrations from a cell phone and movement of hairs, and, importantly, painful stimuli," said Dr. Rice.  "Much to our surprise, the skin we received from England lacked all the nerve endings that we normally associated with skin sensation.  So how were these individuals feeling anything?"  The answer appeared to be in the presence of sensory nerve endings on the small blood vessels and sweat glands embedded in the skin.  "For many years, my colleagues and I have detected different types of nerve endings on tiny blood vessels and sweat glands, which we assumed were simply regulating blood flow and sweating. We didn't think they could contribute to conscious sensation.  However, while all the other sensory endings were missing in this unusual skin, the blood vessels and sweat glands still had the normal types of nerve endings.  Apparently, these unique individuals are able to 'feel things' through these remaining nerve endings," said Dr. Rice.  "What we learned from these unusual individuals is that there's another level of sensory feedback that can give us conscious tactile information.  Problems with these nerve endings may contribute to mysterious pain conditions such as migraine headaches and fibromyalgia, the sources of which are still unknown, making them very difficult to treat."  In addition to international collaborations such as this one, Dr. Rice and his principle AMC colleague, Dr. Philip Albrecht, in the Center for Neuropharmacology and Neuroscience, collaborate extensively with neurologists Dr. Charles Argoff at AMC and Dr. James Wymer of Upstate Clinical Research Associates, who also holds a joint AMC appointment.  All are co-authors on the study, which included normal subjects from the Albany, N.Y. area.  Several studies on chronic pain are being conducted by this team with support from National Institutes of Heath (NIH) and several pharmaceutical companies. About Integrated Tissue Dynamics (INTIDYN)To facilitate these collaborations, Dr. Rice and Dr. Albrecht, recently founded a new biotechnology company, Integrated Tissue Dynamics, LLC, also known as Intidyn (www.Intidyn.com).  Intidyn provides flexible and scalable research capabilities on behalf of pharmaceutical companies to detect chemical and structural changes in the skin that may cause the chronic numbness, pain and itch associated with a wide variety of afflictions such as diabetes, shingles, complex regional pain syndrome, carpal tunnel syndrome, sciatica, fibromyalgia, psoriasis, chemotherapy and even the unintended side effects caused by many drugs.  Such afflictions and the associated neurological problems respond poorly to existing treatments. The preclinical and clinical research conducted by AMC and Intidyn facilitates biomarker identification and the evaluation of potential therapeutic strategies to prevent or treat these naturally-occurring afflictions and drug-induced side effects that harm the skin and nerves.  "By looking carefully at genomics and the structural and chemical differences between normal and diseased skin, we can better determine if a treatment is working or if it's even targeting the right problem," said Dr. Rice.  "For example, in cases of 'unexplained' pain that's unresponsive to conventional treatment, it's important to know if nerve receptors in the vascular and sweat gland tissue are involved, and if so, whether a given treatment is targeting those nerves.  We can also see if a pain treatment is damaging vascular tissue, for example, and make inferences about what the impact of that damage might mean clinically."Most recently, Intidyn has partnered with neurologists and fellow co-authors, Drs. Argoff and  Wymer to study a mysterious condition called fibromyalgia.  They suspect the unrelenting pain may be related to the sensory nerve endings on blood vessels deep in the skin.   About Albany Medical CollegeAt Albany Medical College, one of the nation's oldest medical schools, basic research scientists work to facilitate discoveries that translate into medical innovations at patients' bedsides. NIH-funded scientists are conducting research in many exciting areas including infectious disease, biodefense, addiction, cancer, pain, and more.  Albany Medical Center is northeastern New York's only academic health sciences center. It consists of Albany Medical College, Albany Medical Center Hospital; and the Albany Medical Center Foundation, Inc.  Additional information about Albany Medical Center can be found at www.amc.edu.Citation: Bowsher D, Geoffrey Woods C, Nicholas AK, Carvalho OM, Haggett CE, Tedman B, Mackenzie JM, Crooks D, Mahmood N, Aidan Twomey J, Hann S, Jones D, Wymer JP, Albrecht PJ, Argoff CE, Rice FL.  Absence of pain with hyperhidrosis: A new syndrome where vascular afferents may mediate cutaneous sensation. PAIN. 2009 Dec 15;147(1-3):287-98.
frances.white@pnl.govA flash of light turns graphene into a biosensorDisease diagnosis, toxin detection and more are possible with DNA-graphene nanostructureBiomedical researchers suspect graphene, a novel nanomaterial made of sheets of single carbon atoms, would be useful in a variety of applications. But no one had studied the interaction between graphene and DNA, the building block of all living things. To learn more, PNNL's Zhiwen Tang, Yuehe Lin and colleagues from both PNNL and Princeton University built nanostructures of graphene and DNA. They attached a fluorescent molecule to the DNA to track the interaction. Tests showed that the fluorescence dimmed significantly when single-stranded DNA rested on graphene, but that double-stranded DNA only darkened slightly  an indication that single-stranded DNA had a stronger interaction with graphene than its double-stranded cousin. The researchers then examined whether they could take advantage of the difference in fluorescence and binding. When they added complementary DNA to single-stranded DNA-graphene structures, they found the fluorescence glowed anew. This suggested the two DNAs intertwined and left the graphene surface as a new molecule. DNA's ability to turns its fluorescent light switch on and off when near graphene could be used to create a biosensor, the researchers propose. Possible applications for a DNA-graphene biosensor include diagnosing diseases like cancer, detecting toxins in tainted food and detecting pathogens from biological weapons. Other tests also revealed that single-stranded DNA attached to graphene was less prone to being broken down by enzymes, which makes graphene-DNA structures especially stable. This could lead to drug delivery for gene therapy. Tang will discuss this research and some of its possible applications in medicine, food safety and biodefense.	Reference: Zhiwen Tang, Biofunctionalization of Graphene for Biosensing and Imaging, Tuesday, September 22, 3:30  5:30 p.m. in Ross Island/Morrison at the Doubletree Lloyd Center, Portland, Ore.	TIPSHEET: Scientists from the Department of Energy's Pacific Northwest National Laboratory will be presenting their research at the 2009 Micro Nano Breakthrough Conference, which runs Sept. 21-23 in Portland, Ore. http://oregonstate.edu/conferences/MNBC/.This research was funded by PNNL as part of its Transformational Materials Science Initiative, http://materials.pnl.gov/.Pacific Northwest National Laboratory is a Department of Energy Office of Science national laboratory where interdisciplinary teams advance science and technology and deliver solutions to America's most intractable problems in energy, national security and the environment. PNNL employs 4,200 staff and has an $850 million annual budget. Ohio-based Battelle has managed PNNL since the lab's inception in 1965.
dguerin@latech.eduLouisiana Tech receives $1.8M in grants for nanosystems, energy researchLouisiana Tech University's College of Engineering and Science has been awarded a $1.4 million grant from the U.S. Department of Energy (DOE), while Dr. Long Que, an assistant professor of electrical engineering, has received a $400,000 National Science Foundation (NSF) CAREER award.The DOE grant will equip and support research in engineered systems to meet U.S. energy needs, using bio- , nano-, and geo-derived technologies, enabling Louisiana Tech to support several applied and fundamental research projects during 2009-10.DOE funds will also be used to support faculty, students, operating expenses, and new equipment for related research.  Projects to be funded will contribute to carbon capture, nuclear energy, renewable fuels, electrical energy storage, and energy harvesting."The proposed research projects will support the Department of Energy's overarching mission to advance the national, economic, and energy security of the United States, and to promote scientific and technological innovation in support of that mission," says Dr. Stan Napper, dean of Louisiana Tech's College of Engineering and Science."In particular, the projects at Louisiana Tech will assist DOE in its strategic goals of providing energy security, scientific discovery and innovation, and environmental responsibility."Que's NSF CAREER grant funds a project titled "Biomolecular Nanophotonic Fabry-Perot Interferometry (BioNanoFPI)."  This project will result in devices with nanostructures integrated with micro and nano networks to provide real-time, point-of-care monitoring.  These devices will require only very small volumes of sample fluids, offering a significant contribution for high throughput drug screening and pathogen detection for the pharmaceutical industry.	The CAREER award is NSF's most prestigious, supporting junior faculty who exemplify the role of teacher-scholars through outstanding research, excellent education and the integration of education and research within the context of the mission of their organizations.Que joined Louisiana Tech University in 2007, after a career in industry, including two years as a project and task leader at the G.E. Global Research Center.The CAREER award will help to fund research involving students in the fabrication, characterization and testing of these devices, using laboratory facilities at Louisiana Tech's Institute for Micromanufacturing.Louisiana Tech's College of Engineering and Science is a nationally recognized leader in educational innovation whose goal is to become "the best college in the world at integrating engineering and science in education and research." The Institute for Micromanufacturing (IfM) is an integrated nanomanufacturing and micromanufacturing center, dedicated to micro/nano scale technologies and systems research, education, and commercialization.
akoyama@yz.yamagata-u.ac.jpUbiquitous healthEnabling telemedicine to cut hospital visits, save moneyA ubiquitous health monitoring system that automatically alerted the patient's family or physician to problematic changes in the person's vital signs could cut hospital visits and save lives, according to Japanese researchers writing in the International Journal of Web and Grid Services.Akio Koyama of Yamagata University and colleagues there and at Yamagata College of Industry and Technology and the Fukuoka Institute of Technology explain that the population of the developed world is growing older, medical costs are rising, and there are not enough doctors to heal the elderly sick.One solution might be to reduce the incidence of illness that requires a hospitalisation by providing those at risk with a remote monitoring device. The team is developing a wearable vital sensor that might be worn like an emergency call device familiar to many elderly people and their families. Indeed, the team has designed the device to be used anywhere without disrupting the everyday life of the patient.The vital monitor would keep check on specific facets of the patient's health. In the development device, temperature, pulse, and waist size are monitored. The data is transmitted through the cellular telephone network and on to a web database that is accessible via a browser and flags up any problems for the patient's family or doctor and sends an emergency alert if necessary.Body temperature is a useful indicator of overall patient health, significant deviation from the norm usually indicates a serious illness. The pulse sensor can detect arrhythmias in the heart by measuring the shape of the waves and the pulse rate. The waist sensor is associated with more long-term monitoring of the patient, allowing the doctor to automatically keep track of whether the patient is gaining or losing weight significantly.The team has not only developed the appropriate sensors but has also outlined a data transmission protocol that could use the cellular network efficiently. They are currently extending the concept and developing a remote sensor for metabolic syndrome/diabetes."Design and implementation of a ubiquitous health monitoring system" in Int. J. Web and Grid Services, 2009, 5, 339-355
mark.esser@nist.govNew NIST trace explosives standard slated for homeland security dutySecurity personnel need to be able to find explosive materials and persons who have been in contact with them. To aid such searches, the National Institute of Standards and Technology (NIST), with support from the Department of Homeland Security, has developed a new certified reference material, Standard Reference Material (SRM) 2905, Trace Particulate Explosives. Compatible with field and laboratory assay methods, the SRM will be helpful in calibrating, testing and developing standard best operating procedures for trace-explosives detectors.Most air travelers have probably had some experience with prototype walkthrough portal or tabletop-type trace explosive detectors. Customs inspectors use the machines to check international cargo shipments, and firefighters and police officers use them to evaluate suspicious packages.The goal of these detectors is to effectively collect residue particles that result from handling materials that might be used to fabricate a bomb and then evaluate the explosives content. For example, when operating the tabletop device, security personnel use a piece of material to swab packages and bags for explosive residues. The security officer then places the swab in a tabletop device that heats the material, separating any chemical residues that may have been absorbed.Like other sensitive instruments, these machines need well-defined calibration standards to ensure that they are working properly. According to NIST chemist William MacCrehan, the calibration materials that the vendors of these machines provide are typically of unknown quality."These detectors need to be reliable and precise enough to detect particles that weigh as little as a few billionths of a gram," says MacCrehan. "We created this SRM to provide manufacturers and operators with high quality, independently generated and validated reference test materials to enable better designs and reduce the number of false positives and negatives."SRM 2905 consists of four different test substances designed to simulate trace residues of C-4 plastic explosives and TNT. The substances themselves consist of inert solid particles about 20 to 30 microns in diameter. The particles have been coated with explosive materials and a florescent tag, which enables the material to be seen using specially filtered optics or glasses. Although the particles are coated with explosive material, MacCrehan says they are incapable of exploding on their own and are completely safe to handle.This release is part of a larger, ongoing project to develop other wet and dry materials that simulate SEMTEX, gunpowder and peroxide-type explosives. According to MacCrehan, efforts also are underway to develop reference materials to help train bomb-sniffing dogs.
info@plataformasinc.esVisual system that detects movement, colors and textures created in GranadaMimicking the way in which a retina works is a hard as it sounds. Scientists from Stanford University, in the United States, have spent the past two years working on imitating the way in which information is processed in biological systems, in other words through the transmission of events in specifically connected networks (where information is captured and transmitted at the same time). Now a research team from the UGR has evaluated the degree of precision of different models in estimating movement, and have combined the responses of four movement detection cells, two of which are static (on and off), and two transitory (decrease and increase). "One of our developments is a multimodal attention operator, which can detect movement in objects of different colours and textures", Fran Barranco, one of the researchers involved in this project, tells SINC. The objective of this study, which has been published in the latest issue of the journal IEEE Transactions on Systems Man and Cybernetics, was to combine movement and attention based on information provided by the artificial retina, a visual system capable of selectively capturing moving objects in real time.The use of an event-driven model, which makes it possible to focus only on areas of activity, has been fundamental, both in the movement processing model as well as in the multimodal selective attention model created in Granada. One of the most interesting results of the study is the ability to estimate movement reasonably precisely using the responses from each of these cells alone (4% of the information provided by a camera). "By selecting only 10-20% of the information, which we selected on the basis of reliability of the measurements, we obtained precise results at a lower computational cost and with greater stability", explains Barranco. This point is very important in enabling the model to be used in applications with broadband limitations. The Spanish researchers have also developed 'advanced integrated intelligent sensors', which can pre-process a scene in a manner similar to that used by retinas. A science fiction future"We are carrying out reverse engineering. In other words we are trying to study how Nature behaves in order to imitate it, because thousands of years of evolution have created a highly-advanced model adapted to the task for which it evolved", says Barranco.The devices created have been designed for use in video surveillance and monitoring applications. However, their low energy consumption could make them of great interest in the future for implants in patients or in work to understand the functioning of the brain, and particularly the visual system. References:Francisco Barranco; Javier Díaz, Eduardo Ros, Begoña del Pino. 'Visual System Based on Artificial Retina for Motion Detection'. IEEE Transaction on Systems Man and Cybernetics Part-B Cybertenics. Volumen 39 (3), páginas 752-762. Junio de 2009.
lisa.newbern@emory.eduYerkes researchers identify parallel mechanism monkeys and humans use to recognize facesThis study with rhesus monkeys suggests the human ability to distinguish faces is 30+ million years oldResearchers at the Yerkes National Primate Research Center, Emory University, have demonstrated for the first time rhesus monkeys and humans share a specific perceptual mechanism, configural perception, for discriminating among the numerous faces they encounter daily. The study, reported in the June 25 online issue of Current Biology, provides insight into the evolution of the critical human social skill of facial recognition, which enables us to form relationships and interact appropriately with others."Humans and other social primates need to recognize other individuals and to discriminate kin from non-kin, friend from foe and allies from antagonists," said lead researcher Robert R. Hampton of the Yerkes National Primate Research Center and Emory's Department of Psychology. "Our research indicates the ability to perform this skill probably evolved some 30 million or more years ago in an ancestor humans share with rhesus monkeys." The remarkable capability humans have to distinguish among thousands of faces stems from our sensitivity to the unique configuration, or layout, of facial features. "Because faces share so many features in common  eyes, nose, mouth, etc.  the simple detection of the collection of these features alone would not permit us to tell many faces apart," Dr. Hampton noted. "It's our ability to perceive small changes in the relations among the features that enables us to distinguish thousands of faces and recognize those we know," he explained.Hampton and his research team used the Thatcher Effect, a perceptual illusion named for Margaret Thatcher because it was first demonstrated using an image of the former British prime minister, to determine if rhesus monkeys use configural perception to recognize other monkeys. In the study, the researchers presented images of six different monkeys to four 4-year-old rhesus macaque monkeys raised for two to three years in large social groups at the Yerkes Research Center. The researchers "thatcherized" the images of faces by positioning the eyes and mouths upside down relative to the rest of each face. The researchers presented monkeys with normal images of each face upside down and right side up until the monkeys were bored and ceased looking at the pictures. The researchers then showed the monkeys the thatcherized faces. In the upright position, the monkeys were surprised by the distorted features and began looking at the pictures again. In contrast, when the faces were upside down, they were not at all surprised and treated the faces as if nothing had been done to them. This is similar to the human response to the Thatcher Effect, which shows that when the eyes and mouth are rotated and, thus, distorted, humans surprisingly process the upside-down version of the image more as a collection of features and with less emphasis on the relations among the features. As a result, the face appears fairly normal despite being thatcherized. However, when viewed right side up, humans say the image looks awkward or grotesque, demonstrating they clearly see the eyes and mouth have been rotated. "This study advances our understanding of social processes critical for a healthy and successful social life in primates. Early primates apparently solved the problem of recognizing each others' faces in this way well before humans arrived on the planet," Dr. Hampton concluded. For nearly eight decades, the Yerkes National Primate Research Center, Emory University, has been dedicated to conducting essential basic science and translational research to advance scientific understanding and improve the health and well-being of humans and nonhuman primates. Today, the center, as one of only eight National Institutes of Health-funded national primate research centers, provides leadership, training and resources to foster scientific creativity, collaboration and discoveries. Yerkes-based research is grounded in scientific integrity, expert knowledge, respect for colleagues, and open exchange of ideas and compassionate quality animal care.Within the fields of microbiology and immunology, neuroscience, psychobiology and sensory-motor systems, the center's research programs are seeking ways to: develop vaccines for infectious and noninfectious diseases, such as AIDS and Alzheimer's disease; treat cocaine addiction; interpret brain activity through imaging; increase understanding of progressive illnesses such as Parkinson's and Alzheimer's; unlock the secrets of memory; determine behavioral effects of hormone replacement therapy; address vision disorders; and advance knowledge about the evolutionary links between biology and behavior.	Note: Prior to the embargo lifting, reporters may obtain a copy of the study from Cell Press. Please contact Cathleen Genova at cgenova@cell.com or 617-397-2802. It is also available under embargo on EurekAlert.For a photo demonstrating the Thatcher Effect, please contact Lisa Newbern at lisa.newbern@emory.edu or 404-727-7709. 
s.g.j.mathijssen@tue.nlMysterious charge transport in self-assembled monolayer transistors unraveledAn international team of researchers from the Netherlands, Russia and Austria discovered that monolayer coverage and channel length set the mobility in self-assembled monolayer field-effect transistors (SAMFETs). This opens the door to extremely sensitive chemical sensors that can be produced in a cost-effective way. The research was done at Philips Research Eindhoven and Eindhoven University of Technology. The findings were published as an Advanced Online Publication in Nature Nanotechnology. SAMFETsThe SAMFET is a recent example of the development of 'plastic micro-electronics'- i.e. electronics based on organic materials. Last year, Philips Research managed to build such a transistor by immersing a silicon substrate into solution containing liquid crystalline molecules that self-assemble onto this substrate, resulting in a semi-conductive layer of just a single molecule thick. The monolayer of the SAMFET consists of molecules that are standing upright. Conduction takes place by charges jumping from one molecule to the other. However, in previous attempts to make a SAMFET, it was observed that as the length of the SAMFET increased, its level of conductivity counterintuitively decreased exponentially. In a joint project Philips Research, the Eindhoven University of Technology (TU/e), the University of Groningen, the Holst Centre, the Enikolopov Institute for Synthetical Polymer Materials in Moscow and the Technical University in Graz, Austria discovered that this decrease is determined by the monolayer coverage, which could be explained with a widely applicable two-dimensional percolation model.  The ultimate chemical sensorOne could compare this to crossing a river by jumping from rock to rock. The closer the rocks are to each other, the quicker one can jump or even walk to the other river bank. So if the monolayer displays more voids, the conductivity decreases dramatically. Up till now, this behavior was an uncharted area and inhibited the use of SAMFETs in applications such as sensors and plastic electronics. The SAMFET's extreme sensitivity could open doors to the development of the ultimate chemical sensor, the research team points out. "If we go back to that river again, another benefit of a SAMFET becomes clear", Martijn Kemerink, assistant professor at the TU/e indicates. "Imagine that there are just enough rocks to cross that river. When you remove just one rock, the effect is significant, for it is impossible to make it to the other side of the river. The SAMFET could be used to make sensors that give a large signal that is triggered by a small change", he continues.  Future stepsAt present, SAMFETs are not widely used, for there are alternatives of which the production process is well-established. However, the production process of SAMFETs is extremely simple and material efficient. The transistor requires only a single layer of molecules that is applied by simple immersion into a chemical solution. The same solution can be used for many substrates, for the substrate only takes the necessary (small) amount of molecules. This makes future large-scale production of monolayer electronics efficient, simple and cost-effective.PublicationThe publication "Monolayer coverage and channel length set the mobility in self-assembled monolayer field-effect transistors", by Matthijssen et al. can be found at http://www.nature.com/nnano/journal/vaop/ncurrent/abs/nnano.2009.201.html. The research was conducted at Philips Research Eindhoven and Eindhoven University of Technology. It was funded by STW, ONE-P, the Austrian Nanoinitiative and H.C. Starck GmbH.
J.W.Niemantsverdriet@tue.nlEuropean top universities join forces in energy researchDTU, TUM and TU/e start graduate school on sustainable energy technologiesThe focus on Sustainable Energy Technologies was chosen for multiple reasons: to initiate a research and innovation driven combat against climate change, to provide society with security of energy supply and to stimulate economic growth and development. For this a European graduate school is thought to be the best way. Goals are: establishing a 'school of thought' to foster the development of excellent young scientists, presenting the students with a vibrant, strong and international research community, and covering a wide range of topics and specialisations. For every new project, two existing projects are brought into the school, making it a unique cooperation with more than twenty internationally renowned professors and sixty research projects.Diverse topicsEvery university is hiring five PhD students and two postgraduates. The fifteen new PhD students and six postgraduates are going to work on topics as diverse as solar cells, biofuels and hydrogen storage, all under the theme 'The molecular approach to sustainable energy". 	Summer and Winter schoolsThe new graduate school in Sustainable Energy Technologies will be based on a number of themes. The first theme is 'The Molecular approach to sustainable energy'. Researchers of each theme will meet regularly, but at least twice a year, in Summer and Winter Schools. These serve primarily as platforms for knowledge exchange between researchers and students. Along with the exchange of MSc and PhD students and postgraduates, important elements will be lecture programs, guest courses and the serving as opponents in PhD committees by participating professors. Students who successfully complete the educational program of the graduate school will get a certificate of the graduate school in addition to the PhD degree they receive from their home university. The graduate school will over time also comprise MSc programs.BackgroundDanmarks Tekniske Universitet (DTU), the Technische Universität München (TUM) and the Technische Universiteit Eindhoven (TU/e) are building a network of scientific excellence in the European University Alliance of Science and Engineering. The common profile of the three universities is distinctly technical and innovative with entrepreneurship as a part of their dedicated strategies. The three universities belong to the top level of leading research universities in Europe. Together they have the ambition to be a central part of the network structure that will make up the European Institute of Innovation and Technology (EIT). Linkshttp://www.egs-energy.eu http://www.dtu.dk   http://www.tum.dehttp://www.tue.nl 
pberzins@stevens.eduHit by recession, IT industry changing focus rather than slashing jobsAnnual Society for Information Management trends survey shows corporations rethinking IT's role in cutting corporate costs and boosting productivityThe current recession has focused top information technology executives on cost-cutting, but they are not slashing jobs the way they did in previous economic downturns, according to a benchmark report commissioned by the Society for Information Management. "While IT organizations have slashed spending on infrastructure, they don't seem to be laying off IT people. In fact, the most successful IT organizations are not being asked to cut their own expenses, but to help the overall business reduce their costs."The bad news is that they're not hiring like they have in recent years. Instead, they are filling gaps by outsourcing domestically because this is faster than going overseas, but that will change next year," said Jerry Luftman.Luftman is a Distinguished Professor and an Executive Director in Stevens Institute of Technology's School of Technology Management. For the past six years, he has conducted the IT Industry Trends Survey for the Society for Information Management.Established in 1968, the Society for Information Management (SIM) was one of the first professional societies devoted to using information technology to improve organizational performance. Its more than 3,800 members include senior IT executives and top academics, consultants, and thought leaders in the field. The societys IT Industry Trends Survey dates back to 1980. Since then, it has become an essential tool in identifying emerging practices and trends within the industry, as well as providing guidance for schools developing IT curricula. The 2009 survey questioned top IT executives form nearly 250 companies about management concerns, technology developments, and organizational issues.Changing Management ConcernsThe survey asked participants to rate 39 managerial concerns. In past surveys, the top issues remained relatively steady. In 2009, however, they shifted dramatically.Managers ranked business productivity and cost reduction as their top concern by a wide margin. It ranked seventh in 2008. "In other business downturns, business executives went to IT organizations and said, 'Cut, cut, cut, we don't care how,'" Luftman said. "In this recession, which is even worse than previous recessions, they have rethought IT's role. They are asking IT to help cut the costs and improve the productivity of the rest of the business." Aligning IT and business was the second most important concern, down from number one last year. IT Business alignment has been a top-ten IT concern since the SIM's first survey in 1980.Business agility and speed to market ranked third, and business process re-engineering (automating business processes to reduce manual labor and cost) ranked fourth on the list. Neither appeared on last year's top 10 list. This years top four management concerns all relate to IT obtaining immediate returns by reducing the cost of doing business, Luftman said.  Reducing IT costs, which typically moves to the top of the list in a recession, ranked fifth in 2009.Improving IT reliability and efficiency (sixth), IT strategy (seventh), income-generating IT innovations (eighth), security and privacy (ninth), and CIO leadership (tenth) rounded out the top ten. Some of last year's top concerns, such as making better use of information (which ranked fifth in 2008) and managing change (sixth), never even cracked the top 10. Human resource concerns fell to seventeenth in 2009. Last year, HR issues ranked much higher: building business skills in IT ranked second, attracting new IT professionals rated fourth, and retaining IT professionals was eighth. This year they are off the list.TechnologiesThe recession caused IT executives to rethink their application and technology priorities, though not as radically as their management concerns. Business intelligence was the top technology in 2009, up from number two in 2008 and 2007. The technology involves mining data to find trends. Credit card companies, for example, use business intelligence systems to compare each new charge with previous transactions to uncover possible credit card fraud. Other companies use trends to determine possible causes of warranty repair or to identify customers with fast-growing businesses."This is a very complex technology, and IT organizations have been struggling with it for many years," Luftman noted. "It requires your data repositories to be in good shape, and that is hard to do."Server virtualization moved up to number two, from number five. Server virtualization involves partitioning larger servers into several smaller virtual servers. Using fewer large servers makes it easier to manage resources and improve availability while reducing server sprawl."This is an infrastructure investment, but there is an important message behind it," Luftman noted. "Companies could make other infrastructure improvements, such as software as service, cloud computing, and grid computing, but they're off the list. Virtualization is on the list because its costs are relatively low and it relatively quick to deploy."Enterprise resource planning (ERP), an application streamlines the flow of information through a company, jumped to third place among technical priorities, up from fourteenth in 2008. This is because ERP reduces costs by automating more processes, Luftman stated.Cost cutting is also the drive behind the fourth technical priority, internal and customer portals. These Web-like interfaces reduce manual labor by making it easier for business partners to interact with a company without human intervention. Portals ranked fifth in 2008.Enterprise application integration and management rose to the fifth position, from twelfth in 2008. Continuity and disaster planning dropped to sixth in 2009 after it tied for third in 2008. The top application focus in 2008, antivirus protection, fell out of the top 10 applications entirely."There were not a lot of big strategic initiatives in 2009," said Luftman. "The technologies and applications that received funding were designed to reduce costs and improve productivity inside the company quickly." Going ForwardAbout half the companies surveyed said their IT budgets declined in 2009. Just over half, 52 percent, said their budgets fell, and 23 percent said they remained flat. One in four companies increased spending, though this was down from 46 percent in 2008 and 61 percent in 2007.Budgets will stabilize in 2010. Forty-five percent of IT managers expect budgets to remain flat, 28 percent see them falling, and 27 percent see them rising in the year ahead.There were some significant changes in spending patterns. Infrastructure -- hardware, networking, and software -- fell to 33 percent of total spending, from 43 percent in 2008. Spending on both internal and domestic outsourced staffing rose. As a percentage of overall budget, internal domestic staff rose to 39 percent in 2009, up from 35 percent in 2008. Spending reached 12 percent for consulting services (up from 10 percent in 2008), 8 percent for domestic outsourcing (from 5 percent), and 4 percent for offshore outsourcing (from 3 percent). Next year, IT executives expect to spend modestly less on internal domestic staff and consulting, as well as infrastructure. Domestic outsourcing will grow to 9 percent in 2010. The big moves will come offshore. Offshore internal staff will rise to 6 percent in 2010 from 4 percent in 2009, and offshore outsourcing will rise to 6 percent from 4 percent. This will boost the percentage of IT spend overseas to 12 percent in 2010, from 8 percent in 2009.Still, Luftman remains optimistic: "The level of offshoring is not nearly high enough to start shouting that the sky is falling. It will rise to higher levels than in the past, but it could fall again, just as it has in the past.""IT is still a good career. The tenure of CIOs at their companies continues to increase, they are part of the core management team, and a new generation of business people sees IT as the solution when it comes to cutting business costs and boosting productivity."Overall, IT seems to be weathering the storm, and in most cases better than in previous recessions.  About Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value. Stevens offers baccalaureates, masters and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students with about 250 full-time faculty. Stevens graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.For the latest news about Stevens, please visit www.StevensNewsService.com.
laura.ost@nist.govNIST develops novel ion trap for sensing force and lightMiniature devices for trapping ions (electrically charged atoms) are common components in atomic clocks and quantum computing research. Now, a novel ion trap geometry demonstrated at the National Institute of Standards and Technology (NIST) could usher in a new generation of applications because the device holds promise as a stylus for sensing very small forces or as an interface for efficient transfer of individual light particles for quantum communications.The "stylus trap," built by physicists from NIST and Germany's University of Erlangen-Nuremberg, is described in Nature Physics.* It uses fairly standard techniques to cool ions with laser light and trap them with electromagnetic fields. But whereas in conventional ion traps, the ions are surrounded by the trapping electrodes, in the stylus trap a single ion is captured above the tip of a set of steel electrodes, forming a point-like probe. The open trap geometry allows unprecedented access to the trapped ion, and the electrodes can be maneuvered close to surfaces. The researchers theoretically modeled and then built several different versions of the trap and characterized them using single magnesium ions.The new trap, if used to measure forces with the ion as a stylus probe tip, is about one million times more sensitive than an atomic force microscope using a cantilever as a sensor because the ion is lighter in mass and reacts more strongly to small forces. In addition, ions offer combined sensitivity to both electric and magnetic fields or other force fields, producing a more versatile sensor than, for example, neutral atoms or quantum dots. By either scanning the ion trap near a surface or moving a sample near the trap, a user could map out the near-surface electric and magnetic fields. The ion is extremely sensitive to electric fields oscillating at between approximately 100 kilohertz and 10 megahertz.The new trap also might be placed in the focus of a parabolic (cone-shaped) mirror so that light beams could be focused directly on the ion. Under the right conditions, single photons, particles of light, could be transferred between an optical fiber and the single ion with close to 95 percent efficiency. Efficient atom-fiber interfaces are crucial in long-distance quantum key cryptography (QKD), the best method known for protecting the privacy of a communications channel. In quantum computing research, fluorescent light emitted by ions could be collected with similar efficiency as a read-out signal. The new trap also could be used to compare heating rates of different electrode surfaces, a rapid approach to investigating a long-standing problem in the design of ion-trap quantum computers.Research on the stylus trap was supported by the Intelligence Advanced Research Projects Activity.* R. Maiwald, D. Leibfried, J. Britton, J.C. Bergquist, G. Leuchs, and D.J. Wineland. 2009. Stylus ion trap for enhanced access and sensing. Nature Physics, published online June 28.
aem1@psu.eduSculptured materials allow multiple channel plasmonic sensorsSensors, communications devices and imaging equipment that use a prism and a special form of light -- a surface plasmon-polariton -- may incorporate multiple channels or redundant applications if manufacturers use sculptured thin films.	"Everyone uses surface plasmon resonance sensors. They are a multi billion-dollar industry worldwide," said Akhlesh Lakhtakia, the Charles Godfrey Binder (Endowed) professor of engineering science and mechanics, Penn State. "This type of sensor provides a fairly quick way to see what you have. It can tell you the concentration of chemicals, but in order to test for more than one chemical today, manufacturers have to use more than one sensor."	Surface plasmon resonance devices currently have a wide range of applications. They are commercially used as sensors for humidity, temperature, chemical concentrations and chemical composition. SPR devices can be used in a form of surface microscopy, as wave guides and tunable filters. Creating two or more channels in each device would multiply SPR utility in all areas of application.	Surface plasmon-polaritons are electromagnetic waves that flow along a sandwich of a metal and a dielectric. When light shines through a prism onto the sandwich, electrons form a cloud or plasma in the metal and the molecules of the dielectric get stretched or polarized. Under special conditions, a plasmon-polariton combination forms and moves as a single unit along the sandwich. The formation can be disturbed by the presence of an additional chemical in the dielectric. The disturbance provides the sensing principle. Useful as they are, each sensor can only detect one chemical for each prism and sandwich. 	In a series of papers Lakhtakia and his colleagues report on their theoretical and experimental investigation into the possibility of propagating more than one surface plasmon-polariton wave of the same color on a substrate. They recently reported on their experimental work in the Journal of Nanophotonics and the journal Electonic Letters.	The theoretical work indicated that for one wavelength or color of light, it should be possible to generate not just one, but up to three possible plasmon-polaritons if the dielectric used is not a traditional material, but a periodically non-homogeneous sculptured nematic thin film.	"Just because the mathematics suggest three possible surface plasmon-polariton waves does not mean that they can actually all be created," said Lakhtakia. "We had to find someone who could produce the thin films that we needed to test the possibilities experimentally."	Yi-Jun Jen, professor and chair, and Chia-Feng Lin, graduate student, both of the department of electro-optical engineering, National Taipei University of Technology, fabricated the sculptured nematic thin films that were then used in a standard Kretschmann surface plasma resonance sensor configuration. The researchers found that they produced three surface plasmon-polariton waves of light with the same wavelength or color, but with three different speeds. Two of these were polarized in one direction -- p polarized -- and the third was polarized in the other direction - s polarized.	"This would allow us to test more than two things or to test for the same thing twice in order to reduce sensing errors," said Lakhtakia.	The key to this finding is that sculptured thin films are not the same structure along their thickness. Instead, the pattern of sculpturing does periodically repeat. This periodicity allows the production of two or more surface waves of the same wavelength.	Lakhtakia, working with Devender, an international undergraduate research intern and Drew Patrick Pulsifer, graduated student in engineering science and mechanics, next tried a chiral sculptured thin film. Chiral thin films are similar to periodic sculptured nematic thin films but are like a multitude of parallel corkscrews. Using these thin films the researchers generated two surface plasmon-polaritons waves, but with different speeds, both with p-polarized light.	"If this approach can be optimized and commercialized, there are exciting prospects in store for plasmonic-based sensing, imaging and communications," said Lakhtakia.
mary.beckman@pnl.govNew supercomputer to reel in answers to some of Earth's problemsEMSL's Chinook supercomputer by HP commissioned for researchRICHLAND, Wash. -- The newest supercomputer in town is almost 15 times faster than its predecessor and ready to take on problems in areas such as climate science, hydrogen storage and molecular chemistry. The $21.4 million Chinook supercomputer was built by HP, tested by a variety of researchers, and has now been commissioned for use by Pacific Northwest National Laboratory and the Department of Energy.Housed at EMSL, DOE's Environmental Molecular Sciences Laboratory on the PNNL campus, Chinook can perform more than 160 trillion calculations per second, ranking it in the top 40 fastest computers in the world (see the Top 50). Its predecessor, EMSL's MPP2, could run 11.2 trillion calculations per second. The Office of Biological and Environmental Research within DOE's Office of Science funded EMSL's supercomputer upgrade. Although housed at the Pacific Northwest National Laboratory, scientists the world over can use Chinook, competing for time through a peer review process. Users generally conduct research that supports the DOE's missions in energy, the environment, or national security. "When combined with EMSL's experimental capabilities, the new Chinook supercomputer will provide scientists from academia, national laboratories, and industry with an unprecedented research tool," said Anna Palmisano, DOE Associate Director for Biological and Environmental Research. "This new supercomputer will allow scientists to develop a molecular-level understanding of the complex biological, chemical and physical processes that underlie the environmental and energy challenges facing DOE and the nation."Chinook is fast and dexterous. Its designers tailored its architecture to handle scientific problems whose complexity require more than just power or speed. For example, climate scientists who are trying to understand the tiniest particles in the atmosphere or chemists watching how atoms tug at each other in a molecule need a different kind of supercomputer than physicists studying questions like the birth of the universe. Chinook's top job is to run NWChem, a computational chemistry program that allows researchers to simulate and predict the chemistry within and between molecules. But a wide variety of programs can run on the supercomputer. Scientists are using Chinook to tackle problems such as:	Gas hydrates: Pockets of fuels such as methane are often found deep under the sea, trapped in a lattice of water molecules. Researchers hope to understand these gas hydrates both as a fuel source and as a way to store fuels. But for such a simple molecule, water has pretty complex chemistry. Researchers are using Chinook to help understand how water molecules form stable clusters. The work also gives researchers insight into how small particles in the air form clouds or break them up.	Bacterial transformers: Communities of bacteria live and grow in the soil. And some bacteria have a taste for metals, a talent that can be used to clean up toxic substances in contaminated ground. Researchers use Chinook to understand the inner workings of these bacteria and how they form communities in order to take advantage of their clean-up skills.	Green plastics: Industrial chemists can turn propane gas into plastics and generate only water as a byproduct with the help of compounds called catalysts. Chinook is helping scientists develop a new catalytic material based on small clusters of platinum atoms that does this at least 40 times more efficiently than older materials.Unlike a consumer buying a PC, EMSL purchased a custom-made machine from HP specifically designed for the demands of computational chemistry. After the computer's delivery and set up (see Chinook being set up here), EMSL asked its users to test the system during a period of time called acceptance testing. The many dozens of researchers who helped test the machine allowed Chinook's handlers to work out the bugs. Because there are so few clusters of this size, the testing was a vital part of the process, and the more realistic the testing, the better."If you just have a few people running large jobs, the amount of communication back and forth between the nodes is very different than if you have a hundred people running calculations of various sizes and in various places on the machine," said PNNL's Erich Vorpagel, who manages all research projects on Chinook at EMSL.Chinook has 4620 Quad-core processors built into 2310 nodes. Each node can be thought of as the equivalent of four personal computers. But Chinook's nodes are more like supercharged PCs: the Quad-cores give each node the equivalent of eight processor-cores and 32 gigabytes of memory. The supercomputer was named after Chinook, or king salmon, through a naming contest by EMSL users. Researchers who want to use Chinook write proposals to EMSL and compete for time annually.EMSL, the Environmental Molecular Sciences Laboratory, is a national scientific user facility sponsored by the Office of Biological and Environmental Research within Department of Energy's Office of Science and is located at Pacific Northwest National Laboratory. EMSL offers an open, collaborative environment for scientific discovery to researchers around the world. EMSL's technical experts and suite of custom and advanced instruments are unmatched. Its integrated computational and experimental capabilities enable researchers to realize fundamental scientific insights and create new technologies.Pacific Northwest National Laboratory is a Department of Energy Office of Science national laboratory where interdisciplinary teams advance science and technology and deliver solutions to America's most intractable problems in energy, national security and the environment. PNNL employs 4,200 staff and has an $850 million annual budget. Ohio-based Battelle has managed PNNL since the lab's inception in 1965. 
ericksn@umich.eduU-M physicists create first atomic-scale map of quantum dotsANN ARBOR, Mich.---University of Michigan physicists have created the first atomic-scale maps of quantum dots, a major step toward the goal of producing "designer dots" that can be tailored for specific applications.Quantum dots---often called artificial atoms or nanoparticles---are tiny semiconductor crystals with wide-ranging potential applications in computing, photovoltaic cells, light-emitting devices and other technologies. Each dot is a well-ordered cluster of atoms, 10 to 50 atoms in diameter.Engineers are gaining the ability to manipulate the atoms in quantum dots to control their properties and behavior, through a process called directed assembly. But progress has been slowed, until now, by the lack of atomic-scale information about the structure and chemical makeup of quantum dots.The new atomic-scale maps will help fill that knowledge gap, clearing the path to more rapid progress in the field of quantum-dot directed assembly, said Roy Clarke, U-M professor of physics and corresponding author of a paper on the topic published online Sept. 27 in the journal Nature Nanotechnology. Lead author of the paper is Divine Kumah of the U-M's Applied Physics Program, who conducted the research for his doctoral dissertation."I liken it to exploration in the olden days," Clarke said of dot mapping. "You find a new continent and initially all you see is the vague outline of something through the mist. Then you land on it and go into the interior and really map it out, square inch by square inch."Researchers have been able to chart the outline of these quantum dots for quite a while. But this is the first time that anybody has been able to map them at the atomic level, to go in and see where the atoms are positioned, as well as their chemical composition. It's a very significant breakthrough."To create the maps, Clarke's team illuminated the dots with a brilliant X-ray photon beam at Argonne National Laboratory's Advanced Photon Source. The beam acts like an X-ray microscope to reveal details about the quantum dot's structure. Because X-rays have very short wavelengths, they can be used to create super-high-resolution maps."We're measuring the position and the chemical makeup of individual pieces of a quantum dot at a resolution of one-hundredth of a nanometer," Clarke said. "So it's incredibly high resolution." A nanometer is one-billionth of a meter.The availability of atomic-scale maps will quicken progress in the field of directed assembly. That, in turn, will lead to new technologies based on quantum dots. The dots have already been used to make highly efficient lasers and sensors, and they might help make quantum computers a reality, Clarke said."Atomic-scale mapping provides information that is essential if you're going to have controlled fabrication of quantum dots," Clarke said. "To make dots with a specific set of characteristics or a certain behavior, you have to know where everything is, so that you can place the atoms optimally. Knowing what you've got is the most important thing of all."In addition to Clarke, co-authors of the Nature Nanotechnology paper are Sergey Shusterman, Yossi Paltiel and Yizhak Yacoby.The research was sponsored by a grant from the National Science Foundation. The U.S. Department of Energy supported work at Argonne National Laboratory's Advanced Photon Source.
Joanne.Finlay@csiro.auRainforest rehab in every senseSophisticated sensors that measure leaf wetness, soil moisture and temperature are helping rehabilitate rainforest in the Springbrook World Heritage precinct in south-east Queensland. The CSIRO sensors are being used to uncover the microclimatic conditions favourable for rapid natural regeneration of degraded rainforest environments.A network of ten sensor nodes, connected wirelessly, has been sampling parameters such as rainfall, humidity, temperature, soil moisture and the amount of available light inside the forest every five minutes since May 2008.Over the next two years, another 200 nodes will be installed, some of which will measure biodiversity indicators, such as bird and frog calls.CSIRO ICT Centre Research Scientist, Darren Moore, said the sensors are solar-powered and have been developed specifically for monitoring the complex, interlinked variables found in natural environments."In the rainforest, there is limited sunlight under the canopy which means we've had to develop sophisticated techniques to manage power," Mr Moore said. "Our nodes are able to stay on-line, adaptively reducing their workload, to minimise the amount of power used."Queensland Department of Environment and Resource Management spokesman, Jonathan Hodge, said the technology is helping to "push the boundaries of environmental science."CSIRO's Sensor Network Technologies Research Director, Dr Michael Bruenig said the Springbrook project demonstrates that real-time data can be streamed back from open and covered rainforest using a low bandwidth wireless sensor network."CSIRO's FLECK devices are capable of low-powered wireless mesh networking, intelligent energy management and interfacing to a broad range of sensors," Dr Bruenig said."They are providing the capability to provide reliable, long-term monitoring of the natural environment which  in the case of Springbrook - can be applied to rainforest ecosystems." The project is a partnership between CSIRO, the Queensland Department of Environment and Resource Management and the Australian Rainforest Conservation Society. Image available at: http://www.scienceimage.csiro.au/mediarelease/mr09-97.htmlFurther Information: Michael Brenig, CSIRO ICT CentrePh: 07 3327 4431E: Michael.Bruenig@csiro.au Background information available at:http://www.csiro.au/science/FLECK-rainforest-rehabilitation.htmlMedia Assistance:Jo Finlay, CSIRO ICT Centre 	Ph: 02 9372 4309Mb: 0447 639688E: Joanne.Finlay@csiro.au www.csiro.au
pberzins@stevens.eduAttila Technologies and InterDigital form collaboration in wireless bandwidth aggregation techInterDigital acquires stake in Attila and invests in joint development effortHOBOKEN, NJ  Stevens Institute of Technology announced that its Technogenesis® spin-out company, Attila Technologies, LLC, has entered into a multi-faceted collaboration with InterDigital Communications, LLC, a wholly-owned subsidiary of InterDigital, Inc.  Under the agreement, InterDigital® has provided an undisclosed capital infusion and acquired a minority position in Attila.  In addition, the companies will collaborate on the development and marketing of bandwidth aggregation technologies and related multi-network innovations."Attila is a prime example of how Stevens' engineering excellence and commitment to entrepreneurship can directly benefit society," said Malcolm Kahn, Stevens' Vice President of Enterprise Development and Licensing.  "The Technogenesis program at Stevens has a history of creating breakthrough technology that is turned into practical solutions by world-class teams of science and business people."In today's wireless world, network and equipment providers are seeing an unprecedented growth in wireless data usage, mostly driven by the rapid penetration of smart phones, a clear shift towards cloud-based solutions requiring more resources and the ever-evolving internet toward dynamic applications.  At the same time, users are becoming more reliant on wireless devices and expect their Internet experience to meet bandwidth, availability, and reliability that approach wired connections. " Attila's software is designed to meet this ever-growing need for bandwidth, ubiquity and reliability by simultaneously and dynamically leveraging the individual capabilities of existing wireless networks to create an always-on, uninterrupted connection for mobile users, providing the best level of service, anywhere, anytime" said Nicolas Girard, Attila's President and Chief Operating Officer.  Frank Ianna, Attila's Chief Executive Officer, added, "We are very excited about our collaboration with InterDigital, which will help promote this innovative technology to the mobile wireless industry and provide Attila with added depth and market access." Under the agreement, InterDigital has acquired a minority stake in Attila and a seat on its board of directors. InterDigital also made an additional investment in specified technology development programs. The companies will jointly demonstrate their bandwidth aggregation capabilities on live networks at the 2010 Mobile World Congress in Barcelona. About Attila Technologies & Stevens Institute of TechnologyAttila Technologies, LLC is a start-up company developed under the Technogenesis program for entrepreneurship at the Stevens Institute of Technology.  The company was spun-out of Stevens in 2004 and has been supported by funding from both Stevens and private investors.  About InterDigital InterDigital designs, develops and provides advanced wireless technologies and products that drive voice and data communications.  InterDigital is a leading contributor to the global wireless standards and holds a strong portfolio of patented technologies that it licenses to manufacturers of 2G, 2.5G, 3G, and 802 products worldwide. Stevens is a registered trademark of Stevens Institute of Technology.  InterDigital is a registered trademark of InterDigital, Inc.For more information, please callAttila Contact:Frank LauriaEmail: info@attila-tech.com+1 -8777-ATTILAwww.attila-tech.comAbout Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value. Stevens offers baccalaureates, master's and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students with about 250 full-time faculty. Stevens' graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.For the latest news about Stevens, please visit www.StevensNewsService.com.
angeline.french@srnl.doe.govSRNL, automakers to develop high-performance wireless sensors networksSeveral industries use wireless sensors, which can monitor chemical processes or equipment activity and then transmit the data over a wireless network.  Still, many facilities that could benefit from the use of wireless sensors must continue to use a wired network instead, because the reliability, speed and security of the current generation of wireless sensors do not meet their needs. The U.S. Department of Energy's Savannah River National Laboratory and U.S. automakers now have teamed up to develop a new high-performance platform for these sensors that not only serves the industry's needs, but also meets the DOE National Nuclear Security Administration's requirements for security and reliability for use in its facilities.  SRNL has entered into a cooperative research and development agreement with the United States Council for Automotive Research LLC (USCAR),  the collaborative automotive technology organization for Chrysler Group LLC, Ford Motor Company and General Motors Corporation. The purpose of the collaboration is to develop a new platform for short range wireless sensors networks that meets the NNSA requirements, and can also be adopted as the industry standard. Under the agreement, SRNL will develop designs and specifications for the new wireless hardware, then engage a qualified wireless manufacturer to make a prototype, which the partners will test and validate. The ultimate goal of the agreement is to produce a standard for wireless sensor platforms that can be adopted by the International Society of Automation, a global instrumentation, systems and automation standards body."As partners with SRNL in this endeavor, we look forward to creating an industry standard for wireless sensor platforms that meets the needs of both industry and government and enables significant cost savings for both," said Don Walkowicz, USCAR executive director. "Traditionally, collaborations between the U.S. automakers and U.S. government laboratories have resulted in innovation and great  success." Both the automotive industry and the NNSA have needs for wireless sensors that are reliable, secure, high speed and able to resist interference from existing systems.  This agreement between a DOE laboratory and USCAR to produce a single, agreed-upon platform will broaden the customer base for resulting sensor designs, making it more attractive for developers to design hardware that meets the NNSA requirements.  In the automotive industry, for example, replacing hard-wired body shop robots with wireless-controlled robots would be a prime application area for a new secure, wireless sensor network. NNSA and its contractors use sensors in their facilities to monitor chemical processes, vibration on large pumps and blowers, and environmental conditions such as shock, vibration, and linear acceleration.  The ability to use wireless, rather than wired, sensors, when constructing new facilities or installing new sensors in existing facilities will bring considerable cost savings.  NNSA sensors typically exist in gloveboxes or "hot cells," which protect workers from exposure to radioactive or chemical hazards.  The cost of running cables in "hot" facilities is more than $2,000 per foot.  The electrical/instrument portion of such a facility may have a budget of as much as $400 million; a conservative estimate of the cost savings to use wireless sensors networks has been estimated at $50 million.  Existing facilities that are already contaminated would be able to add instrumentation at less than 10% the cost of a wired solution.  "We are pleased to be working with the three U.S. automakers through USCAR to create an industry standard for wireless sensor platforms," said Joe Cordaro, SRNL advisory engineer and former chair of the NNSA Network of Senior Scientists and Engineers,  who is serving as SRNL lead for the initiative.   "Our common needs will drive a design and framework that are applicable in government and non-government facilities, ultimately providing economies of scale, and ensuring robust and reliable requirements for wireless sensor platforms globally." SRNL is DOE's applied research and development national laboratory at the Savannah River Site (SRS).  SRNL puts science to work to support DOE and the nation in the areas of environmental management, national and homeland security, and energy security.  The management and operating contractor for SRS and SRNL is Savannah River Nuclear Solutions, LLC.Founded in 1992, USCAR is the collaborative automotive technology organization for Chrysler Group LLC, Ford Motor Company and General Motors Corporation. The goal of USCAR is to further strengthen the technology base of the domestic auto industry through cooperative research and development. For more information, visit USCAR's Web site at www.uscar.org
mejanes@sandia.govSandia joins forces with Boeing, Caltrans, others on fuel cell-powered mobile lighting applicationLIVERMORE, Calif.  Sandia National Laboratories, with help from The Boeing Company, the California Department of Transportation (Caltrans), and others, is leading an effort to develop a commercially viable, fuel cell-powered mobile lighting system."Mobile lighting" refers to small, portable lighting systems that are used primarily by highway construction crews, airport maintenance personnel, and even film crews."The beauty of this project is that it ties together the manufacturers [Multiquip, Altergy Systems, Luxim, Lumenworks, Stray Light] with Sandia and the end users [Caltrans, San Francisco International Airport] in one collaboration, hopefully reducing commercialization barriers that so often hinder the widespread use of new technology," said Sandia project lead Lennie Klebanoff. The end goal of the project, according to Klebanoff, is to get fuel cell technology into more widespread commercial use, particularly in general construction and aviation maintenance applications.Two separate designsSandia has adopted a two-prong (alpha and beta) approach to the project. First, along with a number of the external partners who are contributing time and in-kind resources, Klebanoff's team is overseeing the production of the "alpha" mobile lighting unit that is expected to debut Oct. 22-26 at the annual meeting of the American Association of State Highway and Transportation Officials (AASHTO). The alpha unit is separate from the more advanced "beta" design that Sandia recently completed for Boeing and came about due to the enthusiasm of several industry partners and their desire to see a system built sooner rather than later."Caltrans wanted us to get the alpha version in front of their highway transportation peers immediately, and our unit will be in operation and actually illuminating the new electric cars being featured at the AASHTO meeting," said Klebanoff. "It will give all of us good feedback on how interested potential customers are in the technology, and also allow us to get an initial assessment of how the technology performs, particularly the plasma lighting."The alpha system consists of advanced power-saving Light Emitting PlasmaTM technology (contributed by Luxim, Lumenworks, and Stray Light), two high-pressure hydrogen tanks (purchased by Sandia), a trailer to transport the equipment (provided by Multiquip), and a fuel cell (provided and installed by Altergy Systems). Multiquip and Altergy are assembling the overall unit, while Sandia has consulted on its design and formulated the alpha unit technical plan for the team.The project has also attracted the interest of SFO, a long-time partner with Sandia on various homeland security projects. SFO would like to test the system for use in nighttime runway repair work, as well as in its terminal renovation activities. Unlike the diesel systems that traditionally power mobile lighting units, the fuel cell-powered mobile light can be used indoors.Boeing design will use metal hydride storageBoeing funded Sandia primarily to develop the "beta" design, a more sophisticated, technically ambitious unit that utilizes metal hydride storage tanks designed by Ovonic Hydrogen Systems.  These tanks store 12 kilograms of hydrogen, and thus offer some 90 hours of operating time (compared to the 30-40 hours offered by the alpha unit). Sandia's engineers designed the overall beta system and solved the thermal management issues that surround metal hydride storage, including coupling waste fuel cell heat to the hydride bed.  Metal hydride storage is also appealing since it removes many of the safety concerns found with having high pressure on the Alpha unit (whose tanks hold hydrogen at 5000 psi, compared to 250 psi with the metal hydride tank system).  These are all important considerations for commercialization, Klebanoff said.Other funding sources, he said, are being sought so that the beta system can be built and both versions of the system can then be tested and compared on equal terms. The team would also like to use the field-test data to perform quantitative analyses of the emissions reductions and increased energy efficiency afforded by the technology.  Ultimately, Klebanoff said, it will be the manufacturers who decide which system is most attractive for commercial purposes.Traditionally, mobile lighting units are powered by diesel fuel generators that produce CO2, NOx (nitrogen oxides produced during combustion), and soot, making them less than ideal for the environment. In addition, diesel units are noisy, which creates a safety hazard when construction personnel are distracted and can't hear oncoming traffic.  A fuel cell running on pure hydrogen, on the other hand, is both very quiet and a zero-emission electric power source.Klebanoff estimates that each deployed fuel cell-based mobile light would avoid the burning of nearly 900 gallons of diesel fuel per year and eliminate the emission of NOx and soot.  If the hydrogen used is generated from non-fossil fuel sources, then each mobile light unit would also reduce CO2 emissions by about nine metric tons per year.Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin company, for the U.S. Department of Energy's National Nuclear Security Administration. With main facilities in Albuquerque, N.M., and Livermore, Calif., Sandia has major R&D responsibilities in national security, energy and environmental technologies, and economic competitiveness.
mwdorsey@wpi.eduOnline social networks leak personal information to tracking sites, new study showsThe leakage puts social network users at risk of having their identity linked with their browsing behavior; the study is the first to describe a mechanism that tracking sites could use to directly link browsing habits to specific individualsWORCESTER, Mass. -- More than a half billion people use online social networks, posting vast amounts of information about themselves to share with online friends and colleagues. A new study co-authored by a researcher at Worcester Polytechnic Institute (WPI) has found that the practices of many popular social networking sites typically make that personal information available to companies that track Web users' browsing habits and allow them to link anonymous browsing habits to specific people. The study, presented recently in Barcelona at the Workshop on Online Social Networks, part of the annual conference of the Association for Computing Machinery's Special Interest Group on Data Communications, is the first to describe a mechanism that tracking sites could use to directly link browsing habits to specific individuals."When you sign up with a social networking site, you are assigned a unique identifier," says Craig Wills, professor of computer science at WPI, who conducted the study with an industry colleague. "This is a string of numbers or characters that points to your profile. We found that when social networking sites pass information to tracking sites about your activities, they often include this unique identifier. So now a tracking site not only has a profile of your Web browsing activities, it can link that profile to the personal information you post on the social networking site. Now your browsing profile is not just of somebody, it is of you."Like most commercial websites, online social networks use third-party tracking sites, called aggregators, to learn about the browsing habits of their visitors. Cookies are maintained by a Web browser and contain information that enable tracking sites to build profiles of the websites visited by a user. Each time the user visits a new website, the tracking site can review those cookies and serve up ads that might appeal to the user. For example, if the user frequently visits food sites, he or she might see an ad for a new cookbook.Online networking sites have gone a step further by allowing for transmission of unique identifiers. It is a particularly troubling practice for two reasons, Wills says. "First," he notes. "users put a lot of information about themselves on social networking sites. Second, a lot of that information can be seen by other users, by default. There are mechanisms users can use to limit access to their information, but we found through previous research that most users don't take advantage of them." With a unique identifier, a tracking site could gain access to a user's name, physical address, email address, gender, birth date, educational and employment information, and much more.With the "leakage" of this type personal information, there is a significant risk of having one's identity linked to an inaccurate or misleading browsing profile. Browsing profiles record the websites a particular computer has accessed, not who was using the computer at the time or why particular sites were chosen. According to Wills, this leaves room for inaccurate profiling by tracking sites, a situation that has the potential to lead to serious problems. When a computer is used by more than one person, or a person browses for curiosity rather than intent, it leaves room for misinterpretation, he notes. "Tracking sites don't have the ability to know if, for example, a site about cancer was visited out of curiosity, or because the user actually has cancer. Profiling is worrisome on its own, but inaccurate profiling could potentially lead to issues with employment, health care coverage, or other areas of our personal lives."Wills says the researchers do not know what, if anything, tracking sites do with the unique identifiers that social networks transmit to them. They say they have communicated with all of the sites they studied to inform them about the privacy leakage, but have not heard back officially from any. "We are not saying that they are necessarily trying to leak private information," he says. "But once someone is in possession of your unique identifier, there is so much they can learn about you. And even if tracking sites do not use the information themselves, can they guarantee that it will never find its way into other hands? For these reasons, we feel this issue is something that we should to be concerned about."The researchers also note that while users of social networking sites can protect themselves to some degree by limiting the amount of information they post and using the protections the sites make available to them to limit access to their information, the easiest way to prevent privacy leakage would be for social networking sites to stop making unique identifiers visible.	View the full study here:http://conferences.sigcomm.org/sigcomm/2009/workshops/wosn/papers/p7.pdfAbout Worcester Polytechnic InstituteFounded in 1865 in Worcester, Mass., WPI was one of the nation's first engineering and technology universities. WPI's14 academic departments offer more than 50 undergraduate and graduate degree programs in science, engineering, technology, management, the social sciences, and the humanities and arts, leading to bachelor's, master's and PhD degrees. WPI's world-class faculty work with students in a number of cutting-edge research areas, leading to breakthroughs and innovations in such fields as biotechnology, fuel cells, and information security, materials processing, and nanotechnology. Students also have the opportunity to make a difference to communities and organizations around the world through the university's innovative Global Perspective Program. There are 25 WPI project centers throughout North America and Central America, Africa, Australia, Asia, and Europe.
viktor.podolskiy@physics.oregonstate.eduNew nanotech sensor developed with medical, chemistry applicationsCORVALLIS, Ore.  Researchers at Oregon State University and other institutions have developed a new "plasmonic nanorod metamaterial" using extraordinarily tiny rods of gold that will have important applications in medical, biological and chemical sensors.The new device is at least 10 times more sensitive than existing technology, researchers say, can be tuned to sense different types of materials and is easy to make in differing sizes for individual needs. It's one of the first real applications of "metamaterials"  artificial materials that have unusual properties based on their structure, which are not readily available in nature.The findings were announced today in Nature Materials, a professional journal. Collaborators on the project included OSU, the Universite de Mediterranee in France, Ecole Polytechnique de Montreal in Canada, and the Queen's University of Belfast in the United Kingdom. The research was supported by the National Science Foundation and other agencies. "This is very exciting," said Viktor Podolskiy, an associate professor of physics at OSU. "It's an important new application of nanotechnology and the field of metamaterials, and should find some significant uses in medicine, chemistry and physics."The new material is made primarily from gold, but given the minuscule size of the device, the high cost of gold is actually of little importance  and the use of gold aids its performance, because this rare metal is very inert and doesn't interact with biological or many other molecules. The device is a little like the bristles that stick up on a hairbrush, but in this case the bristles are only about 20 nanometers wide  it would take 5,000 such bristles to be the width of a human hair.Using this device and various optical techniques, sensors can determine very precisely the identity and amount of various substances, including extremely small compounds such as drugs, vitamins or hormones. The concept should find near-term applications in medicine and other fields, scientists say.
ghunka@aftau.orgA 'super sensor' for cancer and CSIsTel Aviv University develops tiny device to 'sniff out' disease, heart attacks, poison and environmental pollutionLike the sensitive seismographs that can pick up tremors of impending earthquakes long before they strike, a similar invention from Tel Aviv University researchers may change the face of molecular biology. Coupling biological materials with an electrode-based device, Prof. Judith Rishpon of TAU's Department of Molecular Microbiology and Biotechnology is able to quickly and precisely detect pathogens and pollution in the environment - and infinitesimally small amounts of disease biomarkers in our blood. About the size of a stick of gum, the new invention may be applied to a wide range of environments and situations. The aim is for the device to be disposable and cost about $1."Biosensors are important for the bio-terror industry, but are also critical for detecting pathogens in water, for the food industry, and in medical diagnostics," says Prof. Rishpon. Her latest research appeared in the journals Nanomedicine: Nanotechnology Biology and Medicine, Electroanalysis and Bioelectrochemistry.Portable and preciseWhat makes this particular invention particularly appealing is its small size and the fact that it can be easily connected to a handheld device like a Blackberry or iPhone for quick and reliable results. An electrical signal will pulse "yes" for the presence of a test molecule and a "no" for its absence. Currently, clinical researchers are testing its application in cancer diagnostics, focusing on the detection of proteins associated with colon and brain cancer and efficacy of anticancer drugs. But the device is capable of detecting various types of substances. "It really depends on what you put at the end of the electrode," says Prof. Rishpon. "You can put enzymes, antibodies or bacteria on my electrodes to sense the existence of a chemical target. Then we can measure the amount of the target, assessing its potency by using additional enzymes or by looking at the changes of the electrochemical properties on the device," she says. An early warning system for heart attacksEnzymes released before the onset of a heart attack can also be detected, so this application has obvious uses in an operating room to give a physician warning of an impending attack during a procedure. It could be fitted into an implant like a pacemaker or another future device to alert the user to impending dangers, thus preventing sudden death. Prof. Rishpon is also investigating the application of her technology to detect for pathogens in drinking water such as estrogen, a byproduct of the female birth control pill. The presence of these chemicals in America's drinking water is no minor health concern. And before tackling the problem, water officials need to know what they are up against. Prof. Rishpon's solution could be part of the future toolkit, she believes. A bio-watchdog for the organic food industryDetecting pesticides in food is another very desirable application. The organic food market is calling for more rigorous testing and regulations to ensure spraying doesn't occur on some farms, and that limits are not breached on others. Commercial applications of Prof. Rishpon's basic research are already underway in many areas of diagnostics, but clearly there are more to come. "My super sensors are cheap, accurate and highly sensitive, and in principle they could detect and measure the presence of almost every biological-based material," Dr. Rishpon concludes. She is also collaborating on the device with scientists at Arizona State University.American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
jeff.yan@newcastle.ac.ukTouch typists could help stop spammers in their tracksComputer scientists at Newcastle University are about to give office workers a perfect excuse to play games: it's all in the name of research.Dr Jeff Yan, together with his PhD student Su-Yang Yu, has created 'Magic Bullet' as an effective solution to a problem which no known computer algorithm can yet solve.This simple computer game turns a tedious manual labelling task into a form of light entertainment and could soon help companies improve their chances of tackling online spammers.CAPTCHA (Completely Automated Public Turing Test to Tell Computers and Humans Apart) is widely used by commercial websites such as Google and Yahoo to defend against malicious Internet bots which spread junk emails or grab thousands of free email accounts.A common approach to testing its robustness is to try and attack or break the scheme. This involves acquiring a set of labelled samples, but as computers find it difficult to recognise distorted test or images, this task usually falls to human researchers. "Manually labelling samples is tedious and expensive," explained Dr Yan, who led the research. "For the first time, this simple game turns it into a fun experience with a serious application as it also achieves a labelling accuracy of as high as 98 per cent."Spammers can make a lot of money out of using computer programs that can automatically bypass a heavily used CAPTCHA such as those used by Google, Microsoft and Yahoo. Therefore it is important for researchers to understand and improve the robustness of the system in order to stay one step ahead.To fully evaluate the robustness of a CAPTCHA scheme at least 10,000 segments usually have to be labelled  a task which cannot be automated.Magic Bullet is a dual-purpose online shooting game that can be played just for fun but also contributes to solving a real problem.Players are randomly pitched against each other, with two in each team. Teams or players cannot communicate with each other and security techniques are used to ensure they are geographically apart to reduce the likelihood of cheating. If there are not enough human players, then one of two types of bots - a Data Relay Bot which replays data from old games or a Tailored Response Bot which acts according to an opposing team's performance  will be introduced.During each round a randomly chosen segmented CAPTCHA character appears and will shoot towards the target only when both players correctly identify it before their opponents. Although the computer does not know which character each of the segments is, the answers given by the winning team can be accurate labels for the segments in the majority of cases.The game also includes a high scoring table to encourage players to return to improve on a previous score."An average game session produced 25 correct labels per minute, giving 1,500 per hour," explained Dr Yan. "Although this is not particularly fast, if touch typists were used it would be noticeably improved, and also players need time to get to know how the game works. "As this game supports a large number of parallel sessions, which are limited only by the network bandwidth and game server's CPU and memory, there is also a lot of scope to increase the labelling rate dramatically."Dr Yan is presenting his findings at the IJCAI'09 in Pasadena, CA, USA this month.
ahayenga@uab.eduUAB international conference focuses on preventing high-capacity computer data theftBIRMINGHAM, Ala.  Leading high-performance computing engineers and researchers attending a three-day international conference at the University of Alabama at Birmingham (UAB) have called for renewed vigilance in field-related data security. The group says growing world-wide computer use puts more and more applied modeling, design and other super-computer-processed projects that are stored digitally at risk for theft by hackers and other cyber criminals.The Department of Mechanical Engineering in the UAB School of Engineering hosted the International Conference on Applied Modeling and Information Security Systems (ICAMISS) Oct. 8-10, with 50 applied modeling experts from a range of disciplines attending.Applied modeling is the practice of using high-capacity computers to render three-dimensional images and immersive virtual reality programs in order to view engineering designs and verify their functionality prior to full-scale production. It is useful to numerous disciplines. Testing the viability and integrity of new bridge, fighter plane or building designs are some of the uses for applied modeling; in health care, it is used for review of the feasibility of newly designed joint replacement devices."Modeling and computing helps to solve the world's complex problems, but the information we process on the high-speed devices of our field can easily be abused if lost to those with ill intent," said Bharat Soni, Ph.D., a conference co-chair and the chair of the Department of Mechanical Engineering. "That is why our conference has focused on securing the data we generate as researchers." Information systems security experts from the field of applied modeling lectured on the strategies for securing high-performance computing data but were most concerned with creating awareness about the potential for data theft."Now when we work to generate new information and data, we will know to protect it," said Suhrit K. Dey, Ph.D., conference chair and a professor of mathematics and computer science at Eastern Illinois University. "The information developed in computer modeling is the intellectual property of the researchers and designers, and we do not want it abused."ICAMISS attendees also recognized Joe F. Thompson Jr., Ph.D., professor of aerospace engineering at Mississippi State University. Thompson is considered the father of computational science and engineering after developing the grid generation technology applicable to complex configurations that is commonly used in high fidelity computational field simulations."Joe Thompson is the pioneer of our entire field, and the scientific and engineering community is deeply indebted to his outstanding research and teaching," Soni said. ICAMISS was organized by the Institute of Applied Science and Computations of Charleston, Ill. Conference speakers were selected by an advisory committee of scientists from India, Italy, Lithuania, the United Kingdom and United States.NASA provided external funding for ICAMISS, which was secured by the one of the event's chief coordinators, John Zeibarth, Ph.D., the senior vice president of the Krell Institute. The U.S. Department of Defense, Krell Institute, Eastern Illinois University and UAB also provided financial support.About UABThe UAB School of Engineering offers students real-world experience while they train in one of its degree programs, which include the only undergraduate biomedical engineering program in Alabama. Students experience cutting-edge research opportunities, industry co-ops and unique internships generated by the school's commitment to interdisciplinary learning. VIDEO: www.youtube.com/uabnews TEXT: www.uab.edu/news TWEETS: www.twitter.com/uabnews
taira@seismo.berkeley.eduNew way to monitor faults may help predict earthquakesThis release is available in Chinese.Washington, D.C.Scientists at the Carnegie Institution have found a way to monitor the strength of geologic faults deep in the Earth. This finding could prove to be a boon for earthquake prediction by pinpointing those faults that are likely to fail and produce earthquakes. Until now, scientists had no method for detecting changes in fault strength, which is not measureable at the Earth's surface. Paul Silver* and Taka'aki Taira of the Carnegie Institution's Department of Terrestrial Magnetism, with Fenglin Niu of Rice University and Robert Nadeau of the University of California, Berkeley, used highly sensitive seismometers to detect subtle changes in earthquake waves that travel through the San Andreas Fault zone near Parkfield, California, over a period of 20 years. The changes in the waves indicate weakening of the fault and correspond to periods of increased rates of small earthquakes along the fault."Fault strength is a fundamental property of seismic zones," says Taira, now at the University of California, Berkeley. "Earthquakes are caused when a fault fails, either because of the build-up of stress or because of a weakening of the fault.  Changes in fault strength are much harder to measure than changes in stress, especially for faults deep in the crust. Our result opens up exciting possibilities for monitoring seismic risk and understanding the causes of earthquakes."The section of the San Andreas Fault near Parkfield, sometimes called the "Earthquake Capital of the World," has been intensively studied by seismologists and is home to a sophisticated array of borehole seismometers called the High-Resolution Seismic Network and other geophysical instruments. Because the area experiences numerous repeated small earthquakes, it is a natural laboratory for studying the physics of earthquakes.Seismograms from small earthquakes revealed that within the fault zone there were areas of fluid-filled fractures. What caught the researchers' attention was that these areas shifted slightly from time to time. The repeating earthquakes also became smaller and more frequent during these intervals  an indication of a weakened fault."Movement of the fluid in these fractures lubricates the fault zone and thereby weakens the fault," says Niu. "The total displacement of the fluids is only about 10 meters at a depth of about three kilometers, so it takes very sensitive seismometers to detect the changes, such as we have at Parkfield."What caused the fluids to shift? Intriguingly, the researchers noticed that on two occasions the shifts came after the fault zone was disturbed by seismic waves from large, distant earthquakes, such as the 2004 Sumatra-Andaman Earthquake. Pressure from these waves may have been enough to cause the fluids to flow. "So it is possible that the strength of faults and earthquake risk is affected by seismic events on the other side of the world," says Niu.The paper is published in the October 1 edition of Nature.*Paul Silver died tragically in an automobile accident in August.The Carnegie Institution (www.CIW.edu) has been a pioneering force in basic scientific research since 1902. It is a private, nonprofit organization with six research departments throughout the U.S. Carnegie scientists are leaders in plant biology, developmental biology, astronomy, materials science, global ecology, and Earth and planetary science. 
maria.callier@afosr.af.milResearch continues on secure, mobile, quantum communications Researcher Dr. David H. Hughes of the Air Force Research Laboratory in Rome, N.Y. is leading a team investigating long-distance, mobile optical links imperative for secure quantum communications capabilities in theater. Hughes and his Air Force Office of Scientific Research-funded team have conducted high data-rate experiments using an optical laser link, a tool which exploits the quantum noise of light for higher security. The system uses adaptive optics for transmission of high data-rate video and audio signals over long distances. AOptix Technologies, a developer of ultra-high bandwidth laser communication solutions for government and commercial markets has joined forces with AFOSR and AFRL to conduct flight tests at 10,000 feet to evaluate the performance of the high-altitude, air-to-ground, quantum communications links. Up to this point, the challenge with free space optical links, which use fiber optics for transmission have been the turbulence or distortions from temperature differences that cause motion or wind in the atmosphere. "When you transmit information through turbulence (motion in the atmosphere caused by turbulent cells or "wind") it's distorted just like the information coming from the light reflected off a distant, twinkling star to your eye. It's fuzzy," said Hughes. "You have to overcome that by using adaptive optics to rectify the distortion and get a better quality signal." As of right now, Hughes and his team have established an optical link without distortion in test situations at a distance of 35 kilometers in both stationary and flight situations. The next flight test will aim for increased altitudes to demonstrate further air-to-ground distances. "If we can now put one link on the ground and one on a demo aircraft, it wouldn't take much to apply the technology to operational aircraft for the Air Force," said Hughes. "This new capability may even save lives because it will enable the military to access ultra-high bandwidth ISR (intelligence, surveillance reconnaissance) information in real-time from various manned and unmanned airborne platforms," said Dean Senner, President & CEO of AOptix Technologies. ABOUT AFOSR: The Air Force Office of Scientific Research, located in Arlington, Virginia, continues to expand the horizon of scientific knowledge through its leadership and management of the Air Force's basic research program. As a vital component of the Air Force Research Laboratory, AFOSR's mission is to discover, shape, and champion basic science that profoundly impacts the future Air Force. ABOUT AFRL: The Air Force Research Laboratory is the heart of science and technology for the United States Air Force. AFRL is responsible for developing the systems crucial to aerospace superiority. With a workforce of approximately 9,600 people, the laboratory's wealth of talented individuals help AFRL lead science and technology development through in-house and contractual programs. Additionally, the laboratory outsources approximately 75% of its budget to industry, academia, and the international community - leveraging the world's knowledge to provide the most innovative science and technology to the Air Force. ABOUT AOPTIX TECHNOLOGIES: AOptix Technologies is a privately funded company founded in 2000. With core technology expertise in the application of advanced adaptive optics, they develop free space optical communications and iris biometrics based identification solutions for both government and commercial markets. For additional information, please see www.aoptix.com. 
evelyn.brown@nist.govSeptember PerMIS workshop takes measure of intelligent system performanceIntelligent machines of the future will turn custom designs into finished products quickly and efficiently, save lives in catastrophes, and minimize complications caused by surgeries. Researchers involved in advancing artificial intelligence in robots and other systems will gather Sept. 21-23 at the National Institute of Standards and Technology (NIST) in Gaithersburg, Md. to attend the ninth annual Performance Metrics for Intelligent Systems (PerMIS'09) workshop.Robots with intelligence have to be able to sense, perceive, reason, learn and adapt. Other intelligent systems may have other abilities, for instance, analyzing videotapes to detect suspicious behaviors in individuals within a crowd. The annual PerMIS workshop is the only one of its kind dedicated to defining measures and methodologies to evaluate performance of intelligent systems. PerMIS'09 addresses the question "Does performance measurement accelerate the pace of advancement for intelligent systems?" The workshop focuses on application of performance measures to practical problems in sectors such as industrial, medical, emergency response, transportation, homeland security and defense.PerMIS is an excellent forum for sharing lessons learned and discussions as well as fostering collaborations between researchers and practitioners from industry, academia and government agencies, according to PerMIS General Chair Elena Messina.Notable plenary speakers include Tom Mitchell of Carnegie Mellon University whose research in machine learning methods and brain imaging was recently featured on CBS's 60 Minutes, David Bruemmer of Idaho National Laboratory discussing robots and threat detection, Ben Kuipers of University of Michigan, Ann Arbor, describing a cognitive mapper for mobile robots, Paul Cohen of the University of Arizona arguing against sophistication in performance assessments, and Raffaello D'Andrea of ETH Zurich talking about his plan to use hundreds of networked, autonomous mobile robots for order fulfillment in warehousing.More information about the workshop can be found at www.isd.mel.nist.gov/PerMIS_2009/. For registration information, see www.nist.gov/public_affairs/confpage/090921.htm. For special inquiries, please contact Elena Messina at elena.messina@nist.gov or Raj Madhavan at raj.madhavan@nist.gov. Journalists interested in covering the meeting should contact Evelyn Brown, evelyn.brown@nist.gov.
wu@eng.auburn.eduDenial of service denialNew filtering system could protect networks from zombiesA way to filter out denial of service attacks on computer networks, including cloud computing systems, could significantly improve security on government, commercial, and educational systems. Such a filter is reported in the Int. J. Information and Computer Security by researchers from Auburn University in Alabama.Denial of Service (DoS) and distributed Denial of Service (DDoS) attacks involve an attempt to make a computer resource unavailable to its intended users. This may simply be for malicious purposes as is often the case when big commercial or famous web sites undergo a DDoS attack. However, it is also possible to exploit the system's response to such an attack to break system firewalls, access virtual private networks, and to access other private resources. A DoS attack can also be used to affect a complete network or even a whole section of the Internet.Commonly, attack involves simply saturating the target machine with external internet requests. In the case of a DDoS attack the perpetrator recruits other unwitting computers into a network and uses a multitude of machines to mount the attack. The result is that the resource, whether it is a website, an email server, or a database, cannot respond to legitimate traffic in a timely manner and so essentially becomes unavailable to users.Methods for configuring a network to filter out known DoS attack software and to recognize some of the traffic patterns associated with a mounting DoS attack are available. However, current filters usually rely on the computer being attacked to check whether or not incoming information requests are legitimate or not. This consumes its resources and in the case of a massive DDoS can compound the problem.Now, computer engineers John Wu, Tong Liu, Andy Huang, and David Irwin of Auburn University have devised a filter to protect systems against DoS attacks that circumvents this problem by developing a new passive protocol that must be in place at each end of the connection: user and resource.Their protocol - Identity-Based Privacy-Protected Access Control Filter (IPACF) - blocks threats to the gatekeeping computers, the Authentication Servers (AS), and so allows legitimate users with valid passwords to access private resources.The user's computer has to present a filter value for the server to do a quick check. The filter value is a one-time secret that needs to be presented with the pseudo ID. The pseudo ID is also one-time use. Attackers cannot forge either of these values correctly and so attack packets are filtered out.One potential drawback of the added layer of information transfer required for checking user requests is that it could add to the resources needed by the server. However, the researchers have tested how well IPACF copes in the face of a massive DDoS attacks simulated on a network consisting of 1000 nodes with 10 gigabits per second bandwidth. They found that the server suffers little degradation, negligible added information transfer delay (latency) and minimal extra processor usage even when the 10 Gbps pipe to the authentication server is filled with DoS packets. Indeed, the IPACF takes just 6 nanoseconds to reject a non-legitimate information packet associated with the DoS attack."Modelling and simulations for Identity-Based Privacy-Protected Access Control Filter (IPACF) capability to resist massive denial of service attacks" in Int. J. Information and Computer Security, 2009, 3, 195-223
lcyarris@lbl.govHyper-SAGE boosts remote MRI sensitivityDetection of ultra-low concentrations of cancers and other clinical targetsA new technique in Magnetic Resonance Imaging dubbed "Hyper-SAGE" has the potential to detect ultra low concentrations of clincal targets, such as lung and other cancers. Development of Hyper-SAGE was led by one of the world's foremost authorities on MRI technology, Alexander Pines, a chemist who holds joint appointments with the Lawrence Berkeley National Laboratory (Berkeley Lab) and the University of California, Berkeley. The key to this technique is xenon gas that has been zapped with laser light to "hyperpolarize" the spins of its atomic nuclei so that most are pointing in the same direction."By detecting the MRI signal of dissolved hyperpolarized xenon after the xenon has been extracted back into the gas phase, we can boost the signal's strength up to 10,000 times," Pines says. "It is absolutely amazing because we're looking at pure gas and can reconstruct the whole image of our target. With this degree of sensitivity, Hyper-SAGE becomes a highly promising tool for in vivo diagnostics and molecular imaging."MRI is a painless and radiation-free means of obtaining high quality three-dimensional tomographical images of internal tissue and organs. It is especially valuable for optically opaque samples, such as blood. However, the application of MRI to biomedical samples has been limited by sensitivity issues. For the past three decades, Pines has led an on-going effort to find ways of enhancing the sensitivity of MRI and its sister technology, nuclear magnetic resonance (NMR) spectroscopy. Hyper-SAGE, the latest development, represents a significant new advance for both technologies, according to Xin Zhou, a member of Pines' research group."Hyper-SAGE is a totally novel way to amplify a solvated xenon MRI/NMR signal in that instead of a chemical process, which is what previous signal enhancement techniques relied upon, it is a physical process," says Zhou. "Because gas can be physically compressed, the density of information-carrying polarized gas in our detection chamber can be much greater than the density of an information-carrying solution. This means we can detect MRI signals from concentrations of molecules many thousands of times smaller than can be detected with conventional MRI."Zhou is the first author on a paper that is now available online in the Proceedings of the National Academy of Sciences (PNAS). The paper is entitled: "Hyperpolarized Xenon NMR and MRI Signal Amplification by Gas Extraction." Co-authoring the paper with Zhou and Pines was Dominic Graziani. All hold joint appointments with Berkeley Lab's Materials Sciences Division and UC Berkeley's Chemistry Department, where Pines serves as the Glenn T. Seaborg Professor of Chemistry.So Powerful and Yet so WeakThe great contradiction about MRI/NMR spectroscopy is that for being two of the most powerful tools we have today for studying the chemical composition and structure of a sample, they are based on a stunningly weak signal. Both depend upon atomic nuclei that have an unpaired proton or neutron. Such nuclei spin on an axis like miniature tops, giving rise to a magnetic moment - meaning the nuclei act like magnets with a north and south pole. When exposed to an external magnetic field, these spinning "bar magnets" attempt to align their axes along the lines of magnetic force. Since the alignment is not exact, the result is a wobbling rotation, or "precession," that's unique to each type of atom.If, while exposed to the magnetic field, the precessing nuclei are also hit with a radiofrequency pulse, they will absorb and re-emit energy at specific frequencies according to their rate of precession (NMR). When the rf pulse is combined with magnetic field gradients a spatially encoded signal is produced that can be detected and translated into three-dimensional images (MRI).Obtaining an MRI signal from a sample depends upon the spins of its precessing nuclei being polarized so that an excess point either "up" or "down." MRI's inherent weakness stems from the fact that the natural excess of up versus down spins for any typical population of atomic nuclei in a sample is only about one in 100,000. For this reason, conventional MRI techniques are designed to detect nuclei that are highly abundant in tissue, usually the protons in water. In addition, clinicians use contrasting agents to induce detectable changes in the MRI signal from a sample that can reveal the presence of anomalies. However, the sensitivity is usually too low for molecular imaging, which is needed in cancer detection, for example, where the earliest detections generally produce the most favorable outcomes.Enter Hyper-SAGEPines and his research group have developed numerous ways of increasing the sensitivity of MRI technology and expanding its applicability. Previous work showed that xenon, an inert gas whose nuclei naturally feature a tiny degree of spin polarization, can be hyperpolarized with laser light to produce a population of xenon atoms in which nearly five out of every 10 nuclei - instead of one out of every 100,000 - produce an MRI signal. Pines and his group also showed that xenon can be incorporated into a biosensor and linked to specific proteins or other biological molecules to produce spatial images of a chosen molecular or cellular target. The new technique, Hyper-SAGE, for "hyperpolarized xenon signal amplification by gas extraction," offers other major advantages over conventional MRI/NMR techniques in addition to a signal that is up to 10,000 times stronger than previous signals, according to Zhou."Xenon gas has an intrinsically long relaxation time, greater than 45 minutes, which means the signal lasts long enough for us to collect all the encoded information, which in turn can enable us to detect specific targets, such as cancer-related proteins, at micromolar or parts per million concentrations," he says. "Also, Hyper-SAGE utilizes  remote detection, meaning the signal encoding and  detection processes are physically separated and carried out independently. This is a plus for imaging the lung, for example, where the signal of interest would occupy only a small portion of the traditional MRI signal receiver."In their PNAS paper, Zhou, Graziani and Pines describe the successful testing of the Hyper-SAGE technique on a pair of membranes that mimicked the function of the lungs. Hyper-polarized xenon was dissolved in solution in one membrane to mimic inhalation, and was then extracted as a gas for detection from the other membrane to represent exhalation.Explains Zhou, "In a clinical setting, a patient would inhale the hyperpolarized xenon gas which would be  dissolved in the blood and allowed to flow into the body and brain. The exhaled xenon gas would then be collected and its MRI signal would be detected. Used in combination with a target-specific xenon biomolecular sensor, we should be able to study the gas-exchange in the lung and detect cancerous cells at their earliest stage of development."	This research was supported by the U.S. Department of Energy's Office of Science, through its Basic Energy Sciences programs.Berkeley Lab is a U.S. Department of Energy national laboratory located in Berkeley, California.  It conducts unclassified scientific research for DOE's Office of Science and is managed by the University of California. Visit our Website at www.lbl.gov/
spolowc@clemson.eduClemson facial recognition research advancesIt often takes a pristine look at the iris to pass through some security systems. Today with the help of the Clemson University Image and Video Analysis Lab, systems may just need a wrinkle to verify identity.The lab is working with a $2 million grant from the Office of the Director of National Intelligence and has joined with other universities to create the Center of Academic Studies in the Identification Sciences (CASIS). The aim of the center is to strengthen biometric identification, the measurement of physical characteristics to confirm a person's identity.At Clemson, School of Computing assistant professor Damon Woodard has led the way in developing periocular- and iris-recognition techniques, which can achieve high performance when using less-than-ideal quality data. "Historically, we have looked at the iris for identification, but what if the iris is obstructed when the subject blinks? We then can look at eye region features such as skin folds and wrinkles for more accurate identification along with the iris," said Woodard. "Zeroing in on these features allows for the identification of individuals when a large portion of the face is obstructed or unclear. When we can combine the iris with periocular features, the result is a more accurate system."Woodard believes the techniques could open up additional possibilities for applications that require the identification of an individual, including airport, border and home security. 	Under the five-year grant, which totals $8.9 million, Clemson is working with researchers from Carnegie Mellon University, North Carolina A&T State University and the University of North Carolina at Wilmington.
aem1@psu.edu4 from Penn State receive PECASE awardsThe White House announced today (July 9) that four Penn State researchers will receive 2009 Presidential Early Career Awards for Scientists and Engineers (PECASE).Sean Hallgren and Adam D. Smith, both assistant professors in computer science and engineering; Michael A. Hickner, assistant professor of material science and engineering, and Susan E. Parks, assistant professor of acoustics and research associate, Applied Research Laboratory where among 100 named by the White House to receive this highest honor presented to beginning scientists or engineers in the U.S. They will be recognized at a future ceremony at the White House.The PECASE program was established in 1996 to identify and honor outstanding researchers who are beginning their independent research careers, and to provide recognition of their potential for leadership across the frontiers of scientific knowledge during the 21st century. Recipients are nominated by the National Science Foundation, NASA and the Departments of Health and Human Services, Defense, Energy, Agriculture, Education, Commerce, Veterans Affairs. The NSF nominated two of Penn State's recipients, Hallgren and Smith, from the recipients of NSF's 2008 Faculty Early Career Development Program (CAREER) awards. NSF nominates 20 of the PECASE recipients.Hallgren works in the area of quantum computation, which aims to use quantum mechanical systems for computation. Quantum computers can break widely used cryptosystems, including those used to protect e-commerce transactions. His work aims to find new applications for quantum computers and to determine which cryptosystems are secure against them.Smith studies cryptography and information privacy and their connection to such diverse fields as quantum mechanics, combinatorics, information theory and statistics. He looks at preserving privacy in the publication of statistical data, cryptography based on noisy secrets and quantum cryptography. His CAREER award focuses on the problems stemming from conflicts between data access and privacy in collections of personal and sensitive data such as census surveys, social networks and public health data. His work addresses the need for formal privacy guarantees that remain meaningful even against an intruder with partial knowledge of the sensitive data.Hickner and Parks were among the 41 recipients nominated by the Department of Defense.Hickner's research interests include polymer chemistry, polymer micro- and nano-structure, transport characterization, electrochemistry and new materials for energy applications. His work is motivated by application-specific needs that drive fundamental investigations into new materials chemistry and demand incisive measurements of the structure and transport properties of novel materials. He characterizes technologically important materials and synthesizes model materials systems to probe specific structural and property questions. Parks' primary research interest is in bioacoustics, integrating the fields of biological oceanography, behavioral ecology and physiology to address questions related to acoustic communication. She studies the use of sound for communication, hearing abilities, and the impacts of noise on both sound production and reception. Her current research focuses on the use of sound by the North Atlantic right whale, studying behavioral aspects of sound production, perceptual abilities and impacts of noise on acoustic communication.
evelyn.brown@nist.govNIST develops experimental validation tool for cell phone forensicsViewers of TV dramas don't focus on the technology behind how a forensics crime team tracks a terrorist or drug ring using cell phone data, but scientists at the National Institute of Standards and Technology (NIST) do. NIST researchers have developed a new technique aimed at improving the validation of a crime lab's cell phone forensics tools. Early experiments show promise for easier, faster and more rigorous assessments than with existing methods.Cell phones reveal much about our daily communicationsthe who, when and what of our calls and texts. A small chip card within most phones, called an identity module, stores this and other data for a subscriber. A subscriber identity module (SIM) accommodates phonebook entries, recently dialed numbers, text messages and cellular carrier information. Forensic examiners use off-the-shelf software tools to extract the data, allowing them to "connect the dots" in a criminal case such as identifying affiliations or detecting mobile phone activity around the time of an event.But for this information to be used as evidence in court or other formal proceedings, the software tools that forensic teams employ are normally validated to determine suitability for use. Currently, preparing test materials for assessing cell phone tools is labor intensive and may require learning new command languages to perform the process.NIST scientists detail their proof-of-concept research in a NIST Interagency Report, Mobile Forensic Reference Materials: A Methodology and Reification (available online at http://csrc.nist.gov/publications/nistir/ir7617/nistir-7617.pdf.) They also developed an experimental application, called SIMfill, and a preliminary test dataset that follows the methodology described in the report. SIMfill can be used to automatically upload cell phone data such as phone numbers and text messages to "populate" test SIMs that can then be recovered by forensic cell phone tools. In this way, examiners can use SIMfill as one method to assess the quality of their off-the-shelf tool.The SIMfill software and dataset may be downloaded for free at http://csrc.nist.gov/groups/SNS/mobile_security/mobile_forensics_software.html."In this report," explains coauthor Wayne Jansen, "we document the results of a recent experiment with a number of commonly used mobile phone forensics tools. No tool was found to work perfectly and some worked poorly on fairly simple test cases."The automated features of the applications and XML representation of test data allow technicians to develop new test cases easily. This offers a simple alternative to using manual means or specialized tools with higher learning curves. The data can be adapted to different languages with alternate character sets."Our research was a proof of concept," Jansen says. "Hopefully, forensic examiners will use our work to validate mobile forensics tools thoroughly before they employ them." The next step in the research is open. Scientists could expand the technique for mobile handsets and other types of identity modules, or the forensic community could decide to adopt this dataset and application as an open source project, according to Jansen.
knroark@lanl.govScientists use world's fastest supercomputer to model origins of the unseen universe1 of the largest-ever computer models explores dark matter and dark energy, 2 cosmic constituents that remain a mysteryLOS ALAMOS, New Mexico, October 26, 2009 Understanding dark energy is the number one issue in explaining the universe, according to Salman Habib, of the Laboratory's Nuclear and Particle Physics, Astrophysics and Cosmology group."Because the universe is expanding and at the same time accelerating, either there is a huge gap in our understanding of physics, or there is a strange new form of matter that dominates the universe  'dark energy'  making up about 70 percent of it," said Habib.  "In addition, there is five times more of an unknown 'dark matter' than there is ordinary matter in the universe, and we know it's there from many different observations, most spectacularly, we've seen it bend light in pictures from the Hubble Space Telescope, but its origin is also not understood."Even though it's looking at only a small segment of the "accessible" universe, Habib's "Roadrunner Universe" model requires a petascale computer because, like the universe, it's mind-bendingly large.  The model's basic unit is a particle with a mass of approximately one billion suns (in order to sample galaxies with masses of about a trillion suns), and it includes 64 billion and more of those particles.The model is one of the largest simulations of the distribution of matter in the universe, and aims to look at galaxy-scale mass concentrations above and beyond quantities seen in state-of-the-art sky surveys. "We are trying to really understand how to more completely and more accurately describe the observable universe, so we can help in the design of future experiments and interpret observations from ongoing observations like the Sloan Digital Sky Survey-III. We are particularly interested in the Large Synoptic Survey Telescope (LSST) in Chile, in which LANL is an institutional member, and DOE and NASA's Joint Dark Energy Mission (JDEM)," said Habib.  "To do the science in any sort of reasonable amount of time requires a petascale machine at the least."The Roadrunner Universe model relies on a hierarchical grid/particle algorithm that best matches the physical aspects of the simulation to the hybrid architecture of Roadrunner.  Habib and his team wrote an entirely new computer code that aggressively exploits Roadrunner's hybrid architecture and makes full use of the PowerXCell 8i computational accelerators. They also created a dedicated analysis and visualization software framework to handle the huge simulation database."Our effort is aimed at pushing the current state of the art by three orders of magnitude in terms of computational and scientific throughput," said Habib.  I'm confident the final database created by Roadrunner will be an essential component of dark universe science for years to come."	About Roadrunner, the world's fastest supercomputer, first to break the petaflop barrierOn Memorial Day, May 26, 2008, the "Roadrunner" supercomputer exceeded a sustained speed of 1 petaflop/s, or 1 million billion calculations per second.  "Petaflop/s" is computer jargonpeta signifying the number 1 followed by 15 zeros (sometimes called a quadrillion) and flop/s meaning "floating point operation per second."  Shortly after that it was named the world's fastest supercomputer by the TOP500 organization at the June 2008 International Supercomputing Conference in Dresden Germany.The Roadrunner supercomputer, developed by IBM in partnership with the Laboratory and the National Nuclear Security Administration, will be used to perform advanced physics and predictive simulations in a classified mode to assure the safety, security, and reliability of the U.S. nuclear deterrent.  The system will be used by scientists at the NNSA's Los Alamos, Sandia, and Lawrence Livermore national laboratories.The secret to its record-breaking performance is a unique hybrid design. Each compute node in this cluster consists of two AMD Opteron dual-core processors plus four PowerXCell 8i processors used as computational accelerators. The accelerators used in Roadrunner are a special IBM-developed variant of the Cell processor used in the Sony PlayStation 3®. The node-attached Cell accelerators are what make Roadrunner different than typical clusters.Roadrunner is still currently the world's fastest with a speed of 1.105 petaflop/s per second, according to the TOP500 announcement at the November 2008 Supercomputing Conference in Austin Texas, and it again retained the #1 position at the June ISC09 conference. About Los Alamos National Laboratory (www.lanl.gov) Los Alamos National Laboratory, a multidisciplinary research institution engaged in strategic science on behalf of national security, is operated by Los Alamos National Security, LLC, a team composed of Bechtel National, the University of California, The Babcock & Wilcox Company, and the Washington Division of URS for the Department of Energy's National Nuclear Security Administration.Los Alamos enhances national security by ensuring the safety and reliability of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.
matt_shipman@ncsu.eduHooks hijacked? New research shows how to block stealthy malware attacksThe spread of malicious software, also known as malware or computer viruses, is a growing problem that can lead to crashed computer systems, stolen personal information, and billions of dollars in lost productivity every year. One of the most insidious types of malware is a "rootkit," which can effectively hide the presence of other spyware or viruses from the user  allowing third parties to steal information from your computer without your knowledge. But now researchers from North Carolina State University have devised a new way to block rootkits and prevent them from taking over your computer systems.To give some idea of the scale of the computer malware problem, a recent Internet security threat report showed a 1,000 percent increase in the number of new malware signatures extracted from the in-the-wild malware programs found from 2006 to 2008. Of these malware programs, "rootkits are one of the stealthiest," says Dr. Xuxian Jiang, assistant professor of computer science at NC State and a co-author of the research. "Hackers can use rootkits to install and hide spyware or other programs. When you start your machine, everything seems normal but, unfortunately, you've been compromised."Rootkits typically work by hijacking a number of "hooks," or control data, in a computer's operating system. "By taking control of these hooks, the rootkit can intercept and manipulate the computer system's data at will," Jiang says, "essentially letting the user see only what it wants the user to see." As a result, the rootkit can make itself invisible to the computer user and any antivirus software. Furthermore, the rootkit can install additional malware, such as programs designed to steal personal information, and make them invisible as well.In order to prevent a rootkit from insinuating itself into an operating system, Jiang and the other researchers determined that all of an operating system's hooks need to be protected. "The challenging part is that an operating system may have tens of thousands of hooks  any of which could potentially be exploited for a rootkit's purposes," Jiang says, "Worse, those hooks might be spread throughout a system. Our research leads to a new way that can protect all the hooks in an efficient way, by moving them to a centralized place and thus making them easier to manage and harder to subvert."Jiang explains that by placing all of the hooks in one place, researchers were able to simply leverage hardware-based memory protection, which is now commonplace, to prevent hooks from being hijacked. Essentially, they were able to put hardware in place to ensure that a rootkit cannot modify any hooks without approval from the user.	The research, "Countering Kernel Rootkits with Lightweight Hook Protection," will be presented at the 16th ACM Conference on Computer and Communications Security in Chicago, Nov. 12. The study's co-authors are Jiang, Dr. Peng Ning, associate professor of computer science at NC State, NC State Ph.D. student Zhi Wang and Weidong Cui of Microsoft Research.
laura.ost@nist.govNIST demonstrates 'universal' programmable quantum processorBOULDER, Colo. Physicists at the National Institute of Standards and Technology (NIST) have demonstrated the first "universal" programmable quantum informationprocessor able to run any program allowed by quantum mechanicsthe rules governing the submicroscopic worldusing two quantum bits (qubits) of information. The processor could be a module in a future quantum computer, which theoretically could solve some importantproblems that are intractable today.The NIST demonstration, described in Nature Physics,* marks the first time any research group has moved beyond demonstrating individual tasks for a quantum processoras done previously at NIST and elsewhereto perform programmable processing, combining enough inputs and continuous steps to run any possible two-qubit program.The NIST team also analyzed the quantum processor with the methods used in traditional computer science and electronics by creating a diagram of the processing circuit and mathematically determining the 15 different starting values and sequences of processing operations needed to run a given program. "This is the first time anyone has demonstrated a programmable quantum processor for more than one qubit," says NIST postdoctoral researcher David Hanneke, first author of the paper. "It's a step toward the big goal of doing calculations with lots and lots of qubits. The idea is you'd have lots of these processors, and you'd link them together."The NIST processor stores binary information (1s and 0s) in two beryllium ions (electrically charged atoms), which are held in an electromagnetic trap and manipulatedwith ultraviolet lasers. Two magnesium ions in the trap help cool the beryllium ions.NIST scientists can manipulate the states of each beryllium qubit, including placing the ions in a "superposition" of both 1 and 0 values at the same time, a significant potential advantage of information processing in the quantum world. Scientists also can "entangle"the two qubits, a quantum phenomenon that links the pair's properties even when the ionsare physically separated.With these capabilities, the NIST team performed 160 different processing routines on the two qubits. Although there are an infinite number of possible two-qubit programs, this set of 160 is large and diverse enough to fairly represent them, Hanneke says, making the processor "universal." Key to the experimental design was use of a random number generator to select the particular routines that would be executed, so all possible programs had an equal chance of selection. This approach was chosen to avoid bias in testing theprocessor, in the event that some programs ran better or produced more accurate outputs than others.Ions are among several promising types of qubits for a quantum computer. If they can be built, quantum computers have many possible applications such as breaking today's most widely used encryption codes, such as those that protect electronic financialtransactions. In addition to its possible use as a module of a quantum computer, the newprocessor might be used as a miniature simulator for interactions in any quantum system that employs two energy levels, such as the two-level ion qubit systems that representenergy levels as 0s and 1s. Large quantum simulators could, for example, help explain themystery of high-temperature superconductivity, the transmission of electricity with zero resistance at temperatures that may be practical for efficient storage and distribution of electric power.The new paper is the same NIST research group's third major paper published this year based on data from experiments with trapped ions. They previously demonstratedsustained quantum information processing (http://www.nist.gov/public_affairs/releases/ion_trap_computers080609.html) and entanglement in a mechanical system similar tothose in the macroscopic everyday world (http://www.nist.gov/public_affairs/releases/jost/jost_060309.html). NIST quantum computing research contributes toadvances in national priority areas, such as information security, as well as NIST mission work in precision measurement and atomic clocks.In the latest NIST experiments reported in Nature Physics, each program consisted of 31 logic operations, 15 of which were varied in the programming process. A logicoperation is a rule specifying a particular manipulation of one or two qubits. In traditional computers, these operations are written into software code and performed by hardware.The programs did not perform easily described mathematical calculations. Rather, they involved various single-qubit "rotations" and two-qubit entanglements. As an exampleof a rotation, if a qubit is envisioned as a dot on a sphere at the north pole for 0, at the south pole for 1, or on the equator for a balanced superposition of 0 and 1, the dot might be rotated to a different point on the sphere, perhaps from the northern to the southern hemisphere, making it more of a 1 than a 0.Each program operated accurately an average of 79 percent of the time across 900 runs, each run lasting about 37 milliseconds. To evaluate the processor and the quality of its operation, NIST scientists compared the measured outputs of the programs to idealized, theoretical results. They also performed extra measurements on 11 of the 160 programs, to more fully reconstruct how they ran and double-check the outputs.As noted in the paper, many more qubits and logic operations will be required to solve large problems. A significant challenge for future research will be reducing the errorsthat build up during successive operations. Program accuracy rates will need to be boosted substantially, both to achieve fault-tolerant computing and to reduce thecomputational "overhead" needed to correct errors after they occur, according to the paper.As a non-regulatory agency of the U.S. Department of Commerce, NIST promotes U.S. innovation and industrial competitiveness by advancing measurement science,standards and technology in ways that enhance economic security and improve our quality of life.*D. Hanneke, J.P. Home, J.D. Jost, J.M. Amini, D. Leibfried & D.J. Wineland. 2009. Realization of aprogrammable two-qubit quantum processor. Nature Physics. Posted online Nov. 15.
joyann.callender@utoronto.caLight sensor breakthrough could enhance digital camerasNew research by a team of University of Toronto scientists could lead to substantial advancements in the performance of a variety of electronic devices including digital camerasTORONTO, ON  New research by a team of University of Toronto scientists could lead to substantial advancements in the performance of a variety of electronic devices including digital cameras. The paper appears in the June 19 edition of the journal Science.Researchers created a light sensor  like a pixel in a digital camera  that benefits from a phenomenon known as multi-exciton generation (MEG). Until now, no group had collected an electrical current from a device that takes advantage of MEG. "Digital cameras are now universal, but they suffer from a major limitation: they take poor pictures under dim light. One reason for this is that the image sensor chips inside cameras collect, at most, one electron's worth of current for every photon (particle of light) that strikes the pixel," says Ted Sargent, professor in U of T's Department of Electrical and Computer Engineering. "Instead generating multiple excitons per photon could ultimately lead to better low-light pictures."In solar cells and digital cameras, particles of light - known as photons - are absorbed in a semiconductor, such a silicon, and generate excited electrons, known as excitons. The semiconductor chip then measures a current that flows as a result. Normally, each photon is converted into at most one exciton. This lowers the efficiency of solar cells and it limits the sensitivity of digital cameras. When a scene is dimly lit, small portable cameras like those in laptops suffer from noise and grainy images as a result of the small number excitons. "Multi-exciton generation breaks the conventional rules that bind traditional semiconductor devices," says Sargent. "This finding shows that it's more than a fascinating concept: the tangible benefits of multiple excitons can be seen in a light sensor's measured current."The research was supported by grants from the King Abdullah University of Science and Technology, the Natural Sciences and Engineering Research Council of Canada, the Canada Research Chairs, and the Canada Foundation for Innovation and the Ontario Innovation Trust.
jwilliams@arvo.orgPerceptual learning relies on local motion signals to learn global motionStudy shows perceptual learning of global pattern motionRockville, MD   Researchers have long known of the brain's ability to learn based on visual motion input, and a recent study has uncovered more insight into where the learning occurs.The brain first perceives changes in visual input (local motion) in the primary visual cortex. The local motion signals are then integrated in the later visual processing stages and interpreted as global motion in the higher-level processes. But when subjects in a recent experiment using moving dots were asked to detect global motion (the overall direction of the dots moving together), the results show that their learning relied on more local motion processes (the movement of dots in small areas) than global motion areas."We had expected that higher-level processing could be more involved in task-relevant perceptual learning investigated in this study," said Dr. Shigeaki Nishina who conducted the research in Boston University and now belongs to the Honda Research Institute Japan. "Contrary to the expectation, the result suggested local motion signals are predominantly used for task-relevant perceptual learning of global motion, which was surprising to us."Nishina said the results, which appear in the latest issue of Journal of Vision (http://www.journalofvision.org/9/9/15/) show that the improvement in detection of global motion is not due to learning of the global motion but to learning of local motion of the moving dots in the test.The researchers said the study of perceptual learning can give scientists deeper insight not only about sensory systems but also the whole brain's adaptable nature. "This line of study could give a guideline for optimizing human machine interface," said Nishina. "When we use a new machine, we need to learn how to get information from the machine. In our study, local motion signals were more important for the brain to learn a task based on global motion. This suggests that the optimal information for efficient learning could be different from the visual information that is directly related to the task to be learned."In addition, Nishina said the new understanding of where the brain processes task-relevant perceptual learning can lead to further understanding of how a brain makes decisions based on sensory input."We expect that our results will help the understanding of decision-making process and constructing a more concrete model of the process," he said.The Journal of Vision is an online-only, peer-reviewed, open-access publication devoted to visual function in humans and animals. It is published by the Association for Research in Vision and Ophthalmology. It explores topics such as spatial vision, perception, low vision, color vision and more, spanning the fields of neuroscience, psychology and psychophysics. JOV is known for hands-on datasets and models that users can manipulate online.The Association for Research in Vision and Ophthalmology (ARVO) is the largest eye and vision research organization in the world. Members include more than 12,500 eye and vision researchers from over 80 countries. The Association encourages and assists research, training, publication and knowledge-sharing in vision and ophthalmology. For more information, visit www.arvo.org.
duaneh@aiaa.orgNew AIAA book highlights stealth technology, development of Lockheed blackbirdOctober 19, 2009  Reston, Va.  The American Institute of Aeronautics and Astronautics (AIAA) announces the publication of a new book, "From RAINBOW to GUSTO: Stealth and the Design of the Lockheed Blackbird." Written by Paul A. Suhler, the new work is a part of AIAA's Library of Flight series.Based on recently declassified CIA documents, Lockheed and Convair photographs, reports, and technical drawings, as well as oral histories, memoirs, and interviews of the scientists and engineers involved, the book examines the development of stealth technology for America's reconnaissance aircraft from the U-2 program to the Lockheed A-12 Blackbird. It recounts the evolution of stealth technology development from the earliest days of Project RAINBOW, the attempt to make the U-2 reconnaissance aircraft invisible to Soviet radar, to the success of 1958's Project GUSTO, in which Lockheed and Convair competed to develop the breakthroughs in stealth technology that made possible the development of the A-12 Blackbird. Paul A. Suhler is a computer engineer in the data storage industry. A specialist in communication protocols and computer security, he has participated extensively in the development of computer standards, and received the 2009 Technical Excellence Award from the InterNational Committee for Information Technology Standards. A former U.S. Army officer, Suhler has appeared on the PBS documentary "Warplane."  He holds a Ph.D. in computer engineering from the University of Texas at Austin, and an M.S. in computer engineering from the University of California at Berkeley.(Published by AIAA, 2009, 284 pages, Hardback. ISBN: 978-1-60086-712-5; List Price: $39.95)Review copy requests may be e-mailed to janices@aiaa.org. Orders may be placed online: www.aiaa.org/books; by mail: AIAA Publications Customer Service, P.O. Box 960, Herndon, VA, 20172-0960; by phone: 800.682.2422 or 703.661.1595; or by fax: 703.661.1501.AIAA is the world's largest technical society dedicated to the global aerospace profession.  With more than 35,000 individual members worldwide, and 90 corporate members, AIAA brings together industry, academia, and government to advance engineering and science in aviation, space, and defense.  For more information, visit www.aiaa.org. American Institute of Aeronautics and Astronautics1801 Alexander Bell Drive, Suite 500, Reston, VA  20191-4344Phone: 703.264.7558    Fax: 703.264.7551    www.aiaa.org
molly.lachance@afosr.af.milAir Force invests over $14M for 2010 Young Investigators Research ProgramThe Air Force Office of Scientific Research today announced it will award approximately 14.6 million in grants to 38 scientists and engineers who submitted winning research proposals through the Air Force's Young Investigator Research Program. The YIP is open to scientists and engineers at research institutions across the United States who have received Ph.D. or equivalent degrees in the last five years and show exceptional ability and promise for conducting basic research. The objective of this program is to foster creative basic research in science and engineering, enhance early career development of outstanding young investigators, and increase opportunities for the young investigators to recognize the Air Force mission and the related challenges in science and engineering. According to AFOSR officials, competition for the YIP award is intense. This year AFOSR received 202 proposals in response to the AFOSR broad agency announcement solicitation in major areas of interest to the Air Force. The areas of interest include: aerospace, chemical and material sciences; physics and electronics; and mathematics, information and life sciences. AFOSR officials select proposals based on the evaluation criteria listed in the broad agency announcement. Those selected will receive the grants over a 3 to 5-year period. The recipients and their anticipated research areas are: Dr. Seth R. Bank, The University of Texas, Austin, will work on manipulating the interfacial electrical and optical properties of dissimilar materials with metallic nanostructures. Dr. Mikhail A. Belkin, The University of Texas, Austin, will investigate tunable quantum electronic metamaterials for mid-infrared range. Dr. David L. Burris, University of Delaware, will focus on linking tribofilm nanomechanics to the origins of low friction and wear. Dr. Ross H. Burrows, The University of Alabama, Huntsville, will study space weather effects due to shock acceleration. Dr. Tal Carmon, University of Michigan, will investigate continuous on-chip extreme uv emitter. Dr. Erica L. Corral, The University of Arizona, will work on enabling dynamic oxidation mechanisms in reverse infiltrated ultra-high temperature ceramic coated C-C composites for application in hypersonics. Dr. Xu Du, Stony Brook University, will conduct research on graphene-superconductor junctions and their applications in ultra-high sensitivity bolometers. Dr. Mohamed Y. El-Naggar, University of Southern California, Los Angeles, will investigate biotic-abiotic nanoscale interactions in biological fuel cells. Dr. Thomas Lawlor Griffiths, University of California, Berkley, will focus on fast, flexible, rational inductive inference. Dr. Christopher Scott Hartley, Miami University, will study cross-conjugated nanoarchitectures. Dr. Michael A. Hermele, University of Colorado, Boulder, will research topological states of matter and other novel phases of ultracold alkaline earth fermions. Dr. Tracey Ho, California Institute of Technology, will work on robust network transmission and storage using coding. Dr. Jung-Wuk Hong, Michigan State University, will study coupling of peridynamics and finite element formulation for multiscale simulations. Dr. Kenneth L. Knappenberger, Jr., The Florida State University, will investigate magnetoplasmonic nanomaterials: a route to predictive photocatalytic, light-harvesting and ferrofluidic Properties. Dr. Piyush Kumar, The Florida State University, will work on geometric clustering and its applications. Dr. Jr-Shin Li, Washington University, St. Louis, will conduct research on robust manipulation and computation for inhomogeneous quantum ensembles. Dr. Yang Liu, The University of Texas, Dallas, will work on computational modeling of emotions and affect in social-cultural interaction. Dr. Alan J. H. McGaughey, Carnegie Mellon University, will investigate quantum mechanics-driven prediction of nanostructure thermal conductivity. Dr. Abhay Narayan Pasupathy, Columbia University, will focus on microscopic understanding of device performance in graphne and other low-dimensional quantum materials. Dr. Eric Pop, University of Illinois, Urbana-Champaign, will work on carbon-based avalanche devices for low-power electronics. Dr. Brian C. Odom, Northwestern University, will research on-demand rotational state preparation and molecular quantum logic spectroscopy. Dr. Rafael Pass, Cornell University, will develop new models for protocol security. Dr. Per-Olof Persson, University of California, Berkeley, will work on efficient and robust high-order methods for fluid and solid mechanics. Dr. Stefan F. Preble, Rochester Institute of Technology, will investigate dynamic silicon nanophotonics. Dr. Matthew J. Ringuette, The State University of New York, Buffalo, will work on flapping-wing propulsion characterized using optimal vortex formation. Dr. Tobias Ritter, Harvard College, will focus on synthesis of one-dimensional pd(iii) wires. Dr. Joshua L. Rovey, Missouri University of Science & Technology, will investigate energy conversion and loss processes in heavy gas, field-reversed configuration electric thruster plasma. Dr. R. Mohan Sankaran, Case Western Reserve University, will conduct research on microplasma-assisted dissociation of chemical precursors for nanomaterials synthesis and processing. Dr. Thomas E. Schwartzentruber, University of Minnesota, will investigate internal energy transfer and dissociation model development using accelerated first-principles simulations of hypersonic flow features. Dr. Andrew D. Straw, California Institute of Technology, will focus on quantitative investigation of high level control of free-flight behavior in flies. Dr. Brook O. Swanson, Gonzaga University, will work on discovery of high-performance biomaterials for defense applications. Dr. Ashkan Vaziri, Northeastern University, will study bio-inspired interfaces for hybrid structures. Dr. Michael Walfish, The University of Texas, Austin, will conduct investigation on secure-by-design networks via explicit route control. Dr. Daniel Wasserman, University of Massachusetts, Lowell, will investigate active plasmonics: free space steering and directional control. Dr. Emily A. Weiss, Northwestern University, will focus on electrochemical reduction of CO2 at a TiO2 electrode using quantum dots as multi-electron. Dr. Yiquan Wu, University of Rochester, will research anisotropic laser ceramics and single crystal derivatives. Dr. Eric Poe Xing, Carnegie Mellon University, will investigate socioscape on real-time analysis of dynamic heterogeneous networks in complex socio-cultural systems. Dr. Jingyi Yu, University of Delaware, will develop a new hybrid camera array for tracking and reconstruction under low light. 
david@proofcommunication.comAll digital systems go!New organization launched to help create digital BritainTechnology companies are welcoming a government backed body launched today to help create the new digital society. The Digital Systems Knowledge Transfer Network (KTN) has been set up by the Technology Strategy Board to bring together business, academia, and government to drive technology innovation and create wealth for the UK.The Digital Systems KTN - managed by the National Physical Laboratory, Intellect and QinetiQ - is led by experts in distributed computing, cyber security and satellite navigation. The KTN will create links between organisations to address technology issues such as intelligent transport, cloud computing, smart metering and mobile data access. It will stimulate knowledge sharing and break down business walls that prevent advances in technology. For example it could put academics carrying out the latest research on GPS services in touch with product developers and companies who know how to assure these services are provided in a scalable and secure way on the internet.   The growth in mobile computing, processing power, networking technologies, location-based systems, and internet services is offering us new ways to work, access essential services, socialise, and build relationships. Better access to information means more potential threats to data security and personal privacy. There is a fine balance between using new technology to make our lives easier, and ensuring this is done with appropriate confidentiality and integrity of information. "A successful digital society marries the need to access data on the move with a secure and scalable approach to gathering, processing, and using that information," says Ian Osborne, Director of the Digital Systems KTN. "We will create opportunities to innovate in this field by bringing people together  for example there are many academics making exciting breakthroughs in new tech who don't have the contacts to make them a reality or innovators who need to be connected to funders."Big hitters from UK business have already welcomed the creation of the new KTN. Dr Mike Short, Vice President, Telefonica Europe/O2 believes it will be a critical resource."In a connected world of voice and data we need to focus on pragmatic and secure solutions and digital systems. We need a confluence of skills to make this digital society work effectively across sectors. The Digital Systems KTN will be a key space for sharing knowledge and ideas that will put new innovation into practice."The Digital Systems Knowledge Transfer Network will host conferences and online meetings, provide members with guidance on funding sources and allow them influence on future policy decisions.The Digital Systems KTN builds on the success of three existing knowledge transfer networks  Grid Computing Now!, Cyber Security, and Location and Timing  which have come together to form the new network.Combining innovation and knowledge transfer in these areas creates a critical resource for a converged world where services are accessed anytime, anyplace, anywhere. Notes to EditorsThe Digital Systems KTN is managed by Intellect, QinetiQ, and the National Physical Laboratory, who are jointly responsible for programmes of work in scalable computing (Ian Osborne), cyber security (Tony Dyhouse) and location and timing technologies (Bob Cockshott) respectively. The New KTN will:About The Technology Strategy BoardThe Technology Strategy Board is a business-led executive non-departmental public body, established by the government.  Its role is to promote and support research into, and development and exploitation of, technology and innovation for the benefit of UK business, in order to increase economic growth and improve the quality of life.  It is sponsored by the Department for Business, Innovation and Skills (BIS).  For more information please visit www.innovateuk.org.
spolowc@clemson.eduClemson researchers study energy savings with electric cars and IntelliDrive technologyCLEMSON  Clemson University researchers have been awarded a $470,000 National Science Foundation grant to study making plug-in hybrid electric vehicles (PHEVs) more efficient to reduce fossil fuel use.The researchers will study integrating the vehicles with infrastructure where they act as probes that generate continuous detailed traffic data, such as speed profiles and travel time in cooperation with roadside devices to optimize the total energy used. The purpose is to minimize the total cost of a trip.The hybrid electric vehicles will work with the Vehicle Infrastructure Integration (VII) system, also known as IntelliDrive. It's an initiative that will improve traffic operations and safety in the future by linking vehicles to their physical surroundings. The VII systems can act as probes, providing detailed traffic data of any monitored highway that can be used by roadside sensors, along with data from other sources, to provide predicted trip information to the vehicles."For instance, a roadside sensor could collect data on acceleration and deceleration of the vehicles and use the information to predict operational conditions," said lead investigator Ronnie Chowdhury, associate professor of civil engineering at Clemson.He added that in this case, hybrid electric vehicles would "talk to" sensors that are set up in a road's infrastructure to improve fuel efficiency and overall costs by monitoring and predicting trip conditions."We're doing the study because we anticipate increases in the number of consumers who will drive PHEVs," said Chowdhury. "These electric vehicles would be integrated with a VII system that provides detailed traffic data as a person drives past points outfitted to collect data. With this information we can come up with mathematical models that predict detailed traffic conditions which will be used to optimize fuel consumption and total energy used. It will also minimize the total cost of a trip.""We are looking at a new paradigm," said co-principal investigator Pierluigi Pisu, assistant professor of mechanical engineering. "We're providing the drivers with alternative routes to maximize fuel economy instead of just improving travel time."The researchers expect it has the potential to reduce U.S. reliance on petroleum and other greenhouse gas-producing fuels, reduce pollution, save energy, minimize the long-term cost of living expenses and improve driving conditions. Researchers will collaborate with Ford for real-world evaluation of the system integration and plan to implement results after the completion of the project.This material is based upon work supported by the NSF under Grant No. 0928744. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
matt_shipman@ncsu.eduRobo-bats with metal muscles may be next generation of remote control flyersTiny flying machines can be used for everything from indoor surveillance to exploring collapsed buildings, but simply making smaller versions of planes and helicopters doesn't work very well. Instead, researchers at North Carolina State University are mimicking nature's small flyers  and developing robotic bats that offer increased maneuverability and performance.Small flyers, or micro-aerial vehicles (MAVs), have garnered a great deal of interest due to their potential applications where maneuverability in tight spaces is necessary, says researcher Gheorghe Bunget. For example, Bunget says, "due to the availability of small sensors, MAVs can be used for detection missions of biological, chemical and nuclear agents." But, due to their size, devices using a traditional fixed-wing or rotary-wing design have low maneuverability and aerodynamic efficiency. So Bunget, a doctoral student in mechanical engineering at NC State, and his advisor Dr. Stefan Seelecke looked to nature. "We are trying to mimic nature as closely as possible," Seelecke says, "because it is very efficient. And, at the MAV scale, nature tells us that flapping flight  like that of the bat  is the most effective."The researchers did extensive analysis of bats' skeletal and muscular systems before developing a "robo-bat" skeleton using rapid prototyping technologies. The fully assembled skeleton rests easily in the palm of your hand and, at less than 6 grams, feels as light as a feather. The researchers are currently completing fabrication and assembly of the joints, muscular system and wing membrane for the robo-bat, which should allow it to fly with the same efficient flapping motion used by real bats."The key concept here is the use of smart materials," Seelecke says. "We are using a shape-memory metal alloy that is super-elastic for the joints. The material provides a full range of motion, but will always return to its original position  a function performed by many tiny bones, cartilage and tendons in real bats."Seelecke explains that the research team is also using smart materials for the muscular system. "We're using an alloy that responds to the heat from an electric current. That heat actuates micro-scale wires the size of a human hair, making them contract like 'metal muscles.' During the contraction, the powerful muscle wires also change their electric resistance, which can be easily measured, thus providing simultaneous action and sensory input. This dual functionality will help cut down on the robo-bat's weight, and allow the robot to respond quickly to changing conditions  such as a gust of wind  as perfectly as a real bat."In addition to creating a surveillance tool with very real practical applications, Seelecke says the robo-bat could also help expand our understanding of aerodynamics. "It will allow us to do tests where we can control all of the variables  and finally give us the opportunity to fully understand the aerodynamics of flapping flight," Seelecke says.Bunget will present the research this September at the American Society of Mechanical Engineers Conference on Smart Materials, Adaptive Structures and Intelligent Systems in Oxnard, Calif.
daniel.parry@nrl.navy.milNRL begins Southeast Asia study of aerosols linked to global warming(WASHINGTON, DC, 09/17/09)  NRL's Marine Meteorology Division has deployed the Mobile Atmosphere, Aerosol, and Radiation Characterization Observatory (MAARCO) to the National University of Singapore to begin the first comprehensive radiation and aerosol assessment in the Maritime Continent region. NRL is leading the effort to investigate ways in which to infer larger aerosol and visibility features from limited data sets, and to analyze the aerosol physical interactions and processes to aid numerical aerosol and weather predictions. The Marine Meteorology Division, Monterey, Calif., has specifically designed and developed the unique MAARCO to be a rugged, easily transportable, flexible and comprehensive suite of instruments to make surface measurements of the radiometric and meteorological properties of the atmosphere, the microphysical and compositional properties of aerosols and the vertical distribution of aerosols and clouds. The instrumentation suite includes a LIDAR, Sun photometer, radiometers, particle probes, filter samplers, and impactors as well as a host of weather instruments. The impact of biomass burning and pollution on the Maritime Continent is of considerable concern to global climate change researchers. The deployment of MAARCO is part of the overarching 7 Southeast Asian Studies (7SEAS) programa comprehensive interdisciplinary atmospheric sciences program to study the interactions of pollution and smoke with regional meteorology, air quality, land surface science and oceanography.."The extreme cloud cover and complicated environment of the Maritime Continent allows for very few satellite observations of pollution and smoke to be properly ingested in the models," said Dr. Simon Chang, superintendent, NRL Marine Meteorology Division. "Field investigations will help researchers develop methods to make the best use of the limited data available and yield insight on how to constrain aerosol optical properties in models and satellite retrievals."NRL began developing the 7 SEAS program in 2007 in partnership with NASA, and since has initiated satellite and model based research programs with regional scientists. Through collaboration with Southeast Asia regional science partners (in Malaysia, Philippines, Singapore, Taiwan, Thailand and Vietnam) and with sponsorship by NASA and the Office of Naval Research, NRL's Marine Meteorology Division is co-leading the effort with NASA's Goddard Space Flight Center, to dramatically ramp up activity with the introduction of significant field observatory deployments scheduled over the next five years. Initial field research is being performed in cooperation with the National University of Singapore's Center for Remote Imaging, Sensing and Processing (CRISP) and the Department of Environmental Engineering. "The 7SEAS mission plan calls for active collaborative research with regional scientists, and includes significant outreach and educational components for students," said Jeffrey Reid, 7SEAS mission scientist, NRL Marine Meteorology Division. "This interdisciplinary research program incorporates participation of multiple U.S. and international agencies at a grass-roots level which separates it from other programs in the region." Scientists are concerned that pollution and smoke in the Maritime Continent at times can be so dense that it may modify the weather and hampers their ability to forecast in the region.  According to Anthony Bucholtz, radiation scientist at NRL's Monterey laboratory, Singapore provides an ideal location for studying the effects of the complex interactions between sunlight, haze, clouds and pollution on the radiative balance of the tropical atmosphere. The Navy aerosol models are used for a variety of scientific and operational purposes, including visibility forecasting and the development of aerosol light scattering and absorption climatologies. Recent research has surrounded aerosol-weather interaction and it is anticipated that in the future the direct impact of aerosol particles on the meteorological fields of Navy weather models will be included in forecasts. Following a previous major aerosol/dust field experiment in Southwest Asia (The Unified Aerosol Experiment in the United Arab Emirates (UAE2), interest in 7 SEAS focuses on aerosol meteorology interaction determining how aerosol particles perturb cloud properties and boundary layer dynamics, subsequently feeding back into pollution emissions, concentrations, evolution, scavenging and transport. These feedbacks cause interactions of interest to a wide variety of other Earth systems science fields including oceanography and land surface science. Navy interest lies in aerosol observability and numerical prediction issues, and in particular, aerosol radiative impacts. NRL developed many of the "world's firsts" of aerosol and pollution forecasting. NRL developed the first operational aerosol prediction system (NRL Aerosol Analysis and Prediction System, NAAPS) as well as the world's first operational aerosol data assimilation system which uses NASA satellite measurements to track pollution, dust and smoke (the Navy Variational Analysis Data Assimilation System-Aerosol Optical Depth, NAVDAS-AOD). Lastly, NAAPS is the world's first global operational model to employ satellite data to actively track emissions. Operationally the Navy uses both geostationary meteorological and NASA climate satellite sensors.The Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA.
cgenova@cell.comBrain's fear center is equipped with a built-in suffocation sensorThe portion of our brains that is responsible for registering fear and even panic has a built-in chemical sensor that is triggered by a primordial terror  suffocation. A report in the November 25th issue of the journal Cell, a Cell Press publication, shows in studies of mice that the rise in acid levels in the brain upon breathing carbon dioxide triggers acid-sensing channels that evoke fear behavior.In addition to the insight into the normal fear response, the discovery may help to explain and perhaps even correct what goes wrong in those who suffer from panic attacks, the researchers say. (It's been known for almost a century that carbon dioxide inhalation can trigger panic attacks, and that patients with panic disorder are particularly susceptible.)"The amygdala has been thought of as part of the fear circuitry of the brain," said John Wemmie of the University of Iowa, Iowa City. "Now we see it isn't just part of a circuit, it is also a sensor.""It's interesting that evolution positioned an acid sensor right in this central circuit," added Michael Welsh, also of the University of Iowa. "Detecting an elevated carbon dioxide is critical for survival. When you are suffocating, this circuit triggers mechanisms for escape or relief of the problem."The circuit in question resides in the amygdala, a structure that stimulates the sympathetic nervous system for fight-or-flight and links to other brain regions involved in the response to threat. The amygdala is known from earlier studies to play a role in both innate and learned fears.In previous studies, Wemmie's and Welsh's team discovered that the acid-sensing ion channel-1a (ASIC1a) is particularly abundant in the amygdala and other fear circuit structures, where it is required for normal responses in tests of fear behavior. As the name suggests, ASICs are sensitive to pH and become activated when pH levels fall.The contribution of both the amygdala and ASIC1a to fear behavior led them to suspect that a reduced pH might induce fear behavior by activating the channels, thereby allowing the amygdala to function as a chemosensor deep within the fear circuit. And that's exactly what they've now been able to show.They found that inhaled carbon dioxide reduced brain pH and evoked fear behavior in mice. Mice breathing 5% carbon dioxide tended to avoid open spaces more than usual and, in standard tests of fear learning conducted in the presence of 10% carbon dioxide, the mice displayed exaggerated freezing behaviors.Animals lacking those acid-sensing ion channels showed less fear, a condition that was reversed when the channels were reinstated specifically in their amygdala. Treatments that prevented the pH change reduced fear behavior, while acidic microinjections into the amygdala did just the opposite.The new findings show that the amygdala not only senses the threat posed by carbon dioxide, but it also initiates a response. There is surely good reason for such an integrated alarm system."Because oxygen-breathing organisms are under a constant threat of asphyxiation, it could be argued that the threat of suffocation has had a primary influence on shaping the brain's defensive systems," wrote Stephen Marin of the University of Michigan, Ann Arbor in an accompanying commentary. "The present discovery that chemosensors in the amygdala are involved in generating fear responses to a variety of aversive stimuli suggests that a system that evolved to generate behavior to defend against suffocation was subsequently adapted to deal with both innate and learned threats in the external environment. In some regards, this is not surprising. In the grasp of a predator, suffocation is the ultimate fearit signals imminent death."In addition to revealing the amygdala as an important chemosensor, the new results also give a molecular explanation for how rising carbon dioxide concentrations elicit intense fear and provide a foundation for dissecting the bases of anxiety and panic disorders, the researchers say. A single breath of carbon dioxide can trigger panic attacks in patients with panic disorder, they explained, and dysregulated brain pH has also been implicated in the condition. In addition, patients suffering from respiratory failure are also known to become extremely anxious."It has been proposed that panic and anxiety disorders involve a suffocation alarm gone haywire," Welsh said. "Now, this work may shed some light on this well-known phenomenon and suggests strategies for further exploration."The findings raise the possibility that some people may be more prone to anxiety disorders, including post-traumatic stress disorder, due to genetic variants they carry in components of this ASIC pathway. They also suggest that new therapeutic strategies for panic and anxiety might target changes in brain pH or the acid-sensing channels.The researchers include Adam E. Ziemann, University of Iowa, Iowa City, Iowa; Jason E. Allen, University of Iowa, Iowa City, Iowa; Nader S. Dahdaleh, University of Iowa, Iowa City, Iowa; Iuliia I. Drebot, University of Iowa, Iowa City, Iowa; Matthew W. Coryell, University of Iowa, Iowa City, Iowa; Amanda M. Wunsch, University of Iowa, Iowa City, Iowa; Cynthia M. Lynch, University of Iowa, Iowa City, Iowa; Frank M. Faraci, University of Iowa, Iowa City, Iowa; Matthew A. Howard III, University of Iowa, Iowa City, Iowa; Michael J. Welsh, University of Iowa, Iowa City, Iowa, Howard Hughes Medical Institute, and John A. Wemmie, University of Iowa, Iowa City, Iowa; Department of Veterans Affairs Medical Center, Iowa City, IA.
lucy.goodchild@imperial.ac.ukThink zinc: Molecular sensor could reveal zinc's role in diseasesScientists have developed a new molecular sensor that can reveal the amount of zinc in cells, which could tell us more about a number of diseases, including type 2 diabetes. The research, published today in Nature Methods, opens the door to the hidden world of zinc biology by giving scientists an accurate way of measuring the concentration of zinc and its location in cells for the first time.Zinc is involved in many processes in the body and five percent of all the proteins made by the body's cells are involved in transporting zinc. Scientists believe that zinc plays a role in many diseases; for example, it helps package insulin in pancreas cells and in people with type 2 diabetes, the gene that controls this packaging is often defective. Previously, researchers used crude chemical techniques to get a rough idea of the concentration of zinc in cells. However, they could not produce an accurate picture of how much zinc was present in cells or where it was within them.In today's study, researchers from Imperial College London and Eindhoven University of Technology in The Netherlands have developed a molecular sensor using fluorescence proteins that can measure the distance between zinc ions in individual cells, showing how much zinc is present.Professor Guy Rutter, one of the authors of the study from the Division of Medicine at Imperial College London, said: "There has been relatively little biological work done on zinc compared to other metals such as calcium and sodium, partly because we didn't have the tools to measure it accurately before now. Zinc is so important in the body  studies have suggested it has roles in many different areas, including muscles and the brain."The new sensor, called a fluorescence resonance energy transfer (FRET)-based sensor, is made up of two jellyfish proteins called green fluorescent proteins. The researchers altered the first protein to give off light at a certain wavelength, and altered the second protein to collect that light. When the proteins attached to zinc ions, the proteins became pushed apart and the transmission of light between them became weaker. The researchers used a fluorescence microscope to detect the wavelengths of light emitted by the proteins. This revealed zinc in the cell, with coloured patches visible where the proteins detected zinc.The researchers used their new sensor to look for zinc in pancreatic cells, where insulin is packaged around zinc ions. Previous research had suggested that in people with type 2 diabetes, the gene that controls the packaging process is often defective, affecting the way insulin is stored. The researchers found a high concentration of zinc ions inside certain parts of the cells where insulin is found. They hope their new sensor could help scientists look more closely at this to find out exactly how zinc is involved in diabetes."We can now measure very accurately the concentration of zinc in cells and we can also look at where it is inside the cell, using our molecular measuring device. This sort of information will help us to see what is going on inside different tissues, for example in the brain in Alzheimer's disease, where we also suspect zinc may be involved. We hope this new sensor will help researchers learn more about zinc-related diseases and potentially identify new ways of treating them," added Professor Rutter.The researchers would now like to develop their new sensor to look at zinc in a living mouse model, so they can observe the movement of zinc in different tissues, for example in diabetes.This research in the UK was funded by The Wellcome Trust, Medical Research Council (UK) the EU and Imperial College London.
ncmoore@umich.eduLasers can lengthen quantum bit memory by 1,000 timesANN ARBOR, Mich.---Physicists have found a way to drastically prolong the shelf life of quantum bits, the 0s and 1s of quantum computers. These precarious bits, formed in this case by arrays of semiconductor quantum dots containing a single extra electron, are easily perturbed by magnetic field fluctuations from the nuclei of the atoms creating the quantum dot. This perturbation causes the bits to essentially forget the piece of information they were tasked with storing. A quantum dot is a semiconductor nanostructure that is one candidate for creating quantum bits.The scientists, including the University of Michigan's Duncan Steel, used lasers to elicit a previously undiscovered natural feedback reaction that stabilizes the quantum dot's magnetic field, lengthening the stable existence of the quantum bit by several orders of magnitude, or more than 1,000 times.The findings are published in the June 25 edition of Nature.Because of their ability to represent multiple states simultaneously, quantum computers could theoretically factor numbers dramatically faster and with smaller computers than conventional computers. For this reason, they could vastly improve computer security."In our approach, the quantum bit for information storage is an electron spin confined to a single dot in a semiconductor like indium arsenide. Rather than representing a 0 or a 1 as a transistor does in a classical computer, a quantum bit can be a linear combination of 0 and 1. It's sort of like hitting two piano keys at the same time," said Steel, a professor in the Department of Physics and the Robert J. Hiller Professor of Electrical Engineering and Computer Science. "One of the serious problems in quantum computing is that anything that disturbs the phase of one of these spins relative to the other causes a loss of coherence and destroys the information that was stored. It is as though one of the two notes on the piano is silenced, leaving only the other note."Spin is an intrinsic property of the electron that isn't rotation, but is more like magnetic poles. Electrons are said to have spin up or down, which represent the 0s and 1s.A major cause of information loss in a popular class of semiconductors called 3/5 materials is the interaction of the electron (the quantum bit) with the nuclei of the atoms in the quantum dot holding the electron. Trapping the electron in a particular spin, as is necessary in quantum computers, gives rise to a small magnetic field that couples with the magnetic field in the nuclei and breaks down the memory in a few billionths of a second.By exciting the quantum dot with a laser, the scientists were able to block the interaction of these magnetic fields. The laser causes an electron in the quantum dot to jump to a higher energy level, leaving behind a charged hole in the electron cloud. This hole, or space vacated by an electron, also has a magnetic field due to the collective spin of the remaining electron cloud. It turns out that the hole acts directly with the nuclei and controls its magnetic field without any intervention from outside except the fixed excitation by the lasers to create the hole. "This discovery was quite unexpected," Steel said. "Naturally occurring, nonlinear feedback in physical systems is rarely observed. We found a remarkable piece of physics in nature. We still have other major technical obstacles, but our work shows that one of the major hurdles to quantum computers that we thought might be a show-stopper isn't one," Steel said.The paper is called "Optically-controlled locking of the nuclear field via coherent dark-state spectroscopy." Other authors are with the Naval Research Laboratory, the University of California San Diego, and the University of Hong Kong. The research is funded by the U.S. Army Research Office, the Air Force Office of Scientific Research, the Office of Naval Research, The National Security Agency's Laboratory for Physical Sciences, the Intelligence Advanced Research Projects Agency and the National Science Foundation.For more informationDuncan Steel: http://www.ns.umich.edu/htdocs/public/experts/ExpDisplay.php?ExpID=1226Michigan Engineering:The University of Michigan College of Engineering is ranked among the top engineering schools in the country. At more than $130 million annually, its engineering research budget is one of largest of any public university. Michigan Engineering is home to 11 academic departments and a National Science Foundation Engineering Research Center. The college plays a leading role in the Michigan Memorial Phoenix Energy Institute and hosts the world class Lurie Nanofabrication Facility. Michigan Engineering's premier scholarship, international scale and multidisciplinary scope combine to create The Michigan Difference. Find out more at http://www.engin.umich.edu/.
cdybas@nsf.govA new look beneath the waves: Ocean Observatories Initiative gets underwayNational Science Foundation and Consortium for Ocean Leadership sign cooperative agreement for vast undersea observing networkGiving scientists never-before-seen views of the world's oceans, the National Science Foundation (NSF) and the Consortium for Ocean Leadership (COL) have signed a Cooperative Agreement that supports the construction and initial operation of the Ocean Observatories Initiative (OOI).OOI will provide a network of undersea sensors for observing complex ocean processes such as climate variability, ocean circulation, and ocean acidification at several coastal, open-ocean and seafloor locations.Continuous data flow from hundreds of OOI sensors will be integrated by a sophisticated computing network, and will be openly available to scientists, policy makers, students and the public."Through the Recovery Act, we are putting people to work today to find answers to some of the major scientific and environmental challenges that we face," said Arden L. Bement, Jr., director of NSF."The oceans drive an incredible range of natural phenomena, including our climate, and directly impact society in myriad ways," Bement explained. "New approaches are crucial to our understanding of changes now happening in the world's oceans. OOI will install the latest technologies where they can best serve scientists, policymakers and the public."Added Julie Morris, NSF division director for ocean sciences, "Moving a large project to the construction phase requires rigorous planning. Remarkable cooperation and commitment from the OOI team is translating a long-held dream into a new reality for the ocean sciences research community."Advanced ocean research and sensor tools are a significant improvement over past techniques. Remotely operated and autonomous vehicles go deeper and perform longer than submarines. Underwater samplers do in minutes what once took hours in a lab. Telecommunications cables link experiments directly to office computers on land. At sea, satellite uplinks shuttle buoy data at increasing speeds.Sited in critical areas of the open and coastal ocean, OOI will radically change the rate and scale of ocean data collection. The networked observatory will focus on global, regional and coastal science questions. It will also provide platforms to support new kinds of instruments and autonomous vehicles."OOI is an unprecedented opportunity for, and whole new approach to, advancing our understanding of how the ocean works and interacts with the atmosphere and solid Earth," said Robert Gagosian, president and CEO of COL. "It will allow scientists to answer complex questions--questions only dreamed of a few years ago--about the future health of our planet, such as the ocean's role in climate change. It's very exciting to be part of this huge step forward in the ocean sciences."The five-plus-year construction phase, funded initially with American Recovery and Reinvestment Act (ARRA) of 2009 funds, will begin this month.  The first year of funding under the Cooperative Agreement will support a range of construction efforts, including production engineering and prototyping of key coastal and open-ocean components (moorings, buoys, sensors), award of the primary seafloor cable contract, completion of a shore station for power and data, and software development for sensor interfaces to the network.Subsequent years of funding will support the completion of coastal, deep-ocean, and seafloor systems, with initial data flow scheduled for early 2013 and final commissioning of the full system in 2015.The OOI is managed and coordinated by the OOI Project Office at the Consortium for Ocean Leadership in Washington, D.C., with three major implementing organizations responsible for the construction of the components of the full network:"This award represents the fulfillment of more than a decade of planning and hard work by hundreds of ocean scientists, and reflects the commitment of the National Science Foundation to new approaches for documenting ocean processes," said Tim Cowles, OOI program director at the Consortium for Ocean Leadership."The OOI project team is excited to play a role in implementing this unique suite of observing assets. We're building an infrastructure that will transform ocean sciences."
Robert.J.Gutro@nasa.govNASA's TRMM sees huge Typhoon Parma bringing more rain to the PhilippinesTyphoon Parma is a huge storm and NASA's TRMM satellite sees it is already bringing more unwanted rains and gusty winds to the typhoon-weary and devastated Philippines. Parma, also called "Pepeng" in the Philippines, will bring heavy rains there today and tomorrow before moving back to sea.Parma is expected to make landfall in or near the northeastern province of Isabela on Saturday, October 2 (local time). That is a mountainous region, and not heavily populated, however its rains will cause life-threatening mudslides. Parma is also expected slam Luzon with rain over the next two days adding to the existing flooded conditions.NASA's Tropical Rainfall Measuring Mission satellite (TRMM), a joint mission between NASA and the Japanese space agency JAXA, captured an image of Parma's rains already affecting the Philippines on October 2 at 00:43 UTC, 8:43 a.m. local Manila Time (8:43 p.m. EDT, Oct. 1). TRMM noticed that most of the rainfall around Parma's center is between 20 and 40 millimeters (.78 to 1.57 inches) per hour.TRMM also noticed that some of Parma's "hot towers," towering thunderstorms are reaching as high as 14 kilometers (more than 8.5 miles high), indicating very powerful storms with heavy rainfall.Warnings have already been posted in various areas of the Philippines. Public storm warning signal 1 is in force in Calayan Group of Islands, Babuyan Group of Islands, Ilocos Norte & Sur, Apayao, Abra, Kalinga, Mt. Province, Ifugao, Nueva Viscaya, Benguet, La Union, Pangasinan, Tarlac, Nueva Ecija, Zambales, Bataan, Pampanga, Bulacan, Laguna, Batangas, Cavite, Rizal, Rest of Quezon, Marinduque, Albay, Burias Islands, Sorsogon and Metro Manila.Public storm warning signal 2 is in force in Cagayan, Isabela, Aurora, Quirino, Northern Quezon, Polilio Islands, Camarines Norte and Sur. Public storm warning signal 3 is in force in Catanduanes.On October 2 at 15:00 UTC (11 p.m. local Asia/Manila Time, 11 a.m. EDT), Typhoon Parma had maximum sustained winds near 115 knots (132 mph) with higher gusts. That makes Parma a Category 4 Typhoon on the Saffir-Simpson Scale. Parma's center is located about 230 nautical miles east-northeast of Manila, the Philippines, near 16.2 North and 124.3 East.At 11 a.m. EDT this morning, October 2, Manila is already experiencing rain and gusty winds, which will be tropical storm force later today. Typhoon Parma is a huge storm, and tropical storm-force winds extend outward from the center to as far as 210 miles. Hurricane/Typhoon-force winds extend up to 80 miles from Parma's center. Those winds are generating huge and dangerous ocean waves, as high as 32 feet high.Parma is moving northwest at 8 mph and will continue in that direction, heading toward northeastern Luzon.  The National Hurricane Center's definition for a Category 4 Hurricane/Typhoon reads, "Sustained winds 131-155 mph (114-135 kt or 210-249 km/hr). Extremely dangerous winds causing devastating damage are expected. Some wall failures with some complete roof structure failures on houses will occur. All signs are blown down. Complete destruction of mobile homes (primarily pre-1994 construction). Extensive damage to doors and windows is likely. Numerous windows in high rise buildings will be dislodged and become airborne. Windborne debris will cause extensive damage and persons struck by the wind-blown debris will be injured or killed. Most trees will be snapped or uprooted. Fallen trees could cut off residential areas for days to weeks. Electricity will be unavailable for weeks after the hurricane passes."After departing the Philippines, Parma will veer west, into the Luzon Strait and then into the northern South China Sea. Residents of eastern China and Taiwan need to monitor the progress of this storm.
sue.knapp@dartmouth.eduDartmouth gets $3 million from the National Science Foundation for IT research in health careHANOVER, NH  Dartmouth has received a $3 million grant from the National Science Foundation for research to develop secure and trustworthy computing systems for healthcare settings.The project, funded under the American Recovery and Reinvestment Act of 2009 (ARRA, the national economic stimulus bill), is aimed at improving the security and effectiveness of information technology infrastructure in the healthcare industry, which in turn will help meet two of its most significant challenges of the 21st century: improving the quality of care and controlling costs. Called the Trustworthy Information Systems for Healthcare (TISH) project http://www.ists.dartmouth.edu/projects/tish.html, the Dartmouth research will address fundamental challenges in information security in healthcare, such as protecting the security of clinical information while ensuring that clinicians can access information they need, and enhancing the collection of data from wearable sensor devices to enable physicians to better monitor patients' health with both security and privacy in mind. "Healthcare information systems are a key part of ARRA and are important for other pending healthcare reform proposals," said David Kotz, the principal investigator for TISH and a professor of computer science at Dartmouth College. "Developing, deploying, and using information technology that is both secure and genuinely effective in the complex clinical, organizational, and economic environment of healthcare is a significant challenge. Our research will help address the important security and privacy challenges inherent in such systems."Denise Anthony, part of the TISH team and the research director at Dartmouth's Institute for Security, Technology, and Society (ISTS), added, "As President Obama has made clear, the vision for a 21st century health system requires all health information in electronic form, delivered instantly and securely to individuals and their care providers when needed, and it should be capable of analysis for constant improvement and research." Anthony is also associate professor and chair of the Department of Sociology. The TISH team brings a multidisciplinary approach to develop and analyze information-sharing technology that ensures security and privacy while meeting the pragmatic needs of patients, clinical staff, and healthcare organizations to deliver efficient, high-quality care. For this project, Dartmouth's ISTS was instrumental in bringing together faculty from departments across campus, and in recruiting partners from the Veterans Affairs Medical Center (White River Junction, Vt.), Intel Labs, and Google."The work to be performed by the ISTS team will address some of the major challenges in the effective use of information technology for healthcare," said Martin Wybourne, Vice Provost for Research. "The close association between Dartmouth College and the Dartmouth-Hitchcock Medical Center provides an excellent platform for this research."Added Andrew Gettinger, also on the TISH team, "Identity management and authentication are vexing issues in clinical settings that if poorly managed can result in patient harm from delays or, at the other extreme, breaches in privacy that can undermine confidence in electronic health records. The national focus and exuberance for electronic health records ought to be tempered by the current reality that these systems need additional engineering to fit into the clinical workflow. This project will enable a multidisciplinary research team to examine and hopefully develop strategies to improve or mitigate some of those issues." Gettinger is the senior medical director of information systems and informatics at Dartmouth-Hitchcock Medical Center and an associate professor of anesthesiology at Dartmouth Medical School.Through the three-year project, the researchers will examine privacy concerns, address security challenges, and study economic risks and benefits. The team will develop new secure, efficient, and easy-to-use protocols that allow remote health monitoring through mobile phone and wearable wireless medical sensors; design new machine-learning methods for analyzing and summarizing sensor data; seek a deeper understanding of the economics of information security in healthcare; and explore how patients and clinicians trade off usability, security, and privacy.In addition to Kotz, Anthony, and Gettinger, Dartmouth representatives on the TISH team include Sean Smith, associate professor of computer science; Tanzeem Choudhury, assistant professor of computer science; Eric Johnson, the Benjamin Ames Kimball Professor of the Science of Administration and Director, Glassmeyer/McNamee Center for Digital Strategies at Dartmouth's Tuck School of Business; Ann Flood from The Dartmouth Institute for Health Policy and Clinical Practice; Tom Candon, associate director of ISTS; and Sarah Brooks, the associate director of finance and administration at ISTS.
edgarlee@ntu.edu.sgNew Singapore-French nanotech lab opens at NTUUnique joint set-up by a research center, a university and a private organizationJust two months after the Nanyang Technological University (NTU) signed the Memorandum of Understanding in Paris with the National Center for Scientific Research (CNRS) and the Thales Group of Companies to set up a joint research laboratory, the three parties are meeting again in Singapore to inaugurate the CNRS-International-NTU-Thales Research Alliance (CINTRA) Laboratory at NTU. Located at the Research Techno Plaza, the CINTRA Laboratory aims to harness the latest in science and technology to develop innovations in nanotechnologies for computing, sensing and communications applications. Over the next two years, about 50 Singapore and French researchers will work on critical issues and challenges faced by existing technologies in the microelectronic and photonic industries, promising innovations to meet future commercial as well as defence and security needs.   Examples of such application-driven challenges include the development of enabling technologies such as an imaging chip to process and display real-time multi-dimensional information, and a low-power signal processing chip capable of super high-speed performance of a trillion bits (terabit) per second or more.    His Excellency Mr Olivier Caron, the Ambassador of France to Singapore, who was the guest-of-honour at the inauguration ceremony, said: "The R&D and innovation landscapes have changed, and are no longer confined to simple projects that can be defined, funded and conducted within single companies, universities or research institutions. It is no longer possible to progress in a major field of scientific research without the cooperation and strong commitment of different actors coming from different regions of the world." "France and Singapore have long ago started this process of collaborative work and mutual aid, launching different research projects and tightening the R&D relationships between both countries," said Mr Caron. "This current alliance is unlike most of the others  it brings together a research centre, a university and a private company in the same location. Academia, fundamental research, applied research, and technology transfer are considered in this union. This model should not only drive breakthroughs in innovation but should also result in bringing these innovations to the market." NTU President Dr Su Guaning said: "NTU is acknowledged as a University that seeks to be relevant to the needs of industry, innovating and pioneering new application areas while building on our staff's core competencies. We seek to be nimble, listening closely to the research trends while responding to the needs of the industry.""We are indeed grateful to CNRS and Thales for the confidence that they have shown in NTU. Together with CNRS' strength in research and Thales' experience in the global market, we shall make the CINTRA Laboratory an example of how upstream research can turn into successful commercial products and applications in a most unexpected way," said Dr Su.  Mr Jean-Jacques Gagnepain, Adviser to the CNRS President on International Affairs, said: "CNRS salutes the impressive commitment that NTU has granted to our joint laboratory, the Unité Mixte Internationale CINTRA.""Within these superb, well-designed and ergonomic premises that we visited today, we feel all the more certain that the joint efforts of our scientists will allow NTU, Thales and CNRS to produce far-reaching research results, concepts and inventions. It is our joint ambition to lift the bottlenecks that currently block existing microelectronic and photonic research," said Mr Gagnepain.Mr Patrick Plante, Chief Executive Officer of Thales Technology Centre Singapore, said: "Thales reaffirms its strong commitment to the Unité Mixte Internationale CINTRA and is proud to be one of the three pillars of this unique joint collaboration between academic, research and industry partners outside of France." "Our long-standing relationship with the prestigious NTU of Singapore established since the creation of our joint research lab Thales@NTU, as well as our long-valued partnership with the largest fundamental research organisation in Europe, CNRS, will be significantly reinforced, and we trust that our complementarities will lead CINTRA to excellence in developing cutting-edge technologies. In this research-friendly environment at NTU, CINTRA will fly the flag for French-Singapore advanced research on nanotechnologies and nanophotonics," said Mr Plante. 
huang@mail.ncku.edu.twBuilding the smart home wirelesslyLike the paperless office, the smart home has been a long time coming, but a report published in the International Journal of Internet Protocol Technology, suggests that radio tags coupled with mobile communications devices could soon provide seamless multimedia services to the home.Yueh-Min Huang of the Department of Engineering Science, at the National Cheng Kung University (NCKU), in Tainan, Taiwan, and colleagues explain that as networks and technology develop, the concept of a smart home has become a major focus of major computer, communication and consumer companies.The team has now proposed this an intelligent home network system that works by integrating well-known Radio Frequency Identification (RFID) technology into the Open Service Gateway Initiative (OSGi) to allow people to access a video monitoring and media system throughout their household or even remotely.Their proposal will help solve several common problems for people when they are away from home, such as whether they left devices and lights running that should have been switched off, to check that their security alarm is set and often more worrisome whether the children are doing their homework or watching TV or gaming instead?When you are at home, the same RFID technology could take care of entertainment needs as you move around the house, allowing favorite songs to follow you from room to room, for instance.The team points out that more than 70 manufacturers, including Echelon, IBM, Motorola, Nokia, Nortel, Panasonic, Philips, Sony, and Toshiba have joined OSGi, which means that the standard could be widely adopted and implemented by technologists. The NCKU team has built on this open network system using RFID so that devices and individuals can be connected to video devices in the smart home for entertainment or security purposes.There are several scenarios that their system could enable: users could watch and control a monitor screen in the home through a mobile device, for instance. Users could interact with each other through networked TV whether they are at home or not. And, by having a centralized media server users could have movies or music track them as they move from room to room."The open architecture system in this paper can provide rapid, automatic, and convenient services, thereby substantially improving the quality of life for users," the researchers say. Fundamentally, RFID over the OSGi system will make the smart home a reality by connecting all your media devices and domestic appliances so that they can usefully communicate with each other and with you, the team concludes."RFID-based seamless multimedia services for smart homes" in Int. J. Internet Protocol Technology, 2009, 4, 232-239
mhisham@ntu.edu.sgNew Singapore-France research alliance to develop state-of-the-art nanotechnologiesNTU, Singapore's leading science and technology university has formed a tripartite research alliance with France's National Center of Scientific Research and Thales, the French electronics giant and a global technology leaderNanyang Technological University (NTU), Singapore's leading science and technology university has formed a tripartite research alliance with the National Center of Scientific Research (Centre National de la Recherche Scientifique or CNRS), the largest governmental research organisation in France, and Thales the French electronics giant and a global technology leader in aerospace, space, defence, security and transportation industries.The alliance known as the CNRS International-NTU-Thales Research Alliance (CINTRA) will be setting up a joint laboratory at NTU's Research Techno Plaza. The Memorandum of Understanding for the establishment of CINTRA was signed today in Paris, France, witnessed by Singapore's Minister of Education and Second Minister of Defence, Dr Ng Eng Hen and France's Minister of Higher Education and Research, Mrs Valérie Pécresse. The signatories to the agreement are NTU President, Dr Su Guaning; Director General CNRS, Mr Arnold Migus and Chief Technical Officer Thales, Dr Marko Erman.The CINTRA Laboratory aims to harness the latest in science and technology to develop innovations in nanotechnologies for future computing, sensing, and communication applications. The collaboration will provide opportunities to challenge and tackle critical issues and bottlenecks faced by existing technologies in microelectronic and photonic industries, promising innovations with superior performance beyond what is available today.Such application-driven challenges serve to push the boundary of today's technology in this field to meet future commercial and defence and security needs. Some creative ideas include the development of enabling technologies such as an imaging chip to process and display real-time multi-dimensional information; or a signal processing chip capable of super high-speed performance of a trillion bits (terabit) per second or more, but consuming little power."The CINTRA laboratory will be an important conduit for the transfer of scientific and technological knowledge, breakthroughs and industrial innovations between Singapore and France. NTU is proud to be the university to represent Singapore in this partnership with CNRS and Thales. It is a testament to the University's global links, R&D capabilities and ability to innovate and develop technologies that make a difference to society," says NTU President, Dr Su Guaning."CNRS is very proud to contribute, through the creation of this joint laboratory with NTU and Thales, to the intensification of scientific relations between France and Singapore. This alliance brings together the prestigious NTU of Singapore, the major European multidisciplinary research organisation and a French leading industrial group. This new Unité Mixte Internationale (joint international laboratory) is a structure of excellence which positions us at the cutting edge of Nanotechnologies. It will allow us to combine our respective strengths in scientific and technological research in this field," says Mr Arnold Migus."Thales is pleased to be part of this alliance and to jointly develop new skills and new technologies to fulfil the requirements of future advanced electromagnetic sensors and signal processing functions. This is done by combining ultra high performances and very low power consumption, especially in addressing the integration and interconnection of electronics and photonics nanotechnologies," says Dr Marko Erman.The collaboration will also open opportunities for post-graduate students and research staff to take part in exchange programmes. The CINTRA Laboratory is expected to open by end October 2009. It will be actively competing for research funding in Singapore as well as in Europe. In the next two years, the laboratory is expected to house about 50 Singapore and French researchers.The CINTRA Laboratory will be managed by a Scientific Committee with representatives from the three partners. Professor Dominique Baillargeat of CNRS has been appointed as Director of the laboratory, with Professor Tjin Swee Chuan of NTU and Dr Myriam Kaba of Thales as Deputy Directors. 
bspice@cs.cmu.eduCarnegie Mellon develops Java programming tools employing human-centered design techniquesPITTSBURGHResearchers at Carnegie Mellon University's School of Computer Science have developed two new tools to help computer programmers select from among thousands of options within the application programming interfaces (APIs) that are used to write applications in Java, today's most popular programming language.	The tools  Jadeite (www.cs.cmu.edu/~jadeite) and Apatite (www.cs.cmu.edu/~apatite)  take advantage of human-centered design techniques to significantly reduce the time and guesswork associated with finding the right classes and methods of APIs. APIs are standardized methods that a Java program uses to ask the computer's operating system or another program to do something, such as opening a file or sending an email. Choosing APIs for accomplishing a given task is at the heart of Java programming, but is not intuitive, said Brad A. Myers, professor of human-computer interaction. With more than 35,000 methods listed in 4,100 classes in the current Javadoc library of APIs  and more being added in every new version  not even the savviest developer can hope to be familiar with them all."This is a fundamental problem for all programmers, whether they are novices, professionals or the growing number of end-users who just need to modify a Web page," Myers said. "It's possible to design APIs so that they are easier to use, but that still leaves thousands of existing APIs that are hard to use but essential for Java programming. Jadeite and Apatite help programmers find what they need among those existing APIs." Jadeite (Java Documentation with Extra Information Tacked-on for Emphasis) improves usability by enhancing the existing Javadoc documentation. For instance, Jadeite displays the names of API classes in font sizes that correspond with how heavily used they are based on Google searches, helping programmers navigate past little-used classes. The commonly used "PrintWriter" is in large, prominent letters, while the lesser used "PrintEvent" is in smaller type.Jadeite also uses crowd-sourcing to compensate for the fact that an API sometimes doesn't include methods that programmers expect. For instance, the Message and MimeMessage classes don't include a method for sending an email message. So Jadeite allows users to put so-called placeholders for these expected classes and methods within the alphabetical listing of APIs. Users can edit the placeholder to guide programmers to the actual location of the desired method, explain why a desired method is not part of the API, or note that a desired functionality is impossible.Finding the way to create certain types of objects, such as SSL sockets that enable secure Internet communications, may not be obvious to programmers the first time they encounter these objects. In these cases, Jadeite includes examples of the most popular code used by programmers to create these objects, allowing the user to learn from the examples.User studies showed that programmers could perform common tasks about three times faster with Jadeite than with the standard Javadoc documentation.Apatite (Associative Perusal of APIs That Identifies Targets Easily) takes a different approach, allowing programmers to browse APIs by association, seeing which packages, classes and methods tend to go with each other. It also uses statistics about the popularity of each item to provide weighted views of the most relevant items, listing them in larger fonts. Both Jadeite and Apatite remain research tools, Myers said, but are available for public use. Broader use of the tools will enhance the crowd-sourcing aspects of the tools, while giving the researchers important feedback about how the tools can be improved.Research by Jeffrey Stylos, who was awarded a Ph.D. in computer science this spring, underlies both Jadeite and Apatite. Besides Myers, research programmer Andrew Faulring and undergraduate computer science student Zizhuang Yang contributed to the development of Jadeite and computer science undergraduate Daniel S. Eisenberg led the implementation of Apatite. Eisenberg's work on Apatite earned first place in the Yahoo! Undergraduate Research Awards competition at Carnegie Mellon this spring.Jadeite and Apatite are part of the Natural Programming Project, www.cs.cmu.edu/~NatProg/, an initiative within Carnegie Mellon's Human-Computer Interaction Institute that is investigating how to make programming easier. Both tools have been funded by grants from the National Science Foundation and software giant SAP AG Inc.	 About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion comprehensive campaign, titled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty, students and innovative research, and enhance the physical campus with equipment and facility improvements. For more about Carnegie Mellon, visit http://www.cmu.edu/about/.
jtoon@gatech.edu3-D system based on optical fiber could provide new options for photovoltaicsHidden solar cellsConverting sunlight to electricity might no longer mean large panels of photovoltaic cells atop flat surfaces like roofs.  Using zinc oxide nanostructures grown on optical fibers and coated with dye-sensitized solar cell materials, researchers at the Georgia Institute of Technology have developed a new type of three-dimensional photovoltaic system.  The approach could allow PV systems to be hidden from view and located away from traditional locations such as rooftops."Using this technology, we can make photovoltaic generators that are foldable, concealed and mobile," said Zhong Lin Wang, a Regents professor in the Georgia Tech School of Materials Science and Engineering.  "Optical fiber could conduct sunlight into a building's walls where the nanostructures would convert it to electricity.  This is truly a three dimensional solar cell."Details of the research were published in the early view of the journal Angewandte Chemie International on October 22.  The work was sponsored by the Defense Advanced Research Projects Agency (DARPA), the KAUST Global Research Partnership and the National Science Foundation (NSF).Dye-sensitized solar cells use a photochemical system to generate electricity.  They are inexpensive to manufacture, flexible and mechanically robust, but their tradeoff for lower cost is conversion efficiency lower than that of silicon-based cells.  But using nanostructure arrays to increase the surface area available to convert light could help reduce the efficiency disadvantage, while giving architects and designers new options for incorporating PV into buildings, vehicles and even military equipment.Fabrication of the new Georgia Tech PV system begins with optical fiber of the type used by the telecommunications industry to transport data.  First, the researchers remove the cladding layer, then apply a conductive coating to the surface of the fiber before seeding the surface with zinc oxide.  Next, they use established solution-based techniques to grow aligned zinc oxide nanowires around the fiber much like the bristles of a bottle brush.  The nanowires are then coated with the dye-sensitized materials that convert light to electricity.Sunlight entering the optical fiber passes into the nanowires, where it interacts with the dye molecules to produce electrical current. A liquid electrolyte between the nanowires collects the electrical charges.  The result is a hybrid nanowire/optical fiber system that can be up to six times as efficient as planar zinc oxide cells with the same surface area."In each reflection within the fiber, the light has the opportunity to interact with the nanostructures that are coated with the dye molecules," Wang explained.  "You have multiple light reflections within the fiber, and multiple reflections within the nanostructures.  These interactions increase the likelihood that the light will interact with the dye molecules, and that increases the efficiency."Wang and his research team have reached an efficiency of 3.3 percent and hope to reach 7 to 8 percent after surface modification.  While lower than silicon solar cells, this efficiency would be useful for practical energy harvesting.  If they can do that, the potentially lower cost of their approach could make it attractive for many applications.By providing a larger area for gathering light, the technique would maximize the amount of energy produced from strong sunlight, as well as generate respectable power levels even in weak light.  The amount of light entering the optical fiber could be increased by using lenses to focus the incoming light, and the fiber-based solar cell has a very high saturation intensity, Wang said.Wang believes this new structure will offer architects and product designers an alternative PV format for incorporating into other applications."This will really provide some new options for photovoltaic systems," Wang said.  "We could eliminate the aesthetic issues of PV arrays on building.  We can also envision PV systems for providing energy to parked vehicles, and for charging mobile military equipment where traditional arrays aren't practical or you wouldn't want to use them."Wang and his research team, which includes Benjamin Weintraub and Yaguang Wei, have produced generators on optical fiber up to 20 centimeters in length.  "The longer the better," said Wang, "because longer the light can travel along the fiber, the more bounces it will make and more it will be absorbed."Traditional quartz optical fiber has been used so far, but Wang would like to use less expensive polymer fiber to reduce the cost.  He is also considering other improvements, such as a better method for collecting the charges and a titanium oxide surface coating that could further boost efficiency.Though it could be used for large PV systems, Wang doesn't expect his solar cells to replace silicon devices any time soon.  But he does believe they will broaden the potential applications for photovoltaic energy."This is a different way to gather power from the sun," Wang said.  "To meet our energy needs, we need all the approaches we can get."
bspice@cs.cmu.eduCarnegie Mellon leads NSF initiative to develop modeling tools for disease and complex systemsPITTSBURGHA multidisciplinary team led by Carnegie Mellon University computer scientist Edmund M. Clarke has received a five-year, $10 million grant from the National Science Foundation's Expeditions in Computing program to create revolutionary computational tools that will advance science on a broad array of fronts, from discovering new cancer treatments to designing safer aircraft.	The researchers will combine Model Checking and Abstract Interpretation, two methods that have been successful in finding errors in computer circuitry and software, and extend them so they can provide insights into models of complex systems, whether they are biological or electronic. 	Specifically, computer scientists, biomedical researchers and engineers from eight leading research institutions will use the techniques to better understand what causes deadly pancreatic cancer and the common heart rhythm problem known as atrial fibrillation. At the same time, they will use the techniques to study the embedded computer systems that are increasingly critical to the safe operation of aircraft and automobiles.	"Biological and embedded computer systems may be on opposite ends of the research spectrum, but they pose similar challenges for creating and analyzing computational models of their behavior," said Clarke, the FORE Systems University Professor of Computer Science and the 2007 winner of the Association for Computing Machinery's Turing Award, the computer science equivalent of the Nobel Prize. "Solutions to these problems at either end will enable new approaches to modeling across the spectrum that ultimately will improve health and safety. With this new initiative, I think we finally have achieved the critical mass of expertise and effort needed to crack these puzzles."	In addition to Clarke, who is one of the co-inventors of Model Checking, the research team includes project Deputy Director Amir Pnueli, a New York University computer scientist and a Turing Award winner for his work on systems verification. Among the other notables on the team are Patrick Cousot, an NYU computer scientist and co-inventor of Abstract Interpretation, and James Glimm, a National Medal of Science winner who heads the Department of Applied Mathematics and Statistics at the State University of New York at Stony Brook.	"Professor Clarke has truly assembled a dream team for this important new initiative," said Carnegie Mellon President Jared L. Cohon. "Computational modeling and simulation have become critical to discoveries in almost every scientific discipline, so finding new ways to build and explore these models will pay research dividends for years to come."	Carnegie Mellon is one of three lead institutions receiving the latest round of awards under the National Science Foundation's Expeditions in Computing program. The program, established last year by the Directorate for Computer and Information Science and Engineering (CISE), provides the CISE research and education community with the opportunity to pursue ambitious, fundamental research agendas that promise to define the future of computing and information and render great benefit to society. Funded at levels up to $2 million per year for five years, the Expeditions in Computing program represents some of the largest single investments currently made by the directorate.	Model Checking and Abstract Interpretation are the result of more than 30 years of research. Model Checking is the most widely used technique for detecting and diagnosing errors in complex hardware and software designs. It considers every possible state of a hardware or software design and determines if it is consistent with the designer's specifications; it produces counterexamples when it uncovers inconsistencies. It is limited, however, by the size of the systems it can analyze.	Abstract Interpretation, by contrast, doesn't attempt to look at every possible state of a system, but to develop a simplified approximation of a system that preserves the particular properties that need to be assessed. This makes it possible to analyze very large, complex systems, such as the one million lines of code in the Airbus A380's primary flight control system, but with less precision than is possible with Model Checking.	In this new project, the researchers plan to take advantage of the strengths of both methods by tightly integrating the two into what they call MCAI 2.0.  	One of the challenge problems driving this development involves modeling of pancreatic cancer, the fourth-leading cause of cancer deaths in the United States and Europe. Computer modeling is particularly important for discovering how this cancer develops and how it might be detected at an early, treatable stage because researchers have had trouble developing an animal model. Christopher Langmead, a Carnegie Mellon computer scientist, and James Faeder, a computational biologist at the University of Pittsburgh School of Medicine, will lead this effort, working with researchers at the Translational Genomics Research Institute.	"The death last year of our computer science colleague Randy Pausch, who had pancreatic cancer, made all of us at Carnegie Mellon appreciate the importance of improved models for this disease," Clarke said.	Atrial fibrillation, the most common form of heart rhythm disturbance, contributes to congestive heart disease and is responsible for 15 to 20 percent of strokes. Its incidence increases with age, so the aging demographics of America mean that this condition afflicting 2 to 3 million people today could be a problem for 10 million by 2050. A team led by Flavio Fenton, a biomedical researcher at Cornell University, and Radu Grosu, a computer scientist at SUNY at Stony Brook, will explore how modeling can enable physicians to predict the onset of atrial fibrillation.	A growing number of embedded systems are being integrated into cars  electronic stability control, anti-skid systems, hybrid powertrains, collision-avoidance systems  though the ability to develop models of how these systems interact with each other is severely limited. Rance Cleaveland, a computer scientist at the University of Maryland, and Bruce Krogh, a Carnegie Mellon electrical and computer engineer, will focus on distributed automotive control and electronic stability control as they lead the development of models that can help manufacturers integrate these systems into automobiles.	The aerospace industry has been a key driver of embedded software technology since the earliest weather-satellite launches of the 1960s, but it is now faced with exponential growth in the size and complexity of these systems in both spacecraft and commercial aircraft. With aircraft manufacturers seeking to better utilize microprocessors, NYU's Cousot and Gerard Holzmann, a computer scientist at NASA's Jet Propulsion Laboratory, will develop models that identify potential conflicts that can occur as microprocessors are shared between systems.	Research will be coordinated through a new Institute for Model Discovery and Exploration of Complex Systems, which will be headquartered in Carnegie Mellon's newly constructed Gates Center for Computer Science. 	Considerable education and outreach activities are planned. These include an interdisciplinary educational program in complex systems science directed by Scott Smolka, a computer scientist at SUNY at Stony Brook, and minority-focused summer workshops on complex embedded and biological systems under the direction of Nancy Griffeth, a computer scientist at the City University of New York's Lehman College.	Clarke emphasized that Carnegie Mellon will funnel the bulk of its project money to support graduate students, rather than faculty salaries. In addition to the NSF grant, the School of Computer Science and the Ray and Stephanie Lane Center for Computational Biology at Carnegie Mellon are providing supplemental support for the project.About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion comprehensive campaign, titled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty, students and innovative research, and enhance the physical campus with equipment and facility improvements. For more about Carnegie Mellon, visit http://www.cmu.edu/about/.
evelyn.brown@nist.govNew NIST publications describe standards for identity credentials and authentication systemsTwo publications from the National Institute of Standards and Technology (NIST) describe new capabilities for authentication systems using smart cards or other personal security devices within and outside federal government applications. A report describes a NIST-led international standard, ISO/IEC 24727, which defines a general-purpose identity application programming interface (API). The other is a draft publication on refinements to the Personal Identity Verification (PIV) specification.NIST is responsible for developing specifications for PIV cards required for the government under Homeland Security Presidential Directive 12. These smart cards have embedded chips that hold information and biometric data such as specific types of patterns in fingerprints called "minutiae" along with a unique identifying number. The goal is to develop methods that allow each worker to have a PIV card that works with PIV equipment at all government agencies and with all card-reader equipment regardless of the manufacturer.Because there is growing interest in using secure identity credentials like PIV cards for multiple applications beyond the federal workplace, NIST provided its smart card research expertise in the development of an international standardISO/IEC 24727  Identification cards  Integrated circuit card programming interfacesthat provides a set of authentication protocols and services common to identity management frameworks.The new NIST report, Use of ISO/IEC 24727 is an introduction to that standard. It describes the standard's general-purpose identity application programming interface, the "Service Access Layer Interface for Identity (SALII)", which allows cards and readers to communicate and operate with applications seamlessly. The report also describes a proof-of-concept experiment demonstrating that existing PIV cards and readers can work interoperably with ISO/IEC 24727. The applications tested included logging on to Windows or Linux systems, signing and encrypting email, and performing Web authentications.NIST Interagency Report 7611 Use of ISO/IEC 24727 may be downloaded at http://csrc.nist.gov/publications/nistir/ir7611/nistir7611_use-of-isoiec24727.pdf.NIST researchers also are involved in improving PIV components and providing guidelines that the private sector and municipalities can use with a similar smart ID card. They have drafted an update to an earlier publication that contains the technical specifications for interfacing with the PIV card to retrieve and use identity credentials.Special Publication 800-73-3, Interfaces for Personal Identity Verification, provides specifications for PIV-Interoperable and PIV-Compatible cards issued by non-federal issuers, which may be used with the federal PIV system. It also provides specifications designed to ease implementation, facilitate interoperability and ensure performance of PIV applications in the federal workplace. The new publication specifies a PIV data model, card edge interface and application programming interface. The report also provides editorial changes to clarify information in the earlier version. (For background, see "Updated Specification Issued for PIV Card Implementations," NIST Tech Beat, Oct. 14, 2008 [http://www.nist.gov/public_affairs/techbeat/tb2008_1014.htm].)The draft version of NIST SP 800-73-3 is open for public comment through Sept. 13, 2009. The document is available online at http://csrc.nist.gov/publications/PubsDrafts.html#800-73-3. Comments should be addressed to PIV_comments@nist.gov with "Comments on Public Draft SP 800-73-3" in the subject line.
avogel@gatech.eduGrant awarded to improve the security of mobile devices and cellular networksSmart phones -- like BlackBerrys and iPhones -- have become indispensable to today's highly mobile workforce and tech-savvy youngsters. While these devices keep friends and colleagues just a few thumb-taps away, they also pose new security and privacy risks. "Traditional cell phones have been ignored by attackers because they were specialty devices, but the new phones available today are handheld computers that are able to send and receive e-mail, surf the Internet, store documents and remotely access data -- all actions that make them vulnerable to a wide range of attacks," said Patrick Traynor, assistant professor in the School of Computer Science at the Georgia Institute of Technology.Traynor and Jonathon Giffin, also an assistant professor in the School of Computer Science, recently received a three-year $450,000 grant from the National Science Foundation to develop tools that improve the security of mobile devices and the telecommunications networks on which they operate. These Georgia Tech faculty, together with a team of graduate students, are developing methods of identifying and remotely repairing mobile devices that may be infected with viruses or other malware.Malware can potentially eavesdrop on user input or otherwise steal sensitive information, destroy stored information, or disable a device. Attackers may snoop on passwords for online accounts, electronic documents, e-mails that discuss sensitive topics, calendar and phonebook entries, and audio and video media. "Since mobile phones typically lack security features found on desktop computers, such as antivirus software, we need to accept that the mobile devices will ultimately be successfully attacked. Therefore our research focus is to develop effective attack recovery strategies," explained Giffin.The researchers plan to investigate whether cellular service providers -- such as AT&T and Verizon Wireless -- are capable of detecting infected devices on their respective networks. Since infected devices often begin to over-utilize the network by sending a high volume of traffic to a known malicious Internet server or by suddenly generating a high volume of text messages, monitoring traffic patterns on the network should allow these infected phones to be located, according to the researchers."While a single user might realize that a phone is behaving differently, that person probably won't know why. But a cell phone provider may see a thousand devices behaving in the same way and have the ability to do something about it," said Traynor.Once infected devices are located, those phones will need to be cleared of the malicious code. To accomplish this, the researchers are developing remote repair methods, which will allow service providers to assist in the cleaning of infected devices without requiring that the phones be brought to a service center. The methods will also have to work without much effort on the part of the customer.This repair may require disabling some functionality on the phone, such as the ability to use downloaded programs, until the malicious program is located and removed. While the repair is underway, phone calling and text messaging functionality would continue to operate."Using this remote repair strategy, the service provider no longer has to completely disable a phone. Instead they just put the device into a safe, but reduced, mode until the malware can be removed," said Giffin.To assess their proposed methods of finding and repairing infected mobile devices, the researchers plan to build a cellular network test bed at Georgia Tech that will simulate how cellular devices communicate over a network."We hope that developing these attack recovery strategies will let potential mobile phone and network attackers know that these response mechanisms are in place, ultimately making their attacks far less widespread or successful," said Traynor.This material is based upon work supported by the National Science Foundation (NSF) under Award No. CNS-0916047. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the researcher and do not necessarily reflect the views of the NSF.
dirk.ebling@ipm.fraunhofer.deEnergy-autonomous sensors for aircraftIf a bird collides with a plane the consequences can be fatal, not only for the creature itself. The impact can deform the structure of the aircraft fuselage, causing stresses in the material which can later turn into cracks. In future, sensors in the aircraft skin will detect such damage at an early stage and simplify maintenance and repair work. The sensors are light  they don't need any cables or batteries. They draw their energy from the temperature difference between the outside air (about minus 20 to minus 50 degrees Celsius) and the passenger cabin (about 20 degrees Celsius). Because there are no batteries to change, the sensors can be located at inaccessible places on the aircraft.EADS Innovation Works heads the development consortium. Researchers at the Fraunhofer Institute for Physical Measurement Techniques IPM in Freiburg are developing the energy supply system for the sensors. "We use thermoelectric generators, developed in cooperation with Micropelt GmbH, and adapt them so that they work efficiently," explains Dr. Dirk Ebling, scientist at the IPM. Thermoelectric materials are semiconductors which generate electric power under the influence of a temperature difference. If a number of these thermoelectric elements are connected in series, enough energy is produced to power small sensors as well as a radio device transmitting the measurement results to a central unit. "We are also optimizing the heat flow," the research scientist continues. A key question is how to couple the thermoelectric generator to the warm and cold environments so that it transports enough heat. To obtain the answer the scientists set up a climate chamber in which the temperature profile of the aircraft fuselage is simulated. The first optimized prototypes have already been built. Development of a prototype of the entire system including the sensor, thermoelectric generator, energy storage device, charging electronics and signal transmission module is scheduled for completion in about three years' time, hopefully enabling the system to enter series production.The applications for energy-autonomous sensors are numerous. In automobiles they could help to reduce weight by removing the need for heavy cable assemblies. They would also be useful in old buildings, where they could be easily affixed to walls e.g. to monitor dampness. Their use in the medical sector is feasible too. A sensor system integrated in a running shirt could monitor an athlete's pulse during training, and hearing aids could obtain their energy from body heat.
kkline@psychologicalscience.orgOur metallic reflection: Considering future human-android interactionsEveryday human interaction is not what you would call perfect, so what if there was a third party added to the mix  like a metallic version of us?  In a new article in Perspectives on Psychological Science, psychologist Neal J. Roese and computer scientist Eyal Amir from the University of Illinois at Urbana-Champaign investigate what human-android interactions may be like 50 years into the future. With knowledge of present day technology, the scientists predict that within 50 years androids will be able to speak in human-like voices, identify spoken words with precision, answer questions from a body of textual information, walk and run in a human-like motion, display realistic facial expressions, and detect others' emotions through visual processing.However, even with these advances, it will be more than 50 years before we see the human-acting and organic-looking androids of sci-fi movies. By 2060, it is predicted that androids will still be unable to detect aspects of natural language, and be incapable of forming conclusions from visual sensory input (specifically, seeing but not understanding). The most difficult development in artificial intelligence (AI) is trying to program the "Theory of Mind," or the effortless human ability to process other people's speech, actions, underlying motives, and emotional state.Roese and Amir predict that by 2060 androids will be used for menial jobs, such as toll collectors, where the presence of a non-human is practical, but not frightening. A major worker shift from people to androids, similar to the shift to machines in factories, is expected to occur.The psychological challenges of human-android interaction involve the absence of basic human functions such as blinking, body language, eye contact, and the coordination of personal space in an android, which could potentially make people uneasy when interacting with them.  But would people be more or less comfortable interacting with androids if they were ever indistinguishable from humans? Would stereotypes towards non-humans occur? Being unable to gauge who is human and who is not might cause confusion and fear in the public, even though we are the ones creating androids for our own benefit.  Roese and Amir conclude that the psychological impacts of human-android interaction must be considered in the present to shape android development in the future.Author Contact: Neal J. Roese roese@illinois.eduFor a copy of "HumanAndroid Interaction in the Near and Distant Future" and other articles in the Perspectives on Psychological Science special issue "The Next Big Questions in Psychology," visit www.psychologicalscience.org.Perspectives on Psychological Science is a journal of the Association for Psychological Science. It publishes an eclectic mix of thought-provoking articles on the latest important advances in psychology. For access to other Perspectives on Psychological Science research findings, please contact Katie Kline at 202-293-9300 or kkline@psychologicalscience.org.
lbrooks@rsna.orgImage sharing demonstration and pilot network link to PHRs and enable 'meaningful use'What:	The Image Sharing Demonstration at RSNA 2009 will encompass methods for sharing images, reports and related information for the improvement of patient care in radiology. The demonstration features patient control of documented medical history and imaging examinations through individual accounts in personal health record systems. The standards-based network infrastructure used in the demonstration will also be the foundation for an Image Sharing network pilot project currently under development by RSNA and five leading medical centers. That two-year project is funded under a $4.7 million contract from the National Institute of Biomedical Imaging and Bioengineering, with funds provided by the American Recovery and Reinvestment Act of 2009. The project meets objectives of the federal HITECH program for enabling "meaningful use" of electronic health records, which will be the basis of physician reimbursement incentives beginning in 2011.Who:	National Institute of Biomedical Imaging and Bioengineering 	  	Radiological Society of North America Interview opportunities with leaders from sponsoring organizationsWhere:  	95th Scientific Assembly and Annual Meeting of the Radiological Society of North America, Lakeside Center, McCormick Place, ChicagoSouth Building, Hall A  Booth 2843When:	Tuesday, Dec. 1, 3:00 p.m. CTWhy:	The Image Sharing Demonstration will show how institutions can provide better access to imaging information for patients and doctors by replacing image CDs with network access to images. The demonstration will convey how medical images and radiology reports can become part of a patient's personal health record, available securely via the Internet to patients and their doctors. The demonstration features leading vendors in medical imaging and electronic health records and is based on interoperability specifications from the Integrating the Healthcare Enterprise (IHE®) initiative."The goal is to provide patients with the interoperability necessary for easy, secure access to medical data and control of medical information," says David S. Mendelson, M.D., professor of radiology at the Mount Sinai School of Medicine in New York City and co-chair of the Board of IHE International.	Contact:	Press attendees must be registered as press through the RSNA Newsroom. Call 312-949-3233 to pre-register.  Newsroom staff will assist in arranging interviews.
daniel.parry@nrl.navy.milNRL sensor provides critical space weather observations(Washington, DC  Nov. 3, 2009)  Launched from Vandenberg Air Force Base, Calif., aboard an United Launch Alliance Atlas V launch vehicle, Oct. 18, 2009, the Special Sensor Ultraviolet Limb Imager (SSULI) developed by NRL's Space Science Division and Spacecraft Engineering Department offers a first of its kind technique for remote sensing of the ionosphere and thermosphere from space. Flying on-orbit the U.S. Air Force Defense Meteorological Satellite Program (DMSP) F18 (flight 18) satellite, SSULI's characterization of the Earth's upper atmosphere and ionosphere provide the necessary scientific data to support military and civil systems. The upper atmosphere affects many systems from global to tactical scales including GPS positioning, High-Frequency (HF) radio communications, satellite drag and orbit determination, and over the horizon radar."Space Weather can impact systems in both the military and civilian sectors," said Sean Lynch, program manager, NRL Spacecraft Engineering Department. "Observations acquired by SSULI will be processed at the Air Force Weather Agency to generate products to provide a more accurate representation of the atmosphere at a level that can directly affect civilian and military assets."   Offering global observations, that yield near real-time altitude profiles of the ionosphere and neutral atmosphere, over an extended period of time, SSULI makes measurements from the extreme ultraviolet (EUV) to the far ultraviolet (FUV) over the wavelength range of 80 nanometers (nm) to 170 nm with 2.4 nm resolution. Building on the successes of the NRL High Resolution Airglow/Aurora Spectroscopy (HIRAAS) experiment flown aboard the Space Test Program (STP) Advanced Research and Global Observations Satellite (ARGOS), SSULI also measures the electron density and neutral density profiles of the emitting atmospheric constituents. SSULI uses a spectrograph with a mirror capable of scanning below the satellite horizon from 10 degrees to 27 degrees every 90 seconds. These observations represent a vertical slice of the Earth's atmosphere from 750 kilometers (km) to 50 km in depth."The performance characteristics of this sensor are outstanding," said Andrew Nicholas, SSULI principal investigator, NRL Space Science Division. "SSULI measures vertical profiles of the natural airglow radiation from atoms, molecules and ions in the upper atmosphere and ionosphere and enables the development of new techniques for global ionospheric remote sensing and new models of global electron density variation." Models being developed include the Global Assimilation of Ionospheric Measurements. GAIM applies novel and sophisticated data assimilation techniques to space weather forecasting and will become an operational ionospheric model providing real-time ionospheric specification and forecasts for Department of Defense (DoD) and civilian customers.An extensive data processing suite was developed to support on-orbit observations and flight operations. It includes data reduction software using unique science algorithms developed at NRL and comprehensive data validation techniques. After launch, the SSULI sensor, software, and derived atmospheric specification will undergo an extensive validation. After validation, SSULI products will be distributed by AFWA to support operational DoD systems.The Defense Meteorological Satellite Program is a Department of Defense program run by the Air Force Space and Missile Systems Center (SMC). The program designs, builds, launches, and maintains several near-polar orbiting, sun synchronous satellites monitoring the meteorological, oceanographic, and solar-terrestrial physics environments. 	The Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA.
spolowc@clemson.eduClemson researchers receive EPA grant to study carbon emission storageCLEMSON  Clemson University researchers Ron Falta and Larry Murdoch have received an $891,000 Environmental Protection Agency grant to study the safe storage of carbon dioxide in geological formations located deep below the earth's surface.With carbon dioxide emissions from burning fossil fuels believed to be the leading cause of global warming, geologic storage of the gas is one of the most promising alternatives for reducing emissions using current technology."Geologic storage of carbon dioxide is considered to be a very secure way to isolate the carbon dioxide from the atmosphere for thousands of years.  However, there is a slight chance that some of the carbon dioxide could start to leak from some formations," said Falta. "Our work will help to identify the geologic conditions that may lead to leakage, and we will develop techniques for minimizing the impact of leaks if they do occur."With geologic storage, carbon dioxide is captured from coal, gas or oil-fired power plants, compressed, then injected into deep geologic formations. Suitable carbon dioxide-storage formations include depleted oil and gas reservoirs and saline aquifers. These formations are confined by impermeable caprocks and they typically are thousands of feet below the ground surface.Falta and Murdoch, professors in the department of environmental engineering and earth science, will focus on the behavior of carbon dioxide dissolved in saltwater at high pressure and methods to keep it safely away from shallow drinking water aquifers.Under EPA STAR Grant No. R834383, the researchers will work with Stanford University scientist Sally Benson, director of Stanford's Global Climate and Energy Project.Contact:Ron Falta864-656-0125faltar@clemson.edu
smackay@vt.eduImproved robotic hand captures mechanical engineering top awardBlacksburg, Va. -- The Virginia Tech College of Engineering's Robotics and Mechanisms Laboratory (RoMeLa) has captured another top award for its updated innovative robotic hand that can automatically change its grasping force using compressed air.A team of five undergraduate students won First Place in the American Society of Mechanical Engineers (ASME) Student Mechanism and Robot Design Competition at the International Design Engineering Technical Conference. The winning entry was RAPHaEL 2, a second-generation version of a robotic hand that previously won an award from the Compressed Air and Gas Institute.Held in San Diego, the ASME competition included undergraduate and graduate school teams. RoMeLa bested graduate student teams from MIT and the University of California Berkeley, and an undergraduate team from Purdue University, said Dennis Hong, director of RoMeLa and an associate professor with the Virginia Tech mechanical engineering (http://www.me.vt.edu) department.Student team members, all ME majors, are:The RAPHaEL (Robotic Air Powered Hand with Elastic Ligaments) series robotic hand is powered by compressed air and a novel accordion type tube actuator. Because the hand's grasping force and compliance is adjusted by changing the air pressure, it does not require the use of motors or other expensive and bulky actuators, Hong said.The most significant change for RAPHaEL 2 is the closed loop control mechanism and sensors for automatic position and force feedback of the fingers using LabVIEW and data acquisition hardware donated by National Instruments. The first version of RAPHaEL relied on solenoids with a microcontroller to operate. The material that comprises the hand also were changed to a durable polycarbonate material, replacing a fragile acrylic-based material that was prone to breakage, said Cothern. "This gives us a lot more control over the kinds of things we can do with the hand," said Cothern. "Eventually, we might be able to tell how soft an object you're grabbing is just by touching it." Additional tweaks to come: The ability to grasp small moving objects as well as the use of silicone, carbon fiber and other materials to make the hand lighter, simpler in structure, and also appear more human. As a possible prosthetic, the hand is easy to operate and its fingers are easy to replace if broken, Cothern said. RAPHaEL 2 is part of a larger RoMeLa project: The humanoid robot CHARLI (Cognitive Humanoid Robot with Learning Intelligence). Once the hand is connected to the larger body, it will be able to pick up  not just grasp and hold  objects as would a person, said Hong. CHARLI is expected one day to walk about campus giving tours of Virginia Tech to visitors and potential students. Learn more about RoMeLa: http://www.me.vt.edu/romela/Learn about Dennis Hong: http://www.me.vt.edu/people/faculty/hong.htmlRead about previous award: http://www.eng.vt.edu/news/article.php?niid=1686Learn more about the ASME competition: http://www.asmeconferences.org/idetc09/The College of Engineering (http://www.eng.vt.edu/) at Virginia Tech is internationally recognized for its excellence in 14 engineering disciplines and computer science. The college's 6,000 undergraduates benefit from an innovative curriculum that provides a "hands-on, minds-on" approach to engineering education, complementing classroom instruction with two unique design-and-build facilities and a strong Cooperative Education Program. With more than 50 research centers and numerous laboratories, the college offers its 2,000 graduate students opportunities in advanced fields of study such as biomedical engineering, state-of-the-art microelectronics, and nanotechnology. Virginia Tech, the most comprehensive university in Virginia, is dedicated to quality, innovation, and results to the commonwealth, the nation, and the world.
zfhan@ustc.edu.cnField experiment on a robust hierarchical metropolitan quantum cryptography networkKey Laboratory of Quantum Information (CAS), University of Science and Technology of China has recently demonstrated a metropolitan Quantum Cryptography Network (QCN) for Government Administration in Wuhu, China. Because of its scientific significance and social impact, the project is reported in Volume 54, Issue 17 (September, 2009) of the Chinese Science Bulletin authored by Fang-xing Xu et al.During the process of economic globalization, information security has become more and more important for both organizations and individuals. The secure communication is the basic requirement for all the confidential solutions to defend illegal eavesdropping and tampering. However, the security of a majority of classical cryptography is based on the complexity of the cipher algorithms and the development of distributed computing and specific hacking chips. Especially the quantum computer has become as a serious threat to classical cryptography nowadays. Consequently, a brand-new generation of quantum cryptography is refined as the urgent demand of secure communication.Quantum cryptography can distribute secret keys by encrypting the information in a quantum system, such as photons. It is founded on the principles of fundamental physics rather than assumptions about the resources available to a potential adversary, which is provably secure against any attack by eavesdroppers allowed by quantum mechanics. Combined with the quantum key distribution (QKD) and the "one-time pad" algorithm, quantum cryptography can establish unconditional secure communication between legal users, for now and the future. Moreover, in the process of QKD industrialization, networking is a milestone for the popularization of quantum cryptography service, especially a robust QCN compatible with the classical optical network which is a potential solution for the fast inflation of user number and unforeseen emergent demands of communication.Aiming at that, the Key Laboratory of Quantum Information (KLQI) built this brand-new quantum cryptography network. Compared with the prior network projects, Wuhu QCN implements hierarchical structure with multi-levels and contains three different existing networking techniques. Nodes with different priorities and demands are set in the central backbone net or the subnet, and choose suitable networking technique. All the QKD links are based on the BB84 protocol with decoy state method which can promise the security level for the communication. Meanwhile, QKD software that all nodes run, application programs for encrypting text messages, sound and video are developed as well.As the authors said in the paper that "In the process of QKD industrialization, the stability of the QKD system and the networking techniques are two heavy cruxes.", the Wuhu QCN implements the Faraday-Michelson Interferometer (FMI) system, an unidirectional QKD scheme with the strict proof of its security and stability which can auto-compensate the influence of the birefringence in the transmitting channel that will jeopardize the performance of QKD system. Several field demonstrations of KLQI group including Beijing-Tianjin QKD experiment (2004), four-port star type network in Beijing (2007) and the Wuhu quantum cryptography network for Government administration (2009) clearly show that the stability and robustness of this QKD basic device is sufficient for practical implementations.Networking is a milestone for the popularization of quantum cryptography service. However, the no-clone theorem of quantum system makes data traffic difficult to route in the net while guaranteeing the security of the protocol. The Wuhu cryptography network assembles the widely-used techniques of quantum router, active optical switch routing and trusted relay to construct a hierarchical and extendable structure. A full-mesh backbone network is built with a quantum router in the center to supply a no-congestion communication between all the gateways simultaneously, while the quantum switch based on the time multiplexing can achieve a balance for subnets between network efficiency and speed. In addition, trusted relay is a compromising method to extend the scale of the network as long as a practical quantum repeater is still missing. The whole implement of this hierarchical framework is a big step toward the actualization of practical large-scale quantum cryptography network.How to implement quantum cryptography into the practical utility is an essential problem as well. As a solution to the basic question to distribute secure key in the classical cryptography, quantum cryptography and quantum key distribution have a splendid prospective in the Internet and communication network for secure telephony, confidential fax and VPN etc. To some extend, Wuhu cryptography network is quite a creative and interesting attempt on the electronic administration. Massive data traffic of government confidential files and personal information obviously has the right to increase the secure level to "quantum" unconditional secure level. In the future, quantum cryptography will become widely spread as the sustainable development of secure media communication with instant video, sound and text message improves rapidly.It is the ultimate goal for all the security researchers to eliminate "Hackers" and "Trojan horses". Quantum cryptography as the earliest utility of quantum mechanism can supply an unconditional secure communication to benefit people. In the practical realization, QKD scheme's stability and key rate are not the only two important issues. Especially with the urgent and inflating demand of emergent quantum cryptographic service, networking and routing techniques should be taken into serious consideration, as well as the application mode of QCN. The hierarchical metropolitan QCN field in Wuhu cannot only serve public secure communication with QKD but also act as a test bed to research those problems in realizations and applications of QCN in depth.Supported by the National Fundamental Research Program of China (Grant No. 2006CB921900), the National Natural Science Foundation of China (Grant No. 60537020 and 60621064) and the Innovation Funds of Chinese Academy of Sciences Reference: 	Xu F X, Chen W, Wang S, et al. Field experiment on a robust hierarchical metropolitan quantum cryptography network. Chinese Science Bulletin, 2009, 54: 2991-2997, doi: 10.1007/s11434-009-0526-3
lhuang2@fau.eduRecognition at lastFace recognition computers can see through your disguiseA rapid but superior method for computerized face recognition could revolutionize security systems especially if it can see through disguises, according to research published in this month's issue of the International Journal of Intelligent Systems Technologies and Applications.Every face has special features that define that person, yet faces can also be very similar, explains Lin Huang, of Florida Atlantic University, in Boca Raton. That makes computerized face recognition for security and other applications an interesting but difficult task.Face recognition software has been in development for many years. However, for biometric authentication at border crossings, for access to buildings, for automated banking, crime investigation, and other applications, has not yet become a mainstream application. The main technical limitation is although the systems are accurate they require a lot of computer power.Early face recognition systems simply marked major facial features - eyes, nose mouth - on a photograph and computed the distances from these features to a common reference point. In the 1970s, a more automated approach using a facial template extended this idea to map the individual face on to a global template. By the 1980s, an almost entirely statistical approach led to the first fully automated face recognition system.In the late 1980s researchers at Brown University developed the so-called "eigenface method", which was extended by a team at MIT in the early 1990s. Since then, approaches based on neural networks, dynamic link architectures (DLA), fisher linear discriminant model (FLD), hidden Markov models and Gabor wavelets. Then a way to create a ghost-like image that would succumb to an even more powerful analysis was developed that could accurately identify the majority of differences between faces.However, powerful techniques have so far required powerful computers. Now, Huang and colleagues Hanqi Zhuang and Salvatore Morgera in the Department of Electrical Engineering, have applied a one-dimensional filter to the two-dimensional data from conventional analyses, such as the Gabor method. This allows them to reduce significantly the amount of computer power required without compromising accuracy.The team tested the performance of their new algorithm on a standard database of 400 images of 40 subjects. Images are grey scale and just 92 x 112 pixels in size. They found that their technique is not only faster and works with low resolution images, such as those produced by standard CCTV cameras, but also solves the variation problems caused by different light levels and shadows, viewing direction, pose, and facial expressions. It can even see through certain types of disguises such as facial hair and glasses."A method towards face recognition" in Int. J. Intelligent Systems Technologies and Applications, 2009, 7, 282-295
Joanne.Finlay@csiro.auCSIRO hosts Australia's first ICT summit in ChinaThe first summit between Australia and China on the topic of future information and communication technologies (ICT) is underway in Shanghai.	Sponsored by CSIRO and the Science and Technology Commission of Shanghai Municipality, the summit brings together almost 70 policy makers, industrial leaders and prominent researchers to focus on wireless broadband, sensor networks and e-health.	"These are three important areas for our economy, environment and our wellbeing," CSIRO Information Sciences Group Executive Dr Alex Zelinsky said in his opening remarks as Honorary Co-Chair of the summit.	"Australia's planned national broadband network will serve as a technology platform for the further development of these three areas," Dr Zelinsky said.	"For example, giving all Australians access to the same level of high quality healthcare regardless of where they live, enabling large scale networks of intelligent sensors that monitor our environment, and facilitating the establishment of smart energy grids." 	"Australia has a proud history in ICT research, with CSIRO playing a leading role in ICT technology development. 	"We are working with other Australian R&D organisations and, as today's event exemplifies, we are working very closely with our colleagues in China." 	Collaborations between Australia and China in ICT have been growing steadily. 	March this year saw the formation of the Australia-China Research Centre for Wireless Communications, a collaboration led by CSIRO and Beijing University of Posts and Telecommunications that has already attracted 14 partners.	"This summit furthers our work fostering broad, higher level collaborations which we began by establishing the Centre, " CSIRO ICT scientist and Australia-China Research Centre for Wireless Communications Director Dr Jay Guo said.	"These relationships will have a long-lasting positive effect on the prosperity of our two nations." 	The two-day summit concludes today in Shanghai.Image available at: http://www.scienceimage.csiro.au/mediarelease/mr09-295.htmlFurther Information: Dr Alex Zelinsky, CSIRO ICT CentrePh:02 9372 4200   E:Alex.Zelinsky@csiro.auBackground information available at: http://www.csiro.au/events/Aus-China-Info-and-Communication-Summit.html Media Assistance:Jo Finlay, CSIRO ICT Centre	Joanne.Finlay@csiro.auwww.csiro.au
mwdorsey@wpi.eduWPI professor to participate in Federal Trade Commission panel on online privacy concernsCraig Wills co-authored a study that demonstrated that more than half a billion users of popular social networking sites are at risk of having their personal information 'leaked' to third-party sites that track their web browsing habitsWORCESTER, Mass.  Craig Wills, professor of computer science at Worcester Polytechnic Institute (WPI), has been invited to participate in a Federal Trade Commission (FTC) panel on online privacy. Wills will take part in a panel on online behavioral advertising, which starts at 1:30 p.m. on Monday, Dec. 7, at the FTC Conference Center, 601 New Jersey Avenue, NW, Washington, D.C. He was selected to this panel because his research has demonstrated that more than half a billion social network users are at risk of having their personal information "leaked" to third-party sites that track their web browsing habits.The panel is part of a series of daylong public roundtable discussions the FTC is holding to explore the privacy challenges posed by technology and business practices, including social networking, cloud computing, and online behavioral advertising, that collect and use consumer data. The goal is to determine how best to protect consumer privacy while supporting beneficial uses of the information and technological innovation.The other participants on the online behavioral advertising panel are Jeff Chester, executive director of the Center for Digital Democracy; Amina Fazlullah, Counsel for U.S. PIRG; Dave Morgan, CEO of Simulmedia Inc.; Zoë Strickland, vice president and Chief Privacy Officer for Wal-Mart; Berin Szoka, director of the Center for Internet Freedom at the Progress & Freedom Foundation; Omar Tawakol, CEO of BlueKai; and Linda Woolley, executive vice president for government affairs for the Direct Marketing Association. The discussion will be moderated by Peder Magee and Michelle Rosenthal of the FTC's Division of Privacy and Identity Protection. The panel will be webcast live at www.ftc.gov/bcp/workshops/privacyroundtables/index.shtml.A recent study coauthored by Wills found that the practices of many popular social networking sites make the personal information that hundreds of millions of users post on the sites available to companies that track Web users' browsing habits, allowing them to link anonymous browsing habits to specific people. The study was the first to describe a mechanism that tracking sites could use to directly link browsing habits to specific individuals.Specifically, the study showed that when social networking sites pass information to tracking sites about a user's activities, they often include the user's unique identifier--a string of numbers or characters that points to their online profile. With this unique identifier, the tracking site can link the personal information in that profile (such items as user's name, physical address, email address, gender, birth date, educational and employment information) to data it has already gathered on the Web sites that the user has visited. "Now your browsing profile is not just of somebody," Wills says. "It is of you."Like most commercial websites, online social networks use third-party tracking sites, called aggregators, to learn about the browsing habits of their visitors. These third-party sites track browsing behavior using cookies. Cookies are maintained by a web browser and contain information that enable tracking sites to build profiles of the websites visited by a user. Each time the user visits a new website, the tracking site can review those cookies and serve up ads that might appeal to the user. For example, if the user frequently visits food sites, he or she might see an ad for a new cookbook.Social networking sites have gone a step further by allowing for transmission of unique identifiers. It is a particularly troubling practice for two reasons, Wills says. "First," he notes, "users put a lot of information about themselves on social networking sites. Second, a lot of that information can be seen by other users, by default. There are mechanisms users can use to limit access to their information, but we found through previous research that most users don't take advantage of them."Privacy "leakage" by social networking sites raises the possibility of a user's identity being linked to an inaccurate or misleading browsing profile (for example, when a computer is used by more than one person, or a person browses for curiosity rather than intent). "Tracking sites don't know, for example, if a site about cancer was visited out of curiosity, or because the user actually has cancer," Wills says. "Inaccurate profiling could potentially lead to issues with employment, health care coverage, or other areas of our personal lives."Wills says the researchers do not know what, if anything, tracking sites do with the unique identifiers that social networks transmit to them. They informed all of the sites they studied about their findings, but have not heard back officially from any. "We are not saying that they are necessarily trying to leak private information," he says. "But once someone is in possession of your unique identifier, there is so much they can learn about you."The researchers also note that while users of social networking sites can protect themselves to some degree by limiting the amount of information they post and using the protections the sites make available to them to limit access to their information, the easiest way to prevent privacy leakage would be for social networking sites to stop making unique identifiers visible.	View the full study here:http://conferences.sigcomm.org/sigcomm/2009/workshops/wosn/papers/p7.pdfAbout Worcester Polytechnic InstituteFounded in 1865 in Worcester, Mass., WPI was one of the nation's first engineering and technology universities. WPI's14 academic departments offer more than 50 undergraduate and graduate degree programs in science, engineering, technology, management, the social sciences, and the humanities and arts, leading to bachelor's, master's and PhD degrees. WPI's world-class faculty work with students in a number of cutting-edge research areas, leading to breakthroughs and innovations in such fields as biotechnology, fuel cells, and information security, materials processing, and nanotechnology. Students also have the opportunity to make a difference to communities and organizations around the world through the university's innovative Global Perspective Program. There are 25 WPI project centers throughout North America and Central America, Africa, Australia, Asia, and Europe.
avogel@gatech.eduHidden diversity in key environmental cleanup microbes found by systems biology assessmentResearchers have completed the first thorough, system-level assessment of the diversity of an environmentally important genus of microbes known as Shewanella. Microbes belonging to that genus frequently participate in bioremediation by confining and cleaning up contaminated areas in the environment.The team of researchers from the Georgia Institute of Technology, Michigan State University and the Pacific Northwest National Laboratory analyzed the gene sequences, proteins expressed and physiology of 10 strains of Shewanella. They believe the study results will help researchers choose the best Shewanella strain for bioremediation projects based on each site's environmental conditions and contaminants.The findings, which further advance the understanding of the enormous microbial biodiversity that exists on the planet, appear in the early online issue of the journal Proceedings of the National Academy of Sciences. This research was supported by the U.S. Department of Energy through the Shewanella Federation consortium and the Proteomics Application project.Similar to a human breathing in oxygen and exhaling carbon dioxide, many Shewanella microbes have the ability to "inhale" certain metals and compounds and convert them to an altered state, which is typically much less toxic. This ability makes Shewanella very important for the environment and bioremediation, but selecting the best strain for a particular project has been a challenge."If you look at different strains of Shewanella under a microscope or you look at their ribosomal genes, which are routinely used to identify newly isolated strains of bacteria, they look identical. Thus, traditional microbiological approaches would suggest that the physiology and phenotype of these Shewanella bacteria are very similar, if not identical, but that is not true," explained Kostas Konstantinidis, an assistant professor in the Georgia Tech School of Civil and Environmental Engineering. Konstantinidis, who also holds a joint appointment in the School of Biology, led the research team in analyzing the data.Using the traditional method for determining interrelatedness between microbial strains -- sequencing of the 16S ribosomal gene -- the researchers determined that the 10 strains belonged to the same genus. However, the technique was unable to distinguish between most of the strains or define general properties that would allow the researchers to differentiate one strain from another. To do that, they turned to genomic and whole-cell proteomic data. By comparing the 10 Shewanella genomes, which were sequenced at the Department of Energy's Joint Genome Institute, the research team found that while some of the strains shared 98 percent of the same genes, other strains only shared 70 percent. Out of the almost 10,000 protein-coding genes in the 10 strains, nearly half -- 48 percent -- of the genes were strain-specific, and the differences in expressed proteins were consistently larger than their differences at the gene content level. "These findings suggest that similarity in gene regulation and expression constitutes an important factor for determining phenotypic similarity or dissimilarity among the very closely related Shewanella genomes," noted Konstantinidis. "They also indicate that it might be time to start replacing the traditional microbiology approaches for identifying and classifying new species with genomics- or proteomics-based methods."Upon further analysis, the researchers found that the genetic differences between strains frequently reflected environmental or ecological adaptation and specialization, which had also substantially altered the global metabolic and regulatory networks in some of the strains. The Shewanella organisms in the study appeared to gain most of their new functions by acquiring groups of genes as mobile genetic islands, selecting islands carrying ecologically important genes and losing ecologically unimportant genes.The most rapidly changing individual functions in the Shewanellae were related to "breathing" metals and sensing mechanisms, which represent the first line of adaptive response to different environmental conditions. Shewanella bacteria live in environments that range from deep subsurface sandstone to marine sediment and from freshwater to saltwater. All but one of the strains was able to reduce several metals and metalloids. That one exception had undertaken a unique evolution resulting in an inability to exploit strictly anaerobic habitats."Let's say you have a strain of Shewanella that is unable to convert uranium dissolved in contaminated groundwater to a form incapable of dissolving in water," explained Konstantinidis. "If you put that strain in an environment that contains high concentrations of uranium, that microbe is likely to acquire the genes that accept uranium from a nearby strain, in turn preventing uranium from spreading as the groundwater flows."This adaptability of bacteria is remarkable, but requires further study in the bioremediation arena, since it frequently underlies the emergence of new bacterial strains. Konstantinidis' team at Georgia Tech is currently investigating communities of these Shewanella strains in their natural environments to advance understanding of the influence of the environment on the evolution of the bacterial genome and identify the key genes in the genome that respond to specific environmental stimuli or conditions, such as the presence of heavy metals. Ongoing studies should broaden the researchers' understanding of the relationship between genotype, phenotype, environment and evolution, he said.
jfhirsch@mit.eduA new kind of micro-mobilityMethod of moving tiny particles using magnetic fields could find uses in microchips and in medicineA new microscopic system devised by researchers in MIT's department of materials science and engineering could provide a novel method for moving tiny objects inside a microchip, and could also provide new insights into how cells and other objects are propelled around within the body.Inside organs such as the trachea and the intestines, tiny hair-like filaments called cilia are constantly in motion, beating in unison to create currents that sweep along cells, nutrients, or other tiny particles. The new research uses a self-assembling system to mimic that kind of motion, providing a simple way to move particles around in a precisely controlled way.Alfredo Alexander-Katz, the Toyota Career Development Assistant Professor of Materials Science and Engineering, and his doctoral student Charles Sing and other researchers, devised a system that uses tiny beads made of polymers with specks of magnetic material in them. With these beads suspended in a liquid, they applied a rotating magnetic field, which caused the beads to spontaneously form short chains which began spinning, creating currents that could then carry along surrounding particles  even particles as much as 100 times larger than the beads themselves.Alexander-Katz refers to the microscopic beads  each just one micron (a millionth of a meter) in diameter  as "micro-ants," because of their ability to move along while "carrying" objects so much larger than themselves. A paper describing the research will appear the week of Dec. 14 in the Proceedings of the National Academy of Sciences.The new method could provide a simpler, less-expensive alternative to present microfluidic devices, a field that is still in its early stages of development. Now, such devices require precisely made channels, valves and pumps created using microchip manufacturing methods, in order to control the movement of fluids through them. But the new system could offer such precise control over the movement of liquids and the particles suspended in them that it may be possible to dispense with the channels and other plumbing altogether, controlling the movements entirely through variations in the applied magnetic field. In short, software rather than hardware could control the chip's properties, allowing it to be instantly reconfigured through changes in the controlling software  and approach Alexander-Katz refers to as "virtual microfluidics." This could reduce the cost and increase the flexibility of the devices, which might be used for such things as biomedical screening or the detection of trace elements for pollution monitoring or security screening. It might also provide even finer spatial control than can presently be achieved using conventional channels on chips.Alexander-Katz says the work might also help biologists better understand the way cilia work, by providing a way to mimic their activity in the lab. "People are still trying to understand how you get synchronization in the system" of cilia in organisms. "This might be a way to test many of the theories."He says the way the chains of beads moved is a bit like a person trying to do cartwheels while standing on an icy surface. "As they rotate, they slip a bit," he says, "but overall, they keep moving," and this imparts a directional flow to the surrounding fluid. Ultimately, such a system might someday even be developed to use in medical diagnostics, by allowing controlled delivery of particles inside the body to specifically targeted locations, for example while the patient is in a nuclear magnetic resonance (NMR) imaging system.
hickeyh@uw.eduLow-cost temperature sensors, tennis balls to monitor mountain snowpackFictional secret agent Angus MacGyver knew that tough situations demand ingenuity. Jessica Lundquist takes a similar approach to studying snowfall. The University of Washington assistant professor of civil and environmental engineering uses dime-sized temperature sensors, first developed for the refrigerated food industry, and tennis balls. In summer months she attaches the sensors to tennis balls that are weighted with gravel, and uses a dog-ball launcher to propel the devices high into alpine trees where they will record winter temperatures.	This isn't TV spy work  it's science. Lundquist studies mountain precipitation to learn how changes in snowfall and snowmelt will affect the communities and environments at lower elevations. If the air temperature is above 32 degrees Fahrenheit the precipitation will fall as rain, but if it's below freezing, it will be snow.	"It's fun, like backyard science," Lundquist said of her sensors, which were originally designed to record temperature of frozen foods in transit. She began adapting the devices for environmental science while a postdoctoral researcher in Colorado and has refined them over the years. "It turns out they work phenomenally well." 	Last year the American Geophysical Union awarded Lundquist its Cryosphere Young Investigators Award for her fieldwork. This week at the AGU's fall meeting in San Francisco she will present her low-cost temperature-sensing technology and some current applications.	Scientific weather stations typically cost about $10,000. Lundquist's system measures and records the temperature every hour for up to 11 months in remote locations for just $30 apiece. Another advantage is that they are easily deployed in rough terrain.	Her temperature sensors are a fun approach to studying a serious problem. One quarter of the Earth's continents have mountainous terrain, Lundquist said, and mountain rivers provide water for 40 percent of the world's population. Those mountain rivers are largely fed by snowmelt. But if winters become warmer due to climate change, the snow line is expected to inch up the mountainside, and snow is expected to melt earlier in the springtime. 	"Mountains are the water towers of the world," Lundquist said. "We essentially use the snow as an extra reservoir. And you want that reservoir to hold the snow for as long as possible."	Her sensors are being used to improve computer models in areas where water managers want to know exactly where snow is accumulating and on what date it starts to melt.	"People typically assume that temperature decreases with elevation," Lundquist says. But actual mountain temperatures depend on the vegetation, slope and variable weather. "If you have a management decision, there's a specific place you have to make a decision for."	If more rain falls instead of snow, it will increase the risk of flooding during storms. Lundquist's sensors are currently being used by the California-Nevada River Forecasting Center as part of a project pinpointing at what elevation snow turns to rain, to improve storm flooding forecasts. As part of that project, UW graduate students are placing her sensors in river canyons that are too steep for traditional weather stations.   	She is also deploying sensors in Yosemite National Park to see if earlier snowmelt may cause earlier drying of streambeds and affect vegetation growth in the Tuolumne Meadows. Her sensors there provide ground verification of satellite measurements.	The City of Seattle is also using Lundquist's sensors to study how different restoration approaches for trees in the Cedar River watershed, which supplies water to the city, affect snow retention. 	"I have a lot of fun deploying my sensors because I love being in the mountains," Lundquist said. "They also sense conditions in these remote environments that we can't know about any other way."For more information, contact Lundquist at jdlund@uw.edu or 206-685-7594. She will be at the AGU until Thursday and will be checking e-mail once a day. 
Marina.Johnson@csiro.auNew analyzers to unlock mineral valueScientists are working on a new range of materials characterisation analysers and techniques that could help unlock the value contained in Australia's mineral deposits and improve processing performance, according to the October issue of Process.Machine-mounted sensors, being developed through CSIRO Minerals Down Under Flagship, could help locate ore deposits, characterise the mining environment, and differentiate ore grades.  This will enable automated mining machines to respond 'intelligently' to the changing detail of the environment and offer real-time amendments to the mine plan.Another prototype in development combines the best features of two existing materials characterisation techniques  x-ray diffraction and x-ray fluorescence  into a new slurry analyser. The new prototype, dubbed XRDF for its dual origins, is capable of measuring both mineralogy and ultra-low elemental composition directly on a process-stream, without the need for labour-intensive, time-consuming and potentially error-prone sampling. CSIRO scientist Dr James Tickner said the new prototype could offer a number of benefits over existing on-stream analysers."We're not aware of any other system capable of doing accurate, on-stream mineralogy," Dr Tickner said. "The ability to detect elements at parts-per-billion levels in an on-stream system is unique."Dr Tickner and his team are also working on gamma-activation analysis  a new analysis method that may deliver all the benefits of neutron activation without the need for a nuclear reactor. The method is expected to provide accurate, multi-element analysis of mineral samples without extensive sample preparation, and measure very low levels of more than 30 elements in samples weighing just a few hundred grams. The method could significantly improve sampling accuracy.Other stories in this issue of Process include:These and other stories can be found in the October issue of Process, which will be released on Thursday 8 October.  A pdf of the magazine is available now at: www.csiro.au/resources/Process-Oct-09.htmlImage available at: http://www.scienceimage.csiro.au/mediarelease/mr09-170.htmlMedia Assistance:Marina Johnson, Editor, ProcessPh: 03 9545 8746	Mb: 0422 393 486	E: Marina.Johnson@csiro.auwww.csiro.au
rmackar@niehs.nih.govElectronic nose sniffs out toxinsImagine a polka-dotted postage stamp-sized sensor that can sniff out some known poisonous gases and toxins and show the results simply by changing colors. Support for the development and application of this electronic nose comes from the National Institute of Environmental Health Sciences, part of the National Institutes of Health. The new technology is discussed in this month's issue of Nature Chemistry and exemplifies the types of sensors that are being developed as part of the NIH Genes, Environment and Health Initiative (GEI) (http://www.gei.nih.gov/index.asp). Once fully developed, the sensor could be useful in detecting high exposures to toxic industrial chemicals that pose serious health risks in the workplace or through accidental exposure. While physicists have radiation badges to protect them in the workplace, chemists and workers who handle chemicals do not have equivalent devices to monitor their exposure to potentially toxic chemicals. The investigators hope to be able to market the wearable sensor within a few years. "The project fits into the overall goal of a component of the GEI Exposure Biology Program that the NIEHS has the lead on, which is to develop technologies to monitor and better understand how environmental exposures affect disease risk," said NIEHS Director Linda Birnbaum, Ph.D. "This paper brings us one step closer to having a small wearable sensor that can detect multiple airborne toxins."The paper's senior author is Kenneth S. Suslick, Ph.D., the M.T. Schmidt Professor of Chemistry at the University of Illinois at Urbana-Champaign. Suslick and his colleagues have created what they refer to as an optoelectronic nose, an artificial nose for the detection of toxic industrial chemicals (TICs) that is simple, fast, inexpensive, and works by visualizing colors. "We have a disposable 36-dye sensor array that changes colors when exposed to different chemicals. The pattern of the color change is a unique molecular fingerprint for any toxic gas and also tells us its concentration," said Suslick. "By comparing that pattern to a library of color fingerprints, we can identify and quantify the TICs in a matter of seconds."The researchers say older methods relied on sensors whose response originates from weak and highly non-specific chemical interactions, whereas this new technology is more responsive to a diverse set of chemicals. The power of this sensor to identify so many volatile toxins stems from the increased range of interactions that are used to discriminate the response of the array. To test the application of their color sensor array, the researchers chose 19 representative examples of toxic industrial chemicals. Chemicals such as ammonia, chlorine, nitric acid and sulfur dioxide at concentrations known to be immediately dangerous to life or health were included. The arrays were exposed to the chemicals for two minutes. Most of the chemicals were identified from the array color change in a number of seconds and almost 90 percent of them were detected within two minutes. The laboratory studies used inexpensive flatbed scanners for imaging. The researchers have developed a fully functional prototype handheld device that uses inexpensive white LED illumination and an ordinary camera, which will make the whole process of scanning more sensitive, smaller, faster, and even less expensive. It will be similar to a card scanning device. "One of the nice things about this technology is that it uses components that are readily available and relatively inexpensive," said David Balshaw, Ph.D., a program administrator at the NIEHS. "Given the broad range of chemicals that can be detected and the high sensitivity of the array to those compounds, it appears that this device will be particularly useful in occupational settings."The NIEHS supports research to understand the effects of the environment on human health and is part of NIH. For more information on environmental health topics, visit our Web site at http://www.niehs.nih.gov. The National Institutes of Health (NIH)  The Nation's Medical Research Agency  includes 27 Institutes and Centers and is a component of the U.S. Department of Health and Human Services. It is the primary federal agency for conducting and supporting basic, clinical and translational medical research, and it investigates the causes, treatments, and cures for both common and rare diseases. For more information about NIH and its programs, visit www.nih.gov. Reference(s): Lim SH, Feng L, Kemling JW, Musto CJ, Suslick KS. 2009. An Optoelectronic Nose for Detection of Toxic Gases. Nature Chemistry. Published Online in Advance of Print. DOI: 10.1038/NCHEM.360.
ghunka@aftau.orgZooming in to catch the bad guysNew 'perfection tool' from Tel Aviv University enhances video to catch criminals and terroristsIt's a frequent scene in television crime dramas: Clever police technicians zoom in on a security camera video to read a license plate or capture the face of a hold-up artist. But in real life, enhancing this low-quality video to focus in on important clues hasn't been an easy task. Until now. Prof. Leonid Yaroslavsky of Tel Aviv University and his colleagues have developed a new video "perfection tool" to help investigators enhance raw video images and identify suspects. Commissioned by a defense-related company to improve what the naked eye cannot see, the tool can be used with live video or with recordings, in color or black-and-white. "This enhancement of resolution can be a critical factor in locating terrorists or identifying criminal suspects," says Prof. Yaroslavsky. His team's findings were recently published in Optical Letters and the Journal of Real Time Image Processing. Seeing using computational imagingThe new invention enhances the resolution of raw video images from security cameras, military binoculars, and standard personal-use video cameras, improving the quality at which the images were originally recorded or transmitted. This can mean the difference between "seeing" trees blowing in the wind and finding a terrorist hiding in those trees. "Our video perfection tool works to improve visual quality and achieving a higher resolution of the video image," says Prof. Yaroslavsky. Once a commercial partner is found, the device can be integrated into existing technology within a matter of months, he says. Digitally calming the "turbulent atmosphere"A major challenge in video analysis is that images of objects become distorted over long distances due to variations in the air that can affect our sight and the "sight" of a camera. In the language of optical science, this is known as a "turbulent atmosphere." A critical image of a person or object can become unstable and almost impossible to identify with any amount of accuracy. The TAU team exploited the fact that most parts of a video scene remain still. While there are moving objects such as people, animals or vehicles, a major part of the video ― the background  does not move at all. Using specially designed algorithms, the team built a software application that lets cameras and video analysis equipment stabilize images, allowing objects that are really moving to be distinguished from chaotic atmospheric changes. The technology will increase the odds of identifying suspects in court, says Prof. Yaroslavsky, but its civilian applications are equally significant. Instead of sending large video files over the Internet, smaller and lower-resolution files could be sent, to be enhanced at their destination points. This could save bandwidth and time. "It's quite a new approach to video perfection," says Prof. Yaroslavsky. "A lot of work has been done in this field, so it's very gratifying to find a new and original application." 	American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
swaney@andrew.cmu.eduCarnegie Mellon researcher wins NSF grantCreating new computer tools	PITTSBURGHCarnegie Mellon University's Ole Mengshoel was awarded a two-year, $498,000 grant from the National Science Foundation to create new computer tools for improving and integrating the way information is displayed and analyzed.	"The project will develop new techniques that integrate visualization processes and analytical tools, including data mining and machine learning, that will enable improved monitoring of large utility projects," said Mengshoel, a senior systems scientist at Carnegie Mellon Silicon Valley.   	Mengshoel reports that his novel monitoring and data mining algorithms will provide improved operation estimates for large-scale networks like those used to power the nation's critical electricity power grid. 	Because the quality of what industry is getting and creating in terms of large-scale systems needs improvement, Mengshoel's research team is dedicated to making large networks, including electrical power and computer networks, work more robustly, efficiently and economically. Other Carnegie Mellon research team members include Marija Ilic, a professor of electrical and computer engineering and engineering and public policy, and Ted Selker, associate director of the CyLab Mobility Research Center at Carnegie Mellon Silicon Valley. Ilic is an expert in electrical engineering and electrical power systems, while Selker is an expert in computational visualization and user interfaces. 	In addition to the research, the grant also helps support future software development projects and the training of both undergraduate and graduate students. Students will be engaged in the project in several ways, including through the use of the Advanced Diagnostics and Prognostics Testbed (ADAPT)  a real-world electrical power system that is similar to the power system often used in vehicles and micro-grids.  	"As the global population grows and energy demand increases, and generation shifts toward the use of renewable energy sources such as solar and wind, we all need to learn to create and use energy more reliably and efficiently, and our research will help us to push energy economy to the extreme," Mengshoel said.	Carnegie Mellon Silicon Valley, established in 2002 at Moffett Field, offers full-time and part-time master's degree programs in software engineering, software management, networking, security and mobility, and engineering and technology innovation management.  The campus also offers an associated bi-coastal electrical and computer engineering Ph.D. program focused on mobility, security and networking.      	About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology, and business to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion fundraising campaign, titled: "Inspire Innovation: The Campaign for Carnegie Mellon University,'' which aims to build its endowment, support faculty, students and innovative research and enhance the physical campus with equipment and facility improv
codyja@msu.eduNew mobile lab allows MSU researchers to study air quality, health effectsRoving lab will analyze fine air particlesEAST LANSING, Mich.  A new mobile air research laboratory will help a team of researchers led by a Michigan State University professor better understand the damaging health effects of air pollution and why certain airborne particles  emitted from plants and vehicles  induce disease and illness.Jack Harkema, a University Distinguished Professor of pathobiology and diagnostic investigation in the College of Veterinary Medicine, will deploy the new 53-foot, 36,000-pound center  dubbed "AirCARE 2"  throughout southern Michigan, including metropolitan Detroit."The mobile laboratory allows us to analyze 'real-world' pollution in communities that may be at risk," he said. "We can study why certain ailments, such as asthma, cardiovascular disease and even obesity, may be more pronounced after exposure to particulate air pollution."With about 450 square feet of indoor laboratory space, the $400,000 center helps researchers study fine and ultrafine particles in air pollution. These small particles have been found to increase mortality and morbidity among susceptible people with pre-existing health conditions such as heart disease. Housed in a converted semitrailer, the mobile laboratory pulls air from the surrounding atmosphere through an air-particle concentrator, allowing the scientists to selectively collect the particles and analyze for chemical components that may be responsible for damaging health effects. Researchers can study the subtle effects of controlled particle exposure on both laboratory animals and human subjects, providing clues on why and how pollutant particles are so harmful to the heart and lungs. Harkema works closely with environmental and biomedical researchers from the University of Michigan on the projects."We know particles in the air can exacerbate pre-existing respiratory and cardiovascular disease in people," Harkema said. "We need to understand why. There are many different components to air pollution, and we want to determine which of these are most harmful and where there come from."The addition of the new mobile laboratory allows Harkema and U-M collaborators Robert Brook, a cardiologist, and Gerald Keeler, an atmospheric scientist, to conduct a new study funded by the Environmental Protection Agency. As part of the project, Harkema, Brook and Keeler will deploy AirCARE 2 in rural southeastern Michigan to study the cardiovascular health effects of transported air pollution originating from distant emission sites in Michigan or adjacent states.AirCARE 2 was partly funded through the MSU strategic partnership grant, the Michigan Agricultural Experiment Station, the College of Veterinary Medicine and the Office of the Vice President for Research and Graduate Studies. The new fine particle concentrator in the AirCARE 2 received some funds from the Electric Power Research Institute and the American Petroleum Institute.The first MSU Mobile Air Research Laboratory, AirCARE 1, currently spends six months of the year in metro Detroit conducting air pollution studies and then six months in Los Angeles as part of a six-university partnership known as the federal Southern California Particle Center in California. The $8 million partnership, funded by the EPA and led by UCLA, is a five-year endeavor to investigate how exposure to airborne particles affects health and how the impact varies with the source, chemical composition and physical size.	Michigan State University has been advancing knowledge and transforming lives through innovative teaching, research and outreach for more than 150 years. MSU is known internationally as a major public university with global reach and extraordinary impact. Its 17 degree-granting colleges attract scholars worldwide who are interested in combining education with practical problem solving.
dguerin@latech.eduLouisiana Tech receives DOE grant for cyberspace education programsRUSTON, La  The U.S. Department of Education has awarded Louisiana Tech University a $951,000 grant to support interdisciplinary cyberspace and science education programs throughout northern Louisiana.Dr. Galen Turner, associate professor of mathematics and associate dean of graduate studies for Tech's College of Engineering and Science, has worked with professors from a number of different disciplines on the Tech campus to develop Cyber K-12: Building a foundation for cyber education in North Louisiana.Cyber K-12 will provide professional development opportunities for K-12 educators throughout northern Louisiana, yielding an increased number of teachers who will gain insight into our nation's cyber challenges.  The project is a product of Louisiana Tech's STEM Talent Expansion Program and builds upon a strong collaborative partnership with the Cyber Innovation Center (CIC) in Bossier City."This project will help advance Tech's cyberspace initiatives by further developing the high school professional development model produced by our engineering and science faculty over the past few years," says Turner."Cyber K-12 has also led to the recently approved Integrated STEM Education Research Center (ISERC) housed in the College of Engineering and Science.  This Department of Education grant will be pivotal in continuing to advance the university as a leader in STEM education in North Louisiana."The strength of Cyber K-12 is rooted in Louisiana Tech's highly-interdisciplinary approach to cyber education.  The Cyber Discovery Summer Camp, for example, is a collaborative between Tech and the CIC that exposes student and teachers to the technological, social, political and historical aspects of cyber."It shows students how life is interconnected and that they must pay attention to all of the issues surrounding the real problems that we face as a society," says Turner.Programs such as Cyber K-12 and the Cyber Discovery Summer Camp are serving as a model for interdisciplinary education and continue to place Louisiana Tech on the national stage.Government entities such as the Department of Education have recognized the need to support and fund institutions that are working to secure the nation's cyber domain."National leaders have emphasized the great importance in protecting the U.S. from cyber attacks," says Dr. Les Guice, vice president for research and development at Louisiana Tech."This grant will leverage other investments and initiatives underway in North Louisiana to position our region as a national resource for cyber research and for a highly skilled cyber workforce."Working with its partners at the Cyber Innovation Center as well as through members of the Consortium for Education, Research and Technology (CERT) of North Louisiana, Louisiana Tech anticipates that programs such as Cyber K-12 will be replicated at other colleges and universities around the nation."Cyber K-12 and the Cyber Discovery Camp initiatives engage students.  They provide a mechanism for teachers to partner and mentor with students to explore new worlds," says G.B. Cazes, vice president of the Cyber Innovation Center.  "We're still teaching science, math and engineering concepts; we're just doing in a much more fun and dynamic way."Turner envisions a world of opportunity for those involved in the future of cyber education."Cyber K-12 can show teachers and students alike just how much of an opportunity they have to study this emerging field with the potential to be leaders in the field in the next 10-20 years."
jbardi@aip.orgAIP awards Industrial Physics PrizeRobert Street of Palo Alto, Calif., recognizedOctober 30, 2009The American Institute of Physics (AIP) is awarding the 2010 Prize for Industrial Applications of Physics next month to Robert Street of the Palo Alto Research Center (PARC) in California. The prize is supported by General Motors.Street's pioneering work at PARC in the early 1990s led to the development of flat-panel digital X-ray detectors, a commercially-available technology that has replaced traditional film X-ray machines for many medical applications.These devices can be found in hospitals and clinics across the United States today, where they assist doctors in diagnosing diseases like breast cancer, helping to save lives. AIP governing board member Rudy Ludeke will present Street with the $10,000 prize and a certificate at a ceremony on November 11 during the AVS 56th International Symposium & Exhibition in San Jose, CA.The secret to Street's flat-panel digital X-ray detector is a dense glass-like material known as amorphous silicon that can be vacuum deposited onto surfaces and formed into electronic devices. Today, it is a standard material used in the manufacture of $100 billion dollars worth of electronics sold each year -- devices like laptop displays and flat panel TV sets.In the 1980s, the material was just starting to find its way into electronics, and Street spent much of that decade studying the basic properties of this material. In the mid-1980s, a PARC team designed a new sensor array for a photocopier and printer based on amorphous silicon.Street soon realized that a similar sensor array using the same material might be able to detect X-rays and capture images of the human body completely electronically. By the early 1990s, he and his colleagues had worked out many of the technological hurdles necessary to do so."We went ahead and made the first X-ray imaging device," Street says. He recalls that he never doubted whether such a detector could be built. The only question was whether the device would be effective in a clinical setting, and could be manufactured with the reliability needed for a medical device.Early tests showed promise, and in 1996, PARC spun off a start-up company called dpiX, Inc. that began commercializing this digital X-ray technology, which has since become a large and growing segment of the medical imaging industry.To take just a single example of how digital X-ray technology is impacting medicine, about half of all mammography systems in the United States today are digital -- up from about a third just a year ago. This growth has come in part because the results of a large study comparing digital with film mammography involving 50,000 women showed in 2005 that the diagnostic accuracy of digital is just as good as film overall. Though digital systems tend to be more expensive, they offer significant advantages in terms of image quality and electronic storage and manipulation. Moreover, digital systems have proven more accurate at diagnosing breast cancer in women under the age of 50, women with dense breasts, and premenopausal or perimenopausal women. Born and raised in Birmingham, England, Robert Street received a Ph.D. in 1971 from Cambridge University for work on the physics of chalcogenide glasses. He was a postdoc at Sheffield University, and then a visiting scientist to the Max Planck Institute in Stuttgart, Germany. He joined PARC in 1976, where he is now a Senior Research Fellow. He is also a Fellow of the American Physical Society and the Materials Research Society.His current research explores finding novel low-cost and large-area electronics for applications ranging from new flat panel displays to radiation sensors. Projects he has been involved with in recent years include ink-jet printing of organic electronic devices, constructing flexible electronic displays, developing technology for truck-size scanners for homeland security, and researching new solar cell structures. As of 2009, Street is the author of about 400 papers, several books and book chapters, and 60 patents.Street and his wife live in Palo Alto, CA. They have two grown children and one grandchild.He will receive his prize at an awards ceremony and reception beginning at 6:15 p.m. on Wednesday, November 11, 2009 in Ballrooms A2-7 of the San Jose Convention Center. The ceremony is part of the AVS 56th International Symposium & Exhibition, which convenes from November 8-13 in San Jose, CA. See: http://www2.avs.org/symposium/	ABOUT THE PRIZEEstablished in 1977, the Prize for Industrial Applications of Physics recognizes outstanding contributions by an individual or individuals to the industrial applications of physics.  The American Institute of Physics (AIP) Corporate Associates and the American Physical Society (APS) alternate in co-sponsoring this $10,000 award with General Motors.  Where the AIP award recognizes scientists who have developed proven technologies, the APS award recognizes research that has excellent potential for future success. For more information, see: http://www.aip.org/industry/prize/ABOUT AIPThe American Institute of Physics is a federation of 10 physical science societies representing more than 135,000 scientists, engineers, and educators and is one of the world's largest publishers of scientific information in physics. Offering full-solution publishing services for scientific societies and for similar organizations in science and engineering, AIP pursues innovation in scholarly journal publishing. AIP publishes 12 journals (some of which are the most highly cited in their respective fields); two magazines, including its flagship publication Physics Today; and the AIP Conference Proceedings. Its online publishing platform Scitation hosts nearly two million articles from more than 185 scholarly journals and other publications of 28 learned society publishers. See: http://www.aip.org
evelyn.brown@nist.govNIST test proves 'the eyes have it' for ID verificationThe eyes may be the mirror to the soul, but the iris reveals a person's true identityits intricate structure constitutes a powerful biometric. A new report by computer scientists at the National Institute of Standards and Technology (NIST) demonstrates that iris recognition algorithms can maintain their accuracy and interoperability with compact images, affirming their potential for large-scale identity management applications such as the federal Personal Identity Verification program, cyber security and counterterrorism.After fingerprints, iris recognition has emerged in recent years as the second most widely supported biometric characteristic. This marketplace rests, in large part, on the ability of recognition algorithms to process standard images from the many cameras now available. This requires images to be captured in a standard format and prepared so that they are compact enough for a smart card and for transmission across global networks. The images also have to be identifiable by computer algorithms and interoperable with any iris-matcher product regardless of the manufacturer.NIST scientists are working with the international biometrics community to revise iris recognition standards and to advance iris images as the global interchange medium in this rapidly evolving field.NIST established the Iris Exchange IREX program as a NIST-industry collaboration to encourage development of iris recognition algorithms operating on images conforming to the new ISO-IEC 19794-6 standard. The first IREX project, IREX I, provided quantitative support to the standard by conducting the largest independently administered test of iris recognition technology to date. The test attracted 19 recognition technologies from 10 different providers. This represents an order of magnitude expansion of the industry over the past five years.The international standard, now under revision, defined three competing image formats and three compression methods: the IREX I test narrowed the field by determining which ones performed consistently at a high level and are included in the IREX report. The image format test showed that two of the three formats performed well: these center and crop the iris, or center, crop and mask eyelids and eyelashes. The study also determined that two compression standards were found to squeeze the images to a size small enough for storage and transmission while retaining the necessary quality level. One is the JPEG2000 which gives better recognition accuracy than the more commonly used JPEG, and the other is PNG format that employs lossless compression to completely preserve the iris information.The IREX I tests also looked at technical factors affecting users. These include speed-accuracy tradeoffs, threshold calibration, storage requirements, image quality assessment, and the effects of iris size, eyelid occlusion and pupil dilation. The test result shows that forensic applications, where image quality is sometimes degraded, can benefit from slower but more powerful algorithms.Recommendations based on the NIST results have been adopted by the standards committees. The report, IREX I: Performance of Iris Recognition Algorithms on Standard Images, can be downloaded from http://iris.nist.gov/irex. Since its inception in 2007, IREX has helped advance iris recognition toward the level of technical maturity and interoperability of fingerprint biometrics and has affirmed the potential for using iris biometrics as a second modality for large-scale identity management applications.Meanwhile, plans for IREX II are under way to calibrate and evaluate the effectiveness and efficiency of iris image quality assessment algorithms. This study will support a new international iris image quality standard by identifying specific iris image properties that are influential on recognition accuracy. The second draft of the IREX II research planavailable online at http://iris.nist.gov/irexIIis open for comments until Nov. 15, 2009. Comments should be submitted to irex@nist.gov.	Funding for IREX is provided by both the Department of Homeland Security's Office of US-VISIT and its Science and Technology Directorate.
evelyn.brown@nist.govNew publication offers security tips for WiMAX networksGovernment agencies and other organizations planning to use WiMAX Worldwide Interoperability for Microwave Accessnetworks can get technical advice on improving the security of their systems from a draft computer security guide prepared by the National Institute of Standards and Technology (NIST).WiMAX is a wireless protocol that can cover an area that incorporates a few miles such as a campus or small town. It has a larger reach than the more familiar "WiFi" networks used in offices or homes, but smaller than wireless areas covered by cell phones. The technology, guided by standards issued by IEEE, originally was designed to provide last-mile broadband wireless access as an alternative to cable, digital subscriber line (DSL) or T1 service. In recent years its focus has shifted to provide a more cellular-like, mobile architecture to serve a broader audience.WiMAX was used after the December 2004 tsunami in Aceh, Indonesia after the communication infrastructure was destroyed and also after Hurricane Katrina along the coast of the Gulf of Mexico.Special Publication 800-127 "Guide to Security for WiMAX Technologies" discusses WiMAX technology's topologies, components, certifications, security features and related security concerns. It covers the IEEE 802.16 standard for WiMAX and its evolution up to the 2009 version.The main threat to WiMAX networks occurs when the radio links between WiMAX nodes are compromised. The systems are then susceptible to denial of service attacks, eavesdropping, message modification and resource misappropriation.SP 800-127 recommends taking advantage of built-in security features to protect the data confidentiality on the network. It also suggests that organizations using WiMAX technology should:The draft version of NIST SP 800-127 is open for public comment through October 30, 2009. The document is available online at http://csrc.nist.gov/publications/PubsDrafts.html#800-127. Comments should be addressed to 800-127comments@nist.gov with "Comments on Public Draft SP 800-127" in the subject line.
knroark@lanl.govScientists use world's fastest supercomputer to create the largest HIV evolutionary treeMapping Darwinian evolutionary relationships results in an HIV family tree that may lead researchers to new vaccine focus areasLOS ALAMOS, New Mexico, October 27, 2009 Supporting Los Alamos National Laboratory's role in the international Center for HIV/AIDS Vaccine Immunology (CHAVI) consortium, researchers are using the Roadrunner supercomputer to analyze vast quantities of genetic sequences from HIV infected people in the hope of zeroing in on possible vaccine target areas.Physicist Tanmoy Bhattacharya and HIV researcher Bette Korber have used samples taken by CHAVI across the globe  from both chronic and acute HIV patients  and created an evolutionary genetic family tree, known as a phylogenetic tree, to look for similarities in the acute versus chronic sequences that may identify areas where vaccines would be most effective.In this study the evolutionary history of more than 10,000 sequences from more than 400 HIV-infected individuals was compared. The idea, according to Korber, is to identify common features of the transmitted virus, and attempt to create a vaccine that enables recognition the original transmitted virus before the body's immune response causes the virus to react and mutate.  "DNA Sequencing technology, however, is currently being revolutionized, and we are at the cusp of being able to obtain more than 100,000 viral sequences from a single person," said Korber.  "For this new kind data to be useful, computational advances will have to keep pace with the experimental, and the current study begins to move us into this new era." "The petascale supercomputer gives us the capacity to look for similarities across whole populations of acute patients," said Bhattacharya.  "At this scale we can begin to figure out the relationships between chronic and acute infections using statistics to determine the interconnecting branches  and it is these interconnections where a specially-designed vaccine might be most effective.The goal of CHAVI, established by the National Institute of Allergy and Infectious diseases, is to solve major problems in HIV vaccine development and design.About Roadrunner, the world's fastest supercomputer, first to break the petaflop barrierOn Memorial Day, May 26, 2008, the "Roadrunner" supercomputer exceeded a sustained speed of 1 petaflop/s, or 1 million billion calculations per second.  "Petaflop/s" is computer jargonpeta signifying the number 1 followed by 15 zeros (sometimes called a quadrillion) and flop/s meaning "floating point operation per second."  Shortly after that it was named the world's fastest supercomputer by the TOP500 organization at the June 2008 International Supercomputing Conference in Dresden Germany.The Roadrunner supercomputer, developed by IBM in partnership with the Laboratory and the National Nuclear Security Administration, will be used to perform advanced physics and predictive simulations in a classified mode to assure the safety, security, and reliability of the U.S. nuclear deterrent.  The system will be used by scientists at the NNSA's Los Alamos, Sandia, and Lawrence Livermore national laboratories.The secret to its record-breaking performance is a unique hybrid design. Each compute node in this cluster consists of two AMD Opteron dual-core processors plus four PowerXCell 8i processors used as computational accelerators. The accelerators used in Roadrunner are a special IBM-developed variant of the Cell processor used in the Sony PlayStation 3®. The node-attached Cell accelerators are what make Roadrunner different than typical clusters.Roadrunner is still currently the world's fastest with a speed of 1.105 petaflop/s per second, according to the TOP500 announcement at the November 2008 Supercomputing Conference in Austin Texas, and it again retained the #1 position at the June ISC09 conference. About Los Alamos National Laboratory (www.lanl.gov) Los Alamos National Laboratory, a multidisciplinary research institution engaged in strategic science on behalf of national security, is operated by Los Alamos National Security, LLC, a team composed of Bechtel National, the University of California, The Babcock & Wilcox Company, and the Washington Division of URS for the Department of Energy's National Nuclear Security Administration.Los Alamos enhances national security by ensuring the safety and reliability of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.
news@nas.eduSecuring biological select agents and toxins will require developing a culture of trustWASHINGTON -- The most effective way to prevent the deliberate misuse of biological select agents and toxins (BSATs) -- agents housed in laboratories across the U.S. considered to potentially pose a threat to human health -- is to instill a culture of trust and responsibility in the laboratory, says a new report from the National Research Council.  Focusing on the laboratory environment will be critical for identifying and reducing concerns about facilities or personnel.  Mechanisms for fostering a safe and secure laboratory environment include engaged management, risk-based security measures, and appropriate monitoring and management of personnel, as well as training for all researchers in scientific ethics and understanding "dual-use" research that could be misused.  Other methods of screening and oversight are incomplete without including the laboratory community in minimizing potential security risks, the report says.  Policies and procedures that make select agent research more difficult to conduct, as opposed to more secure, diminish overall security rather than strengthen it.Individuals cleared for access to select agents and toxins are certified for five years.  Many changes can occur during this time, however, including those that impact whether an individual poses a security risk.  Therefore, efforts to ensure reliable personnel should come from within the laboratories, the report says, through increased engagement and monitoring by managers and staff.  The goal should be that individuals watch out for each other and take responsibility for their own performance and that of others.  In a laboratory context, some security measures can also improve safety, if there is involvement of researchers in the process.  BSAT research is presently defined by a list of more than 80 select agents and toxins, developed and jointly regulated by the Centers for Disease Control and Prevention (CDC) and the Animal and Plant Health Inspection Service (APHIS).  According to the committee that wrote the report, the list should be ordered based on the potential of an agent to be used as a biothreat, and a graded series of security procedures should be applied so that the greatest resources and scrutiny go to securing agents that pose maximum risk. Personnel issues are often the most controversial and difficult aspects of maintaining security for BSATs.  Security programs can be generally divided into two categories: screening individuals to determine whether they are eligible for access, and monitoring the behavior and performance of employees working with these agents.  The current Security Risk Assessment screening process, which relies on screening more than 20 criminal, immigration, and terrorist databases to identify disqualifying behavior or activities, is appropriate, the report says.  However, a change that should be considered is expanding the appeal process beyond a simple determination of factual errors to include the opportunity to consider circumstances surrounding otherwise disqualifying factors, such as the length of time since an offense occurred.  Currently, any discovery of disqualifying factors or behaviors automatically and permanently denies an individual's access. Improved communication is needed among those funding research on select agents, those administering the Select Agent Program, and those conducting the research, the report says.  An advisory committee with members drawn from research institutions and the private sector should be established to provide continued engagement of stakeholders.  Representatives from federal agencies would serve in an EX OFFICIO capacity.  Rigorous and continuing evaluation of the Select Agent Program is needed to ensure that it is running efficiently and also to consider any intended and unintended consequences of operation.   The committee concludes that, because biological select agents can replicate, an undue reliance on accounting techniques, such as counting vials, to monitor whether a biological agent has been removed from a laboratory offers a false sense of security and is counterproductive.  Instead, accountability is best achieved by controlling access to archived stocks and working materials and recording which agents are present, where they are stored, who has access to them, when that access is available, their intended use, and where they are transported, if moved to another off-site location. Physical security is required of all facilities housing select agents, with broad regulatory guidance provided by CDC and APHIS.  The variation in implementing these requirements and regulations can lead to inconsistencies and confusion as facility operators, contractors, and inspectors attempt to determine whether a facility has met the necessary security guidelines.  The report calls upon the Select Agent Program to define minimum physical security requirements to assist facilities in meeting their regulatory obligations.  The report also recommends laboratory inspectors have scientific and laboratory knowledge and experience, as well as appropriate training specific to BSAT research and that the inspections should be harmonized across agencies.  Due to the considerable security and compliance costs involved in working with select agents, a separate category of federal funding should be made available to ensure that facilities always operate with appropriate security measures in place.   This study was sponsored by the National Institutes of Health.  The National Academy of Sciences, National Academy of Engineering, Institute of Medicine, and National Research Council make up the National Academies.  They are independent, nonprofit institutions that provide science, technology, and health policy advice under an 1863 congressional charter.  Committee members, who serve pro bono as volunteers, are chosen by the Academies for each study based on their expertise and experience and must satisfy the Academies' conflict-of-interest standards.  The resulting consensus reports undergo external peer review before completion.  A committee roster follows.Copies of RESPONSIBLE RESEARCH WITH BIOLOGICAL SELECT AGENTS AND TOXINS are available from the National Academies Press; tel. 202-334-3313 or 1-800-624-6242 or on the Internet at HTTP://WWW.NAP.EDU.  Reporters may obtain a copy from the Office of News and Public Information (contacts listed above).  [ This news release and report are available at HTTP://NATIONAL-ACADEMIES.ORG ]NATIONAL RESEARCH COUNCILDivision on Earth and Life StudiesBoard on Life SciencesCOMMITTEE ON LABORATORY SECURITY AND PERSONNEL RELIABILITY ASSURANCE SYSTEMS FOR LABORATORIES CONDUCTING RESEARCH ON BIOLOGICAL SELECT AGENTS AND TOXINSRITA R. COLWELL*(CHAIR)President and CEOCosmosID, Inc.; andDistinguished University ProfessorUniversity of MarylandCollege Park, andDistinguished University ProfessorBloomberg School of Public HealthJohns Hopkins UniversityBaltimoreRONALD M. ATLAS Professor of Biology and Public Health, and Co-DirectorCenter for Health Hazards PreparednessUniversity of Louisville Louisville, Ky.JOHN D. CLEMENTSProfessor and Chair of Microbiology and Immunology, andDirectorCenter for Infectious DiseasesTulane UniversityNew OrleansJOSEPH A.  DIZINNO Technical DirectorHomeland Security and Law EnforcementBAE SystemsWashington, D.C.ADOLFO GARCIA-SASTREProfessor of Microbiology, andFischberg Chair and ProfessorDepartment of Medicine, andCo-DirectorGlobal Health and Emerging Pathogens InstituteMount Sinai School of MedicineNew York CityMICHAEL G. GELLESSenior Manager Deloitte Consulting LLPWashington, D.C.ROBERT J. HAWLEYSenior Adviser of ScienceMidwest Research Institute Frederick, Md.SALLY KATZENExecutive Managing DirectorThe Podesta GroupWashington, D.C.TODD R. LA PORTE Professor Emeritus of Political SciencesDepartment of Political ScienceUniversity of CaliforniaBerkeleyPAUL LANGEVIN Director of Laboratory DesignMerrick and Co., andPresidentMerrick Canada ULCKanata, OntarioSTEPHEN S. MORSE Professor of Clinical Epidemiology, andFounding DirectorCenter for Public Health PreparednessMailman School of Public HealthColumbia UniversityNew York City KATHRYN E. NEWCOMER    Professor and Director Trachtenberg School of Public Policy and Public Administration, andCo-DirectorMidge Smith Center for Evaluation EffectivenessGeorge Washington UniversityWashington, D.C.ELIZABETH RINDSKOPF PARKERDean McGeorge School of LawUniversity of the PacificSacramento, Calif.PAUL R. SACKETTBeverly and Richard Fink Distinguished Professor of Psychology and Liberal ArtsDepartment of PsychologyUniversity of MinnesotaMinneapolisSTAFFADAM P. FAGENStudy DirectorJO L. HUSBANDSSenior Project Director* Member, National Academy of Sciences
wkdodds@k-state.eduCybernetwork to help K-State researchers study tallgrass prairie, respond to global warmingResearch at the Konza Prairie Biological Station and beyond will benefit from a cyber infrastructure grant to connect Kansas State University with other universities in the region.A group of researchers, led by Paul Risser at the University of Oklahoma, has been awarded $6 million from the National Science Foundation to develop cyber infrastructure that will facilitate ecological forecasting and education efforts. Leading K-State's portion of the project are Walter Dodds, university distinguished professor in the Division of Biology, and Daniel Andresen, associate professor of computing and information science. They are joining a multidisciplinary team from Oklahoma State University and the Universities of Kansas and Oklahoma.Dodds said development of this infrastructure will benefit K-State faculty and students whose research centers on Konza Prairie, a 3,487 hectare native tallgrass prairie preserve jointly owned by K-State and The Nature Conservancy."The main benefit to K-State will be getting Konza wired up so we can stream more data directly from the site in real time and link to similar data from other sites," Dodds said. "It will also increase regional data on distribution of plants and animals. This will benefit any researchers using Konza, whether at K-State or elsewhere."The project title is "Oklahoma and Kansas: A cyberCommons for Ecological Forecasting." It will link K-State with KU and the Oklahoma universities to integrate computer hardware, sensor networks and computer databases. This integration will stimulate collaboration and integration by allowing researchers to share scientific information like data, models and analytical synthetic efforts."Multidisciplinary approaches are essential to forecasting environmental trends, and this proposal will help K-State researchers share ecological data streams and connect with researchers at regional institutions," Dodds said.The cyberCommons promises to help ecologists researching the tallgrass prairie by filling in gaps in common modeling methods and ecological forecasting."The ecological data-sharing will assist management of ecological resources in Kansas and help us respond to issues like global warming, invasive species and emerging diseases," Dodds said.The Kansas principal investigator for the project is Kristin Bowman-James, project director of the Kansas National Science Foundation's Experimental Program to Stimulate Competitive Research, also known as EPSCoR. The KU collaborators are: Leonard Krishtalka, director of the Biodiversity Institute; Donald R. McMullen, senior scientist from the office of research and graduate studies; and James Beach, assistant director of informatics at the Biodiversity Center.Kansas' portion of the award  will be administered through the Kansas National Science Foundation's EPSCoR office. The National Science Foundation established EPSCoR in the late 1980s to promote scientific progress in states that previously had been underfunded in the sciences.
Shelley.Dawicki@noaa.govResearchers use new acoustic tools to study marine mammals and fishOver the past decade, researchers have developed a variety of reliable real-time and archival instruments to study sounds made or heard by marine mammals and fish. These new sensors are now being used in research, management and conservation projects around the world with some very important practical results. Among them is improved monitoring of endangered North Atlantic right whales in an effort to reduce ship strikes, a leading cause of their deaths."The tools available to acquire and analyze passive acoustic data have undergone a revolutionary change over the last ten years, and have substantially increased our ability to collect acoustic information and use it as a functional management tool," said Sofie Van Parijs, lead author and a bioacoustician at NOAA's Northeast Fisheries Science Center laboratory in Woods Hole, Mass. "These tools have significantly improved monitoring of North Atlantic right whales and enhanced the efficacy of managing ship traffic to reduce ship strikes of whales through much of the western North Atlantic off the U.S. East Coast."Van Parijs is one of many researchers whose work is described this month in the journal Marine Ecology Progress Series. Her paper is one of about a dozen in a special theme issue focused on acoustics in marine ecology. Van Parijs, who currently heads the NEFSC's Protected Species Branch, is also a co-author of a related paper on acoustic interference or masking, in which marine animals alter their use of sound as a result of changing background noise.Van Parijs and her colleagues focus on two types of acoustic sensors, real-time and archival. Real-time sensors are mounted on surface buoys, usually anchored or cabled to the ocean bottom, or deployed as arrays towed from a surface vessel. Archival sensors are affixed on bottom mounted buoys equipped with hydrophones to continuously record ocean sounds for long periods of time, often up to three months, before the sensors are temporarily recovered and their batteries refreshed. Some archiving sensors can be mounted on individual animals."Marine animals live their lives and communicate acoustically across different time and space scales and use sound for different reasons," said Van Parijs. "We need to use the right tool in the right place for the right need. There is no 'one size fits all' when it comes to using technology in the ocean."Large whales move and communicate over great distances, while smaller whales and dolphins tend to communicate over smaller areas. Pinnipeds, the group of marine mammals animals that includes seals, walrus and sea lions, breed on land, on ice or in the water, andmove and communicate over small to medium distances. Human-produced sounds complicate the sensing problem by adding sounds to what can be a noisy environment.The use of passive acoustic monitoring is increasing as improved reliability and lower hardware and software costs provide researchers with a set of tools that can answer a broad range of scientific questions. This information can, in turn, be used in conservation management and mitigation efforts. While most of the new technologies have been applied in studies of whales and dolphins, the researchers say the sensors can also be used in studying pinnipeds, sirenians (manatees and dugongs) and fish.In addition to Van Parijs and colleagues at NOAA's Northeast and Southwest Fisheries Science Centers, co-authors on the article include researchers from Cornell University's Bioacoustics Research Program, Instituto Baleta Jubarta in Brazil, Pennsylvania State University, Integrated Statistics, and the Alfred Wegener Institute for Polar and Marine Research in Germany.
michael.newman@nist.govNovel temperature calibration improves NIST microhotplate technologyResearchers at the National Institute of Standards and Technology (NIST) have developed a new calibration technique that will improve the reliability and stability of one of NIST's most versatile technologies, the microhotplate. The novel NIST device is being developed as the foundation for miniature yet highly accurate gas sensors that can detect chemical and biological agents, industrial leaks and even signs of extraterrestrial life from aboard a planetary probe.The tiny microhotplatesno wider than a human hairare programmed to cycle through a range of temperatures. They can be coated with metal oxide films tailored to detect specific gas species. Airborne chemicals attach to the surface of the detector depending on the type of film and the temperature of the surface, changing the flow of electricity through the device, which serves as the "signature" for identifying both the type and concentration of the gas in the ambient air.Accurate microhotplate temperature measurements are crucial for the discrimination and quantification of gas species, while reliable, long-term operation demands that the microhotplate's temperature sensors be either highly stable or able to sense when they've drifted, a functionality known as a "built-in self test" (BIST). As demonstrated for the first time in a paper in an upcoming issue of IEEE Electron Device Letters,* the new calibration method satisfies both requirements.A portion of the polysilicon heater making up the microhotplate originally served as the device's temperature sensor. However, this sensor would slowly drift over time from its initial calibration. Within three months, the temperature readings were off by as much as 25 degrees Celsius at high temperatures.The NIST engineers overcame this shortcoming by using data from two additional temperature sensorsa highly stable, thin-film platinum/rhodium thermocouple integrated in the microhotplate structure for one sensor and the thermal efficiency of the structure itself for the other. Comparing the temperatures reported by these two sensors provides the microhotplate with its internal monitoring system. As long as the absolute value of the difference between the reported temperatures remains below a specified threshold value, the average of the two readings is considered reliable. Should the difference exceed the threshold, the system reports an error.The original polysilicon sensor still provides the microhotplate's initial temperature measurement, which is used to calibrate the other two sensors. With the complete "check and balance" system in place, temperature measurements are accurate to within 1.5 degrees Celsius.Having successfully demonstrated the new temperature calibration system for their microhotplate, the NIST researchers are working on additional advancements for the technology. Next in line is the development of a built-in system for sensing contamination of the metal oxide films critical to the microhotplate's use in gas detection.	* M. Afridi, C. Montgomery, E. Cooper-Balis, S. Semancik, K.G. Kreider and J. Geist. Analog BIST functionality for microhotplate temperature sensors. IEEE Electron Devices, Volume 30, No. 9 (September 2009).
pressoffice@epsrc.ac.ukNew Bluetooth application will let sport fans share experiences in real timeEPSRC press releaseImagine watching a football match, seeing a foul and being able to immediately swap comments with friends who saw the same incident from the other side of the stadium.By enabling mobiles to communicate with each other without sending messages via a network, new technology being developed will enable people in different parts of a stadium to share banter, photos and video clips instantly, reliably  and free of charge.  The application makes innovative use of short range communications which would even enable complete strangers to share information and experiences. With Engineering and Physical Sciences Research Council (EPSRC) funding, researchers at the University of Glasgow have developed a series of computer programs that make so-called 'ad hoc networking' possible for any number of fans wanting to swap thoughts with each other at a live event. The programs enable a fan's phone to connect with up to seven other users at the same time, without using mobile phone masts. They do this by harnessing Bluetooth, a well-established form of wireless networking commonly used to connect a headset to a phone, for example. However, ad hoc networking has never been used before for direct phone-to-phone communication in real-world settings. The programs are the first to enable recent advances in ad hoc networking to be applied to phone-based end-user applications. They simply have to be installed onto standard iPhones to make mobile communications faster and more direct.* Currently it can be hard to get a mobile phone signal in a crowded sports stadium where there is a lot of interference. Even if a signal is obtained, messages can take a long time to be delivered to the recipient's mobile."Chat and banter need to be immediate," says Dr Matthew Chalmers, who is leading the project. "If a disputed goal is scored or a yellow card awarded, you want to hear what others have to say about it straight away, from their vantage point in the stadium. Direct mobile-to-mobile communication can make this possible.""Our aim is to let fans share information in real time and build up banks of images and conversation clips that can provide a unique memento of the day," Matthew Chalmers adds. "It's really about extending a Social Networking philosophy to sports stadia and giving spectators a richer experience by making them feel better connected with each other."If picked up by industry, the new technology could start reaching the market within the next year or two.In the long-term, mobile-to-mobile communications could play an important role in assisting emergency healthcare, by allowing people at an accident scene to communicate with each other even in areas remote from a mobile phone mast.The research work uniquely combines the expertise of computer scientists and sociologists. They are three months into a year long period of working with around 15 football fans. This involves studying how they use the technology in and around matches, and obtaining input from them for changes and additions to the technology design. Notes for EditorsThe two and a half year project 'Designing the Augmented Stadium' is due to conclude in March 2010 and is receiving total EPSRC funding of just over £409,000.The project's industrial partners are Microsoft and Arup. * Ad hoc networking among large numbers of mobile phones is not believed to have been done before in any system beyond lab demos. Such networking has been used in high-end Personal Digital Assistants (PDAs) in many research prototypes, but not in commercial applications. In handheld game consoles, ad hoc networking has been used between pairs of Nintendo DS consoles, in the Nintendogs game.Phones using the new computer programs can also collect messages and images swapped at a sports event and send them to Social Networking site Facebook when in range of a mobile phone mast. This lets fans in the stadium share some of their experience with friends who couldn't make it to the event, and also creates a lasting reminder of the event that can be shared easily. In addition, the details of long-term use can be observed and documented.Sport represents an important sector of the UK economy. Every year, tens of millions of people in the UK attend at least one live sporting event. By adding to spectators' enjoyment, the new technology could encourage more people to go to such events more frequently, further boosting the sector.   Social Networking involves the building of online communities of people who share interests and activities.The Engineering and Physical Sciences Research Council (EPSRC) is the UK's main agency for funding research in engineering and the physical sciences. The EPSRC invests more than £800 million a year in research and postgraduate training to tackle the challenges of the 21st Century. www.epsrc.ac.ukFor more information contact:Dr Matthew Chalmers, Department of Computing Science, University of Glasgow. Telephone the university press office on 0141 330 4831/ 3535 or e-mail: matthew@dcs.gla.ac.ukTwo images ('crowd.jpg' and 'stadium.jpg') are available from the EPSRC press office. Telephone: 01793 444404, e-mail: pressoffice@epsrc.ac.uk. Suggested caption: 'The new technology could totally change the stadium experience for the spectator.'
jtoon@gatech.eduImproved techniques will help control heat in large data centersKeeping their coolApproximately a third of the electricity consumed by large data centers doesn't power the computer servers that conduct online transactions, serve Web pages or store information. Instead, that electricity must be used for cooling the servers, a demand that continues to increase as computer processing power grows.And the trend toward cloud computing will expand the need for both servers and cooling.At the Georgia Institute of Technology, researchers are using a 1,100-square-foot simulated data center to optimize cooling strategies and develop new heat transfer models that can be used by the designers of future facilities and equipment.  The goal is to reduce the portion of electricity used to cool data center equipment by as much as 15 percent."Computers convert electricity to heat as they operate," said Yogendra Joshi, a professor in Georgia Tech's Woodruff School of Mechanical Engineering.  "As they switch on and off, transistors produce heat, and all of that heat must be ultimately transferred to the environment.  If you are looking at a few computers, the heat produced is not that much.  But data centers generate heat at the rate of tens of megawatts that must be removed."Summaries of the research have been published in the Journal of Electronic Packaging and International Journal of Heat and Mass Transfer and presented at the Second International Conference on Thermal Issues in Emerging Technologies, Theory and Applications. The research has been sponsored by the U.S. Office of Naval Research, and by the Consortium for Energy Efficient Thermal Management.Five years ago, a typical refrigerator-sized server cabinet produced about one to five kilowatts of heat.  Today, high-performance computing cabinets of about the same size produce as much as 28 kilowatts, and machines already planned for production will produce twice as much."Some people have called this the Moore's Law of data centers," observed Joshi, who is also the John M. McKenney and Warren D. Shiver Chair in the School of Mechanical Engineering.  "The growth of cooling requirements parallels the growth of computing power, which roughly doubles every 18 months.  That has brought the energy requirements of data centers into the forefront."Most existing data centers rely on large air conditioning systems that pump cool air to server racks.  Data centers have traditionally used raised floors to allow space for circulating air beneath the equipment, but cooling can also come from the ceilings.  As cooling demands have increased, data center designers have developed complex systems of alternating cooling outlets and hot air returns throughout the facilities."How these are arranged is very important to how much cooling power will be required," Joshi said.  "There are ways to rearrange equipment within data centers to promote better air flow and greater energy efficiency, and we are exploring ways to improve those."Before long, centers will likely have to use liquid cooling to replace chilled air in certain high-powered machines.  That will introduce a new level of complexity for the data centers, and create differential cooling needs that will have to be accounted for in the design and maintenance.  Joshi and his students have assembled a small high-power-density data center on the Georgia Tech campus that includes different types of cooling systems, partitions to change room volumes and both real and simulated server racks.  They use fog generators and lasers to visualize air flow patterns, infrared sensors to quantify heat, airflow sensors to measure the output of fans and other systems, and sophisticated thermometers to measure temperatures on server motherboards.Beyond studying the effects of alternate airflow patterns, they are also verifying that cooling systems are doing what they're supposed to do.Because tasks are dynamically assigned to specific machines, heat generation varies in a data center.  Joshi's group is also exploring algorithms that could help even out the computing load by assigning new computationally-intensive tasks to cooler machines, avoiding hot spots.Another issue they're studying is what happens when utility-system power to a data center is cut off.  The servers themselves continue to operate because they receive electricity from an uninterruptible power supply.  But the cooling equipment is powered by backup generators, which can take minutes to get up to speed.During the brief time without cooling, heat builds up in the servers.  Existing computer models predict that temperatures will reach dangerous levels in a matter of seconds, but actual measurements done by Joshi's graduate students show that the equipment can run for as much as six minutes without cooling.  "We're developing models for different parts of the data center to learn how they respond to changes in temperature," said Shawn Shields, a former graduate student in Joshi's lab.  "Existing models consider that temperature changes across a server rack will be instantaneous, but we've found that it takes quite a relatively long time for the server to reach a steady state."Beyond reducing cooling load, the researchers are also looking at how waste heat from data centers can be used.  The problem is that the heat is at relatively low temperatures, which makes it inefficient to convert to other forms of energy.  Options may include heating nearby buildings or pre-heating water, Joshi said.Data obtained by the researchers with thermometers and airflow meters is being used to validate computer models that are reasonably accurate, but run rapidly.  In the future, these models will help data center operators do a better job of optimizing cooling in real time, he said.Joshi believes there's potential to reduce data center energy consumption by as much as 15 percent by adopting more efficient cooling techniques like those under development in his lab."Our data center laboratory is a complete sandbox in which we can study all sorts of options without affecting anybody's computing projects," he added.  "We can look at interesting ways to improve rack-level cooling, liquid cooling and thermoelectric cooling."
dguerin@latech.eduCenter for Secure Cyberspace to host 2nd Cyber Research WorkshopWorkshop to be held in conjunction with 2009 Air Force Cyber Symposium in Shreveport, La.The Center for Secure Cyberspace (CSC), a collaborative effort of Louisiana Tech and Louisiana State Universities, will host the 2nd Annual Cyberspace Research Workshop on Monday, June 15, 2009 at the Shreveport Convention Center.The theme of the workshop, held in conjunction with the 2009 Air Force Cyberspace Symposium (AFCS), is "Cyber Security in the 21st Century."  It will serve as a venue for discussing emerging technologies, sharing ideas and creating opportunities for researchers and practitioners in various areas of cyber security and operations."This workshop provides a unique venue to discuss foundational innovations that will shape the course of the connected world in 21st century," says Dr. Vir Phoha, professor of computer science at Louisiana Tech University and principle investigator for the CSC."Convergence of researchers from universities and industry, users of technology from civilian and military, and developers from small and large industry provide one-of-a-kind gathering with profound implications for state-of the-art research and applications in cyberspace security."According to the CSC, cyberspace is a medium that is rampant with attackers. Due to the largely reactive mindset that governs many strategies designed to deal with the onslaught of attacks, governments and large organizations often find themselves in a defensive position compelled to play "catch up."Primary discussions at the workshop will center around strategic cyber defense, global cyber situational understanding, sensor placement, hardware and software anti-tamper, the psychology of cyber security, and malware and anti-malware technologies.For more information on the Cyber Research Workshop or the CSC, please contact Ms. Brenda Brooks at (318) 257-3475 or at bbrooks@latech.edu, or visit the CSC website at http://csc.latech.edu.The CSC was established as a national center of excellence dedicated to education and research in integrated smart cyber-centric sensor surveillance systems.  The CSC is supported by strong commercialization infrastructures at Louisiana Tech and LSU that help to commercialize technologies and establish services supporting secure cyber operations in the military and the private sector.
fssprings@novanthealth.orgForsyth Medical Center launches region's first comprehensive teleneurology programProgram will bring 24/7 emergency stroke and neurology critical care to rural and small suburban hospitals in North Carolina and VirginiaWinston-Salem, N.C  Rural and small, suburban hospitals in North Carolina and Virginia can now provide a higher level of emergency stroke and critical neurology care, 24 hours a day, 365 days a year, as part of a new teleneurology medicine program announced Oct. 26, 2009, by Forsyth Medical Center (FMC).  The program, coordinated through the Forsyth Stroke & Neurosciences Center, allows medical staff at participating hospitals to rapidly connect with highly trained, board-certified neurologists using videoconferencing technology at the patient's bedside.Hugh Chatham Memorial Hospital in Elkin, N.C., Brunswick Community Hospital in Supply, N.C., Kernersville Medical Center in Kernersville, N.C., and Twin County Regional Hospital in Galax, Va., are participating in the network.  A number of other hospitals will join the network in the future."Our program combines the vast resources of FMC's nationally certified, award-winning stroke center with some of the country's leading private practice and academic neuroscience physicians," says Cheré Chase, M.D., medical director of stroke and neurocritical care at Forsyth Medical Center.  "We can now quickly bring world-class emergency stroke and critical neurology care to hospitals that may lack 24/7 on-call neurologists or that want to strengthen their current primary stroke care services."Because of the shortage of experienced, trained neurologists in smaller communities, many stroke victims bypass their local hospitals, causing delay in receiving treatment. Now, they can be treated locally, saving lives and improving the chances for a better recovery.The new program will link participating community hospitals with FMC and Specialists On Call (SOC), a Joint Commission-accredited organization of board-certified, specialty trained, community and university neurologists.  SOC neurologists have a minimum of 10 years in clinical practice and include nationally recognized physicians affiliated with departments of neurology at world-class medical centers, including Brown University, Florida State University College of Medicine and the University of Pennsylvania School Of Medicine.  SOC physicians have provided more than 7,000 consults in 2009 for neurological emergencies."Sixteen SOC physicians are now affiliated with Forsyth Comprehensive Neurology and have become licensed in North Carolina and credentialed at Forsyth Medical Center," Dr. Chase says.  "This allows community hospitals in the Carolinas and Virginia to have access to an expert level of comprehensive stroke and neurological care not available at any single medical center in the country."How it WorksRural and small, suburban community hospital emergency department physicians, intensive care unit physicians and hospitalists will be able to have real-time physician-to-physician consultation to quickly determine the best course of treatment for patients requiring emergency or advanced neurological care.When a potential stroke victim enters the emergency department, the staff will be connected to an on-call expert neurologist and establish a videoconferencing link using a mobile unit that can be brought to the patient's bedside.  The consulting neurologist will then be able to view and discuss diagnostic test results, including CT scan images performed at the local hospital.  During the consult, physicians, patients and family members will continue to have real-time discussions about diagnostic results, course of treatment and patient response.A significant advantage of having an experienced critical stroke care neurologist involved is to assist in determining if the clot-busting drug tPA should be part of the treatment.  This is important as tPA must be administered within a three-hour window from the onset of the first stroke symptoms.  It will also allow participating network hospitals to provide more advanced stroke care for up to six hours, reducing the need to immediately transfer patients to a comprehensive stroke center such as Forsyth Medical Center for additional care."When a person suffers a stroke, 1.9 million nerve cells in the brain die every minute," Dr. Chase explains.  "This steady loss of brain cells can often be curtailed through the use of tPA, but determining its appropriate use is best done by trained and experienced stroke and critical care neurologists."For participating community hospitals, having this level of physician expertise on call will help to save lives and brain function for many patients and may eliminate the need to transport patients to a certified primary stroke center or comprehensive stroke center for further treatment.Providing support for emergency stroke care is only one component of Forsyth's program, which will also include rapid access to consultation for patients with conditions such as aneurysms, brain tumors, concussions, epilepsy and other conditions that can affect the head and spine.  The program will also provide stroke treatment training at participating hospitals and stroke prevention outreach to communities they serve.Forsyth Medical Center plans to offer its 24-hour emergency teleneurology services to other hospital organizations throughout North Carolina and Virginia.About Forsyth Medical CenterForsyth Medical Center is part of Novant Health, a not-for-profit integrated group of hospitals and physician clinics, ranked 12th nationally among the 2009 Top 100 Integrated Healthcare Networks, according to an analysis by the SDI health informatics company.  Novant staff cares for patients and communities in North and South Carolina.  Hospital affiliates include Presbyterian Hospital, Presbyterian Orthopaedic Hospital, Presbyterian Hospital Matthews and Presbyterian Hospital Huntersville in the Charlotte, NC area; Forsyth Medical Center and Medical Park Hospital in Winston-Salem, NC; Thomasville Medical Center in Thomasville, NC; Rowan Regional Medical Center in Salisbury, NC; and Brunswick Community Hospital in Supply, NC.  The Novant Medical Group consists of more than 1,060 providers in 361 clinic locations.  Other Novant facilities and programs include two nursing homes, outpatient surgery and diagnostic centers, rehabilitation programs and community health outreach programs.
smannino@stevens.eduStevens' Center for Science Writings presents a talk on modern warfare by Peter W. Singer, Sept. 23Singer is the author of 'Wired for War' and a senior fellow at the Brookings Institution HOBOKEN, N.J.  The Center for Science Writings at Stevens Institute of Technology presents the talk, " The Robotics Revolution and Conflict in the 21st Century," by Peter W. Singer, on Wednesday, September 23.Singer, a senior fellow at the Brookings Institution, will discuss the transformation of modern warfare.Singer is the author of Wired for War, which reports on how the US is using increasingly sophisticated robots, drones and other futuristic weapons to fights its wars. Singer, who also works as a consultant for the Pentagon and Central Intelligence Agency, has written extensively on topics ranging from child warriors to mercenaries.The talk will be held from 4 p.m.  5:30 p.m. in room 122 of the Babbio Center. All CSW talks are free and open to the public. For more information, contact CSW Director John Horgan, johnhorgan@stevens.edu, or check the Center's website at: www.stevens.edu/csw. The CSW is part of Stevens' College of Arts & Letters.About Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value.Stevens offers baccalaureates, master's and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students, with about 250 full-time faculty. Stevens' graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.  For the latest news about Stevens, please visit StevensNewsService.com.
JacksonKN@missouri.eduResearch leads to improved human, object detection technologyMizzou scientists develop software that detects humans and objects in videos, creating new possibilities for safety and surveillanceCOLUMBIA, Mo.  When searching for basketball videos online, a long list of websites appears, which may contain a picture or a word describing a basketball. But what if the computer could search inside videos for a basketball? Researchers at the University of Missouri are developing software that would enable computers to search inside videos, detect humans and specific objects, and perform other video analysis tasks."The goal of our research is to improve how computers interpret the content of a video and how to identify it," said Tony Han, electrical and computer engineering professor in MU's College of Engineering. "There are lots of possibilities with video-based detection, and it could come at quite a low-cost compared to object and human detection using other sensors, such as thermal sensors."Intelligent video surveillance requires human and object detection. If a security camera captures an image of an injured person lying on the ground, the computer would not only store the surveillance image, but also be able to detect that a human is falling and send signals for help. Human detection software could also be applied to assisted driving. For example, the software could make a car stop immediately when it detects a pedestrian. Computer detection also might improve care for older adults living at home. If an older adult fell suddenly, computer detection software could detect the fall and alert medical professionals."My students and I are working on algorithms for automatic object detection, but these are very difficult to perfect," Han said. "We're trying to find a way to create reliable detection algorithms, but it takes a lot of time to test them. We have manually labeled more than 3,000 images with object locations and have used them to test our algorithms."	This Fall, Han and his students attended the PASCAL grand challenge in object detection, where they competed in detection for objects in 20 categories against researchers from all over the world. In their first time competing, they won first place in detection for potted plants and chairs and second place in detection for humans, cars, horses and bikes.Han's research has been published in numerous publications, such as the IEEE Conference on Computer Vision and the Second IEEE Workshop on CVPR for Human Communicative Behavior Analysis.	
sheryl.m.weinstein@njit.eduNJIT, US Marine Corps Reserve Association host counter-terrorism symposiumA free, day-long terrorism preparedness symposium covering counter-terrorist strategies and highlighting new and developing technologies to combat threats and regional concerns will be held at NJIT on Nov. 14, 2009, from 8:30 a.m. to 4:30 p.m.  The event sponsor, the New Jersey Chapter of the Marine Corps Reserve Association, has invited regional and national groups to brief participants on topics ranging from cyber-security to protecting transportation venues. John D. Hunt, former commanding officer of the New Jersey State Police (NJPS) Emergency Management Section and Commanding Officer of the NJSP Special Operations Section of the Homeland Security Branch, will deliver the keynote address. News and media outlets are welcome to cover this event. For more information, contact Walter F. Conner Col. USMC (ret.), President New Jersey Marine Corps Reserve Association, at (609) 306 4860 or wconner9@comcast.net.Organizations scheduled to provide briefings include the US Dept. of Homeland Security, NJ Office of Homeland Security and Preparedness, NJSP, Customs and Border Protection, the US Coast Guard, and experts from industry and private firms. The NJ Community Emergency Response Team (CERT) and the United States Marine Corps Wounded Warrior Regiment will also participate.Topics to be discussed include cell phone digital forensics, early detection strategies for roadside bombs and other improvised explosive devices, radio inter-operability, and an overview of ongoing regional planning. An expert panel will address concerns facing air, rail, trucking and maritime providers and users.  Other expert panelists will discuss immigration, cyber-security and immigration issues. An expert on science and technology from the US Dept. of Homeland Security will also be there. NJIT, New Jersey's science and technology university, at the edge in knowledge, enrolls more than 8,400 students in bachelor's, master's and doctoral degrees in 92 degree programs offered by six colleges: Newark College of Engineering, College of Architecture and Design, College of Science and Liberal Arts, School of Management, Albert Dorman Honors College and College of Computing Sciences. NJIT is renowned for expertise in architecture, applied mathematics, wireless communications and networking, solar physics, advanced engineered particulate materials, nanotechnology, neural engineering, management, and e-learning. In 2009, Princeton Review named NJIT among the nation's top 25 campuses for technology and among the top 150 for best value. U.S. News & World Report's 2008 Annual Guide to America's Best Colleges ranked NJIT in the top tier of national research universities.
daniel.parry@nrl.navy.milNRL successfully completes first major development milestone on the NPOESS MIS programNRL's Spacecraft Engineering Department and Remote Sensing Division announced it has successfully completed the System Requirements and Design Review (SRDR) for the National Polar-orbiting Operational Environmental Satellite System (NPOESS) Microwave Imager/Sounder (MIS) program. The MIS SRDR was the first major development milestone and was held in May 2009. The program sponsor, the NPOESS Integrated Program Office (IPO), has certified the SRDR to be a success. IPO's Independent Review Team (IRT) reported that "the NRL development team demonstrated outstanding microwave design experience and knowledge" and that it was "an outstanding review by the IPO and NRL team."  NRL is now proceeding toward Preliminary Design Review (PDR), which is scheduled for spring of 2010. "The approval to proceed to the design-build stage in preparation for the PDR is a major milestone for the MIS program," said John Schaub, superintendent, NRL Spacecraft Engineering Department. "Upon completion, this next generation sensor will deliver improved global microwave radiometry and sounding data, providing higher resolution microwave imagery and specialized meteorological and oceanographic products, including tropical cyclone structure."The NPOESS tri-agency Integrated Program Office (IPO) competitively selected NRL in 2008 to build MIS because of its demonstrated experience and knowledge developing the first space-borne polarimetric microwave radiometer, WindSat, operating successfully since 2003 on the Coriolis mission.  The first MIS sensor is scheduled to fly aboard the second NPOESS spacecraft (C2) expected to launch in 2016.NPOESS is a polar-orbiting satellite system used to monitor global environmental conditions, and collect and disseminate data related to Earth's weather, atmosphere, oceans, land, and near-space environment providing data for long-range weather and climate forecasts. In 1994, it was recognized that converging the existing polar systems from the DoC and DoD would result in a higher performance integrated system. NPOESS gathers those existing polar-orbiting satellite systems into a single national program.The Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA.
daniel.parry@nrl.navy.milNRL brings new hyperspectral atmospheric and ocean science to the International Space Station(Washington, DC  09/10/09)  Following a fast-paced 16 month design and development process, NRL's Remote Sensing and Space Science Divisions and the Naval Center for Space Technology provide the first-ever high quality and real-time monitoring of space weather and coastal ocean environment directly from the new Japanese Experiment Module-Exposed Facility (JEM-EF) on the International Space Station (ISS). Designed and built by NRL, the Hyperspectral Imager for the Coastal Ocean (HICO) and Remote Atmospheric and Ionospheric Detection System (RAIDS) Experiment Payload (HREP) launched September 10, 2009, on-board the Japanese Aerospace Exploration Agency (JAXA) H-II Transfer Vehicle. The Aerospace Corporation provided HREP structure analysis and the thermal design and analysis. A complex series of maneuvers, involving both the ISS and the JEM-EF manipulator arms, will transfer HREP from the vehicle to its deployment station on the JEM-EF. HICO is the first space-borne sensor specifically designed for coastal maritime hyperspectral imaging, and RAIDS is a hyperspectral imaging sensor suite for innovative measurement of the Earth's thermosphere and ionosphere.   "Never has the ISS been utilized as a platform to conduct scientific Earth observations of this nature," said Dr. Mike Corson, NRL Remote Sensing Division and HICO Principal Investigator. "This collaboration of a diverse international and interagency consortium opens exciting opportunities for future basic and applied space-based research."The ISS offers a challenging, and novel research platform for oceanic and atmospheric observations. Because it is not in a Sun-synchronous orbit, the ISS offers a wide range of illumination angles and ample opportunity to study local time variations of the upper atmosphere. The ISS provides power to the HICO and RAIDS instruments and offers a high-availability communication link, streaming data to NRL's Washington, D.C., campus for processing, analyses, archiving and distribution. Hyperspectral imaging is a powerful remote sensing technique for environmental characterization of the Earth. HICO is a down-looking hyperspectral imager that includes a high quantum efficiency focal plane array to achieve high signal-to-noise ratios at water-penetrating wavelengths (380-1000 nanometers). It is the first space-borne sensor optimized for scientific investigation of the coastal ocean and nearby land regions. HICO will demonstrate coastal products critical for governmental and scientific applications including water clarity, bottom-types, bathymetry and on-shore vegetation maps.  Initial calibration and processing of the HICO data is performed at the NRL Remote Sensing Division. The data is then sent to NRL's Oceanography Division at Stennis Space Center, Miss., for further processing, archiving, and distribution to government users. Data will also be archived at Oregon State University, which is the primary repository for distribution of HICO data products to civilian users. The Office of Naval Research (ONR) as part of their "Space Innovative Naval Prototype" program funded HICO instrument design and fabrication. RAIDS, built collaboratively by NRL and The Aerospace Corporation, combines a suite of eight optical instruments to study the Earth's thermosphere and ionosphere using hyperspectral limb scanning techniques. RAIDS includes two spectrographs, three spectrometers and three photometers that collectively span the extreme-ultraviolet to near-infrared passband (55-874 nanometers). The science team will use the measurements to explore the effects of lower-atmospheric tides on the ionosphere-thermosphere system, demonstrate advanced dayside ionospheric remote sensing, and study upper atmosphere chemical and thermal processes. By filling existing measurement gaps, these data will not only expose fundamental physics of the upper atmosphere, but also provide a testbed for state-of-the-art global assimilative models that specify and forecast ionospheric weather relevant to civilian and military space-based systems."The thermosphere and ionosphere comprise the rarefied atmospheric region where many space assets orbit," said Dr. Scott Budzien, NRL Space Science Division and RAIDS Principal Investigator. "RAIDS gathers data continuously and transmits this data in real-time, allowing live monitoring of the upper atmospheric environment for more accurate space weather forecasting."The HICO and RAIDS sensors are mounted inside the HREP enclosure along with a computer for communication, instrument control and data storage, and a star camera to provide precise attitude determination. Attitude information from the star camera is used to improve the knowledge of the HICO and RAIDS line-of-sight pointing directions. Costs for the design, assembly, and testing of the HREP instrument enclosure were provided by ONR and the DoD Space Test Program (STP). STP provided launch and integration costs. Supported by NASA, STP selected HREP to be the first US science payload to be deployed on the Japanese Experiment Module-Exposed Facility. The Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA.
swaney@andrew.cmu.eduCarnegie Mellon's Jean VanBriesen leads research team on Monongahela RiverWater quality tested	PITTSBURGH-Carnegie Mellon University's Jeanne M. VanBriesen and Kelvin Gregory will use a $100,000 grant from the Pittsburgh-based Colcom Foundation to study water quality in the Monongahela River. The focus will be on the presence and effect of bromide associated with Marcellus Shale gas produced water, and sulfate from acid mine drainage, according to VanBriesen, a professor of civil and environmental engineering and faculty director of the Center for Water Quality in Urban Environmental Systems (WaterQUEST). "The public has expressed increased concern about the produced water that may result from ongoing development of the southwestern Pennsylvania Marcellus Shale formation, which is reported to contain more than 300 trillion cubic feet of natural gas," VanBriesen said.Developers using hydraulic fracturing, which involves injecting water and sand into major shale formations to help natural gas flow up a well, will need millions of gallons of water to complete the process at well sites. Water that returns to the surface, called flowback or produced water, is collected for reuse or disposal. Disposal at wastewater treatment plants along the Monongahela in 2008 is suspected as a contributing factor in high levels of total dissolved solids (TDS) observed in the river. Carnegie Mellon researchers will work with the River Alert Information Network (RAIN), a regional association of drinking water suppliers that has been selected by the state to monitor the river quality.  "We will essentially collect data from sensors that RAIN deploys at various sites along the Monongahela River," said Gregory, an assistant professor in Carnegie Mellon's Civil and Environmental Engineering Department. The sensors will monitor for some aspects of water quality, and the Carnegie Mellon team will take additional samples for bromide and sulfate.  Besides the fieldwork, Carnegie Mellon researchers will add data to RAIN's Web-based public information system to promote additional education and discussion among environmental groups about river water quality. WaterQUEST will host a  "State of the Monongahela River" event to share data and research about the river."Carnegie Mellon's work to understand the water quality impacts from shale gas production in Pennsylvania represents a thoughtful, farsighted effort to avert a problem before it arises. Carnegie Mellon's research resonates with the mission of the Colcom Foundation, which has a long history of assessing and addressing the cause before it's necessary to respond to the symptom. It's a privilege to support Carnegie Mellon's preventative strategy," said Carol Zagrocki, program director of the Colcom Foundation, established in 1996 by the late Cordelia S. May, a dedicated conservationist who served as chairman until her death in 2005. In addition to the Colcom Foundation, Carnegie Mellon's Steinbrenner Institute for Environmental Education and Research provided seed funding for this research through support of a graduate student researcher.  About Carnegie Mellon: Carnegie Mellon (http://www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion comprehensive campaign, titled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty, students and innovative research, and enhance the physical campus with equipment and facility improvements. For more about Carnegie Mellon, visit http://www.cmu.edu/about/.
matthias.fluegge@fokus.fraunhofer.deFind local rideshares quickly via mobile phoneThere is one striking fact about rush-hour traffic  vehicles on the commuter routes tend to be occupied by just one person, even though motoring costs are continually rising. The OpenRide mobile ridesharing service currently being developed by researchers at the Fraunhofer Institute for Open Communication Systems FOKUS will remedy this situation by organizing ridesharing opportunities spontaneously and conveniently. This will not only save costs but also help the environment by reducing the amount of city traffic. The OpenRide project is funded by the German ministry of economics and technology under the EXIST program.The system developers are paying particular attention to functionality and user friendliness. Users will open the application on their cellphone and select from a menu the options for offering or looking for a ride. They then enter the starting and finishing points, as well as the number of places available or required, and send the enquiry to the OpenRide server, where a search engine for intelligent route matching compares the offers and requests received. The search not only takes the starting and finishing points into account but also partial journeys in between. The minimum lead times required by conventional ridesharing schemes are rendered superfluous by the mobile solution. Drivers can offer lifts spontaneously on OpenRide from their cellphone while out on the road and ride seekers can look for a lift opportunity in their direct vicinity. This information together with the current position is transmitted wirelessly to a server, where a special software program continuously compares offers with requests for rides. Matching offers are displayed in real time on the cellphone with the message "Driver found", stating the name and the probable pickup and travel time. The ad-hoc service even conveys requests for rides placed after the driver has set off. An intelligent search engine takes partial journeys, short detours and the current position of the driver and potential passenger into account. The researchers are also planning to equip OpenRide with a rating system and user profiles to strengthen the trust between driver and passenger. "OpenRide links mobile terminals with navigation and route planning software to automatically organize ridesharing opportunities," explains Dr. Matthias Flügge, project manager at FOKUS. The mobile ridesharing center is particularly suitable for last-minute journeys in towns and cities. "The system opens up a new market because there is no provision at present for the typical spontaneous and shorter trips that take place in local everyday traffic," says Flügge. "We are using device-independent technologies in order to make the service available to as many users as possible," adds Anna Kress, technical director of OpenRide.A key feature of the OpenRide infrastructure is the use of open interfaces, allowing the integration of additional partners. This will not only provide end users with a new means of accessing ridesharing centers but also enable network operators and cellphone manufacturers to widen their service offering. OpenRide is on course for market launch next year. Field trials with industrial partners are planned for the end of 2009. 
jritter@lumc.eduDoctors' bedside skills trump medical technologyPhysician exams better than CT scans in predicting serious complications after brain surgeryMAYWOOD, IL. -- Sometimes, a simple bedside exam performed by a skilled physician is superior to a high-tech CT scan, a Loyola University Health System study has found.Researchers found that physicians' bedside exams did a better job than CT scans in predicting which patients would need to return to the operating room to treat complications such as bleeding."The low cost, simple, but elegant neurological exam appears to be superior to a routine CT scan in determining return to the operating room," researchers report in the Journal of Neurosurgery.Patients typically receive CT scans following open brain surgery to remove tumors, repair aneurysms, treat brain injuries, etc. But practices vary. Some surgeons order CT scans right after surgery. Others wait until the following morning.There are downsides. CT scans cost hundreds of dollars and expose patients to radiation. Transporting patients to scanning machines "involves multiple personnel of varying skills and nursing staff who are taken away from their other unit responsibilities," researchers wrote. "These scans also often interfere with work flow efficiencies of the radiology department."The lead author of the study is Dr. Ahmad Khaldi, chief resident in the Department of Neurological Surgery at Loyola University Chicago Stritch School of Medicine. The senior author is Dr. Thomas Origitano, chairman of the Department of Neurological Surgery.Researchers examined the records of 251 patients who received CT scans within 24 hours of surgery at Loyola. They included 133 patients who received routine scans within seven hours of surgery and 108 patients who received routine scans between 8 hours and 24 hours after surgery. None of the routine scans predicted which patients would need to return to the operating room.Patients also received bedside neurological exams by physicians. In 10 cases, physicians detected serious problems, such as being slow to wake up, that warranted an urgent CT scan. Three of these urgent scans (30 percent) confirmed the patients' problems were serious enough to require a return to the operating room. By comparison, 0 percent of the 241 routine CT scans predicted whether patients would have to return to the emergency room.A normal CT scan given right after surgery might give a doctor a false sense of security, which could lead to less frequent monitoring and neurological exams. Of the 14 patients in the study who took a serious turn for the worse, 13 had had CT scans within four hours of surgery that were normal or showed only minor problems."Scanning technology is really good," Origitano said. "But applying it without a physician's input is not necessarily helpful."Other co-authors are Dr. Vikram Prabhu and Dr. Douglas Anderson. Anderson is a professor and Prabhu is an associate professor in the Department of Neurological Surgery, Stritch School of Medicine.
pressoffice@epsrc.ac.ukRobotic ferret will detect hidden drugs and weaponsA new type of robot being developed will make it easier to detect drugs, weapons, explosives and illegal immigrants concealed in cargo containers. Dubbed the 'cargo-screening ferret' and designed for use at seaports and airports, the device is being worked on at the University of Sheffield with funding from the Engineering and Physical Sciences Research Council (EPSRC).The ferret will be the world's first cargo-screening device able to pinpoint all kinds of illicit substances and the first designed to operate inside standard freight containers. It will be equipped with a suite of sensors that are more comprehensive and more sensitive than any currently employed in conventional cargo scanners. Recent advances in both laser and fibre optic technology now make it possible to detect tiny particles of different substances. The EPSRC-funded project team is developing sensors which incorporate these technologies and that are small enough to be carried on the 30cm-long robot, in order to detect the specific 'fingerprint' of illegal substances at much lower concentrations than is now possible. When placed inside a steel freight container, the ferret will attach itself magnetically to the top, then automatically move around and seek out contraband, sending a steady stream of information back to its controller.  Current cargo-screening methods rely on a variety of separate methods, such as the use of sniffer dogs and external scanners for detecting explosives and drugs and carbon dioxide probes and heartbeat monitors to detect a human presence.Cargo scanners currently in use at seaports and airports only generate information on the shape and density of objects or substances. The ferret, however, will be able to provide information on what they actually consist of as well."It's essential we develop something which is simple to operate and which Border Agents can have total confidence in," says Dr Tony Dodd, who is leading the project. "The ferret will be able to drop small probes down through the cargo and so pinpoint exactly where contraband is concealed."Working prototypes of the cargo-screening ferret could be ready for testing within two years, with potential deployment within around five years. Notes for EditorsThe 3-year project 'Cargo Screening Ferret' began in October 2008 and is receiving total EPSRC funding of nearly £732,000.The project also involves the University of Glasgow, Loughborough University, City University London and defence and security specialists Qinetiq. The idea for the project emerged from an event organised by EPSRC, the Home Office Scientific Development Branch and the UK Borders Agency.The ferret will offer major advantages in combating human trafficking. Currently, it is very difficult to detect people hidden in freight containers (e.g. the use of X-rays is prohibited due to the harm the radiation could do to anyone concealed there). Sensors on board the ferret will be able to detect tiny traces of carbon dioxide which indicate the presence of humans concealed in the containers.  Another key benefit is that the ferret will reduce the need for customs and security officials to enter or unpack freight containers, which is time-consuming and may expose officers to danger or possible contamination by harmful substances. By combining two different types of sensor (laser and fibre optic-based), the ferret will lead to confidence in detection being considerably improved.The Engineering and Physical Sciences Research Council (EPSRC) is the UK's main agency for funding research in engineering and the physical sciences. The EPSRC invests around £850 million a year in research and postgraduate training, to help the nation handle the next generation of technological change. www.epsrc.ac.uk/  For more information, contact:Dr Tony Dodd, Department of Automatic Control and Systems Engineering, University of Sheffield, Tel: 0114 222 5636, E-mail: t.j.dodd@sheffield.ac.ukTwo images (Robot 1.jpg and Robot 2.jpg) are available from the EPSRCPress Office, e-mail: pressoffice@epsrc.ac.uk, tel: 01793 444404. Suggested caption: 'a diagram of what the cargo screening ferret could look like'. 
pberzins@stevens.edu4 NSF grants awarded to Stevens' Department of Electrical and Computer EngineeringResearch thrusts in cognitive radio, wireless communications and network securityHOBOKEN, N.J. - For their research efforts in Cognitive Radio (CR), Wireless Communications and Network Security, the Department of Electrical and Computer Engineering (ECE) at Stevens Institute of Technology has been awarded four recent National Science Foundation (NSF) grants in excess of $1 million, with a focus toward an increased contribution in CR advancement. "We are pleased to be acknowledged for our research accomplishments, and we are poised to introduce next-generation technology," said Professor Yu-Dong Yao, who is Department Director in Electrical and Computer Engineering."One of the main priorities of the ECE Departments research is the application and advancement of security measures," he said. "The department acts as an integral purveyor of the Secure Systems research thrust within the university, and each awarded faculty member works as a part of the overall mission to advance the possibilities in regards to wireless network dynamics and network security."Cognitive Networking that Resembles Human InteractionsHattrick Chair Professor Rajarathnam Chandramouli is using a recent NSF grant entitled, "Human Behavior Inspired Cognitive Radio Network Design," to study communication protocols involving cognitive radio networks that resemble human behavior and psychological interactions. New technology research in the fields of Cognitive Radio (CR), Wireless Communications and Network Analysis are leading to dramatic improvements in both security and functionality. CR, which is a form of wireless communication that allows a transceiver to intelligently detect whether communication channels are in use or not, holds the potential to dramatically increase network availability, as well as offer improved security methods that are particularly useful for first-responders and government agencies.Some of the main theoretical ideas will be implemented in SpiderRadio, a Cognitive Radio network prototype developed in the Multimedia Systems Networking and Communications Laboratory at Stevens, and will have a broader impact on wireless networking research and spectrum policy-making communities. Defense from Denial of Service (DoS) AttacksDenial of service (DoS) attacks are a prominent threat in wired networks, and even more potent in the wireless domain. NSF funding for Professor Yu-Dong Yao, "xBeam: Cross-Layer Beamforming Against Denial of Service Attacks in Wireless Networks," is aimed at advancing research towards a novel beam-forming framework called xBeam that is intended to act as a defense against DoS in wireless networks. Dr. Yao's research examines various DoS attacks, develops xBeam algorithms, evaluates the effectiveness of xBeam in deterring DoS attacks, and validates the algorithms using a wireless test bed.His research will substantially improve wireless network security and contribute to advances in networked mobile and wireless society. First Responder NetworksProfessor K.P. Subbalakshmi has received NSF funding entitled, "Denial-of-Service Attacks and Counter Measures in Dynamic Sepctrum Access Networks," for the study of denial-of-service (DoS) attacks that are unique to dynamic spectrum access (DSA) networks. Since DSA networks are expected to play an important role in first responder networks, the solutions proposed are expected to impact design of such networks. While Subbalakshmi believes cognitive-radio enabled dynamic spectrum access (DSA) networks are poised to make significant improvements in spectrum efficiency, she also emphasizes the importance of incorporating adequate security measures at the design stage."The very feature of DSA can prove to be detrimental to the overall goal of better system performance," she said. "We are one of the first groups to identify security vulnerabilities and provide counter measures that are unique to DSA networks. This project will specifically study denial-of-service attacks in DSA networks and provide solutions that will span several layers of the network protocol stack. "We take a holistic approach to this problem," she continued, "by providing both mathematical analysis of the problem as well as demonstrate some of these attacks and counter measures in a practical setting. Our research will have implications in the design of robust and secure inter-operable first-responder networks."Subbalakshmi has several publications in this area, including book chapters and has given a tutorial on this subject at the IEEE Sarnoff Symposium in 2009. She was also an invited panelist at the IEEE International Conference on Communications (ICC) in 2008 on this topic. IEEE ICC is one of the flag-ship conferences of the IEEE Communications Society (COMSOC). Subbalakshmi is also the Chair of the Security Special Interest group of the Technical Committee on Multimedia, IEEE COMSOC.Resolving Interference and Power ConstraintsProfessor Hongbin Li is researching ways to develop an integrated framework for wireless sensor networks. This grant, "Data-Driven Adaptive Quantization for Distributed Inference," addresses a fundamental challenge of quantization for distributed inference in a sensor network environment, where the optimum quantizer generally cannot be implemented due to its dependence on unknown parameters associated with the random events being monitored by the sensor network."Our goal is to develop new sensing and inference techniques by exploiting learning and collaboration among sensor nodes. These techniques will afford improved awareness of the dynamically changing environment in a cognitive network," said Li.His research has the potential to solve several important distributed inference problems with bandwidth and power constraints, further advancing research and development of wireless sensor networks that are expected to have significant economic and social impact. ECE Departments' Impact on ResearchFaculty in the Department of Electrical and Computer Engineering at Stevens are an integral part of Cognitive Radio and Wireless Communications development within the University. These grants enable continued research and innovation towards the ultimate goals of being a worldwide leader in these technologies."Recent grants also work towards new academic opportunities, Technogenesis® programs, and integrated research and education curriculums that aim at the training of our diverse population of students," said Director Yao.About Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value. Stevens offers baccalaureates, master's and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students with about 250 full-time faculty. Stevens' graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.For the latest news about Stevens, please visit www.StevensNewsService.com.
dcruiksh@nsf.govExpeditions in computing continue to break new groundAmerican Recovery and Reinvestment Act enables groundbreaking discovery in computingEnergy-efficient computers optimally designed for custom applications. New tools to make air travel safer and healthcare interventions more effective. Robotic 'bees' that lend a helping hand in search and rescue operations.Directorate for Computer and Information Science and Engineering (CISE) at the National Science Foundation (NSF) established three new Expeditions in Computing in August of this year. Funded at $2 million per year for five years, these projects represent some of the largest single investments made by the directorate."NSF supports Expeditions in Computing to stimulate and leverage the tremendous creativity of the computing research community," said Deborah Crawford, deputy assistant director for CISE. "The projects we support allow academic researchers and their collaborators to explore ideas that promise significant advances in our understanding of the computing frontier, while also yielding great benefit to society."The first Expeditions in Computing were established in 2008. With funding appropriated to NSF through the American Recovery and Reinvestment Act (ARRA), the agency is supporting three new trailblazer Expeditions in 2009, bringing the number of projects currently supported to seven.The three Expeditions awarded in 2009 address a diverse set of challenging problems in computing. "This year's Expeditions projects are driven by challenge problems that arise from the most pressing issues facing our society today--groundbreaking research shaped by societal needs," said Mitra Basu, program officer for the Expeditions program.The awards each feature a lead principal investigator working in conjunction with collaborators at multiple institutions:Next-Generation Model Checking and Abstract Interpretation with a Focus on Embedded Control and Systems BiologyLead PI: Edmund M. Clarke, Carnegie-Mellon UniversityCollaborators: CUNY, NYU, Stony Brook, University of Maryland, Cornell, Jet Propulsion LaboratoryComputer hardware and software systems can be found in almost all aspects of modern life. While most of us think of computers as the machines on our desks, hardware and software systems embedded in a multiplicity of complex physical systems perform a growing number of important societal functions. For example, embedded computers make our airplanes and cars safer and more efficient, they make our national power grid more reliable, and they provide new diagnostic and therapeutic capabilities in healthcare, ranging from medical imaging systems to implantable heart devices. As these tightly integrated cyber-physical systems perform increasingly complex and important functions, we must engineer them in a way that ensures we can bet our lives on them. This Expedition seeks to explore the use of model checking and abstraction interpretation to analyze and predict the behavior of complex embedded and dynamical cyber-physical systems. The research team will develop the next generation of tools and technologies necessary to enable exhaustive analysis of the behavior of increasingly complex systems, promising safer, more secure embedded cyber-physical systems such as those found in automotive and aerospace applications. Furthermore, the researchers participating in this Expedition intend to use these new tools and technologies to develop transformative systems biology models, promising a deeper understanding of complex biological systems such as inter- and intra-cellular signaling in pancreatic cancer.Customizable Domain-Specific ComputingLead PI: Jason Cong, UCLACollaborators: Rice, UC Santa Barbara, Ohio State UniversitySometimes one-size solutions just don't meet all our needs. Human civilization has made great advances through specialization, yet most computer users have limited choices when it comes to the type of hardware and software systems they can use to solve a problem in a particular area. The researchers involved in this Expedition believe customized computing has the potential to deliver order-of-magnitude improvements in energy efficiency, development effort, time-to-solution, cost, and overall productivity by crafting computing tools tailored to specific applications and needs. The key to the team's success is a customizable heterogeneous platform that includes a wide range of customizable computing elements, customizable and scalable high-performance interconnects based on RF-interconnect technologies; highly automated compilation tools and runtime management systems to enable rapid development and deployment of domain-specific computing systems, and a general, reusable methodology for replicating success in different application domains. In the spirit of hardware-software co-design, the research team will balance software and hardware considerations to better expose opportunities for order-of-magnitude improvements in computing efficiency, while using a software approach that supports automation and reuse and is accessible to domain experts. To demonstrate the power of their approach, the team will apply their domain-specific computing design techniques to revolutionize the role of medical imaging and hemodynamic modeling in healthcare, promising cost-effective, yet convenient solutions for preventative, diagnostic, and therapeutic procedures.RoboBees: A Convergence of Body, Brain and ColonyLead PI: Robert Wood, Harvard UniversityCollaborators: Northeastern UniversityBusy as a bee. A hive of activity. Bees and bee colonies have long been held up as models of efficiency and coordination. Using a host of different sensors, unique communication protocols, and a precise hierarchy of task delegation, thousands of bees can work independently on different tasks while all working toward a common goal--keeping their colony alive. Researchers in this Expedition will create robotic bees that fly autonomously and coordinate activities amongst themselves and the hive, much like real bees. The research team aims to drive research in compact high-energy power sources, ultra-low-power computing, and the design of distributed algorithms for multi-agent systems. Furthermore, the RoboBees created will provide unique insights into how Mother Nature conjures such elegant solutions to solve complex problems.Basu said that the Expeditions program will continue in the future. "In the last two competitions, the research community has responded with great creativity and enthusiasm.  We are now eagerly awaiting the arrival of the next set of Expeditions proposals."
joseph.winters@iop.orgResearchers unite to distribute quantum keysResearchers from across Europe have united to build the largest quantum key distribution network ever built.  The efforts of 41 research and industrial organisations were realised as secure, quantum encrypted information was sent over an eight node, mesh network.With an average link length of 20 to 30 kilometres, and the longest link being 83 kilometres, the researchers from organisations such as the AIT Austrian Institute of Technology (formerly Austrian Research Centers), id Quantique, Toshiba Research in the UK, Université de Genève, the University of Vienna, CNRS, Thales, LMU Munich, Siemens, and many more have broken all previous records and taken another huge stride towards practical implementation of secure, quantum-encrypted communication networks.A journal paper, 'The SECOQC Key Distribution Network in Vienna', published as part of IOP Publishing's New Journal of Physics' Focus Issue on 'Quantum Cryptography: Theory and Practice', illustrates the operation of the network and gives an initial estimate for transmission capacity (the maximum amount of keys that can be exchanged on a quantum key distribution, QKD, network).  Undertaken in late 2008, using the company internal glass fibre ring of Siemens and 4 of its dependencies across Vienna plus a repeater station, near St. Pölten in Lower Austria, the QKD demonstration involved secure telephone communication and video-conference as well as a rerouting experiment which demonstrated the functionality of the SEcure COmmunication network based on Quantum Cryptography (SECOQC). One of the first practical applications to emerge from advances in the sometimes baffling study of quantum mechanics, quantum cryptography has become a soon-to-be reached benchmark in secure communications.  Quantum mechanics describes the fundamental nature of matter at the atomic level and offers very intriguing, often counter-intuitive, explanations to help us understand the building blocks that construct the world around us. Quantum cryptography uses the quantum mechanical behaviour of photons, the fundamental particles of light, to enable highly secure transmission of data beyond that achievable by classical methods.The photons themselves are used to distribute cryptographic key to access encrypted information, such as a highly sensitive transaction file that, say, a bank wishes to keep completely confidential, which can be sent along practical communication lines, made of fibre optics. Quantum indeterminacy, the quantum mechanics dictum which states that measuring an unknown quantum state will change it, means that the information cannot be accessed by a third party without corrupting it beyond recovery and therefore making the act of hacking futile.  The researchers write, "In our paper we have put forward, for the first time, a systematic design that allows unrestricted scalability and interoperability of QKD technologies." 
kimgw@etri.re.krControlling the language of securityA new language could improve home computer securityKorean computer scientists have developed a security policy specification for home networks that could make us more secure from cyber attack in our homes. They report details in the International Journal of Ad Hoc and Ubiquitous Computing.Companies, banks, and other organizations take internet security very seriously and usually have firewalls and IT departments to protect them from attack as a matter of course. Domestic and small office networks are just as vulnerable to hacking, malicious computer code, worms, viruses, and eavesdropping. An attack can wreak havoc on individuals and small businesses when security it compromised.With home and small office networks connecting all kinds of devices - personal computers, mobile devices, remote security cameras, gaming consoles, and more - they represent an even more heterogeneous mix than many larger offices.Now, Geon Woo Kim of the Electronics and Telecommunications Research Institute, in Korea, and colleagues there and at Kyungpook National University, have developed a specification for security policy on home networks that can guarantee reliability and availability. The specification also takes into account authentication, authorization, security policy deployment so that all users in the home are not only protected from malware but also can help ensure everyone can use the network when they need to.Kim and his team explain that home networks most commonly have only a single gateway from the internet. Every packet of information must pass through this gateway at the border between the home network and the internet. It should act as a core component providing all security. "Whenever a new access to the home network is found, it should be able to authenticate and authorize it and enforce the security policy based on rules set by the home administrator," the team says.However, to make such an approach effective but simple requires a way to consistently describe and specify the security policy. The computer scientists first turned to a computer markup language, eXtensible Access Control Markup Language (XACML). XACML is a general purpose language and so it lacks the notation for security policies and authorization rules. The team has now developed a related language - Home security Description Language, xHDL - that includes the necessary notation for securing a home network.The new language consists of seven elements: combining-rule element, authentication element, user element, object element, object-group element, role element, and rule elements. Each of these terms within xHDL could be used to run a browser-based control centre. That program would provide the domestic administrator with simple control options to allow access to the home network only for specific devices and to control the packets of information that can pass through the gateway to and from the internet."Security policy specification for home network" in Int. J. Ad Hoc and Ubiquitous Computing, 2009, 4, 372-378
evelyn.brown@nist.govNew computer security guide can help safeguard your small businessJust in time for October's Cyber Security Awareness Month, the National Institute of Standards and Technology (NIST) has published a guide to help small businesses and organizations understand how to provide basic security for their information, systems and networks. NIST has also created a video that explores the reasons small businesses need to secure their data (at right).The guide, Small Business Information Security: The Fundamentals, was authored by Richard Kissel, who spends much of his time on the road teaching computer security to groups of small business owners ranging from tow truck operators to managers of hospitals, small manufacturers and nonprofit organizations. The 20-page guide uses simple and clear language to walk small business owners through the important steps necessary to secure their computer systems and data.Small businesses make up more than 95 percent of the nation's businesses, are responsible for about 50 percent of the Gross National Product and create about 50 percent of the country's new jobs, according to a 2009 Small Business Administration report. Yet these organizations rarely have the information technology resources to protect their sensitive information that larger corporations do.Consequently, they could be seen as easy marks by hackers and cyber criminals, who could easily focus more of their unwanted attention on small businesses. And just like big companies, the computers at small businesses hold sensitive information on customers, employees and business partners that needs to be guarded, Kissel says. He adds that regulatory agencies have requirements to protect some health, financial and other information."There's a very small set of actions that a small business can do to avoid being an easy target, but they have to be done and done consistently," Kissel says.In the guide Kissel provides 10 "absolutely necessary steps" to secure information, which includes such basics as installing firewalls, patching operating systems and applications and backing up business data, as well as controlling physical access to network components and training employees in basic security principles.He also provides 10 potential security trouble spots to be aware of such as e-mail, social media, online banking, Web surfing and downloading software from the Internet, as well as security planning considerations. The guide's appendices provide assistance on identifying and prioritizing an organization's information types, recognizing the protection an organization needs for its priority information types and estimating the potential costs of bad things happening to important business information.	NIST works with the Small Business Administration and the Federal Bureau of Investigation in this outreach to educate small businesses.Small Business Information Security: The Fundamentals can be downloaded from the Small Business Corner Web site at http://www.csrc.nist.gov/groups/SMA/sbc/.The related video, "Information Technology Security for Small Business. It's not just good business. It's essential business," features experts from NIST and the Small Business Administration. The video is available on You Tube and the Small Business Corner of the NIST Computer Security Web pages. A free DVD of the video may be obtained by contacting Rich Kissel at (301) 975-5017 or by email at richard.kissel@nist.gov
mariangela.dacunto@esa.intESA campaign reveals glimpse of future Sentinel-3 imageryAs part of the development process for ESA's Sentinel-3 Earth observation mission, remote-sensing experts carried out an extensive experiment campaign across southern Europe this summer. The results provide valuable insight into the imagery the mission will deliver after it is launched in 2013.For all new Earth observation missions, a crucial part of the development process, after defining and designing the instruments, is to assess the future performance of the sensors. In addition, the algorithms being developed to transform the satellite data into usable information products also have to be tested. In order to make these assessments, ESA organises test campaigns using airborne instruments that closely match the characteristics of the spaceborne sensors. The effort is coordinated with ground-based teams that collect complementary scientific data for calibration and evaluation.  One such campaign was recently completed for Sentinel-3, which is the third in a series of five space missions ESA is developing for the Global Monitoring for Environment and Security (GMES) initiative. Led by the European Commission, GMES will fulfil the growing need among European policy-makers to access accurate and timely information services to manage the environment, understand and mitigate the effects of climate change, and ensure civil security. The 'Sentinel-3 Experiment' campaign  or Sen3Exp for short  involved a series of coordinated activities with scientists making ground-based measurements in Spain, Italy and the Ligurian and Adriatic Seas, while aircraft with sensitive instrumentation passed overhead and satellites acquired data simultaneously from space. The result is a comprehensive dataset of imagery and ground-truth information that can be used to simulate Sentinel-3 optical data, test the processors under development to generate the data products, and analyse whether these data products will satisfy the requirements of the user communities. The campaign's Principal Investigator, Dr Carsten Brockmann, confirmed that, "A unique, comprehensive and valuable dataset has been created that will significantly support the development of the Sentinel-3 mission." Primarily, Sentinel-3 will support services related to the marine environment, such as maritime safety services that need ocean surface-wave information, ocean-current forecasting services that need surface-temperature information, and sea-water quality and pollution monitoring services that require advanced ocean colour products from both the open ocean and coastal areas. Sentinel-3 will also serve numerous land, atmospheric and cryospheric application areas such as land-use change monitoring, forest cover mapping and fire detection.  The mission's complement of optical sensors will comprise an Ocean Land Colour Instrument (OLCI), which is based on Envisat's Medium Resolution Imaging Spectrometer (MERIS), and a Sea Land Surface Temperature Radiometer (SLSTR), which is a successor to Envisat's Advanced Along Track Scanning Radiometer (AATSR). The Sen3Exp campaign began in June in Barrax, La Mancha, Spain. An aircraft operated by the Spanish National Institute for Aerospace Technology (INTA), equipped with three hyperspectral imaging spectrometers, made two flights over the area. Meanwhile, satellite data were acquired by Envisat's MERIS and AATSR and by the Compact High Resolution Imaging Spectrometer (CHRIS) aboard ESA's Proba-1 satellite. At the same time, ground teams, under the direction of Prof. Jose Moreno from the University of Valencia, made atmospheric radiometric and biophysical measurements. The campaign then moved to Pisa in Italy, from where a pine forest at San Rossore could be reached. At San Rossore, Prof. Federico Magnani from the University of Bologna oversaw the week-long ground measurement programme. The dataset was again complemented with MERIS, AATSR and CHRIS satellite data. In July, activities focused on the marine environment where measurements were taken at two oceanic sites: the Boussole monitoring buoy in the Ligurian Sea and the Aqua Alta Oceanographic Tower (AAOT) in the Adriatic Sea, close to Venice. Both sites have played an important role in supporting ocean colour algorithm development and product validation for many years. Boussole typifies the global ocean, where the measured signal is determined solely by the absorption of phytoplankton. AAOT is in an area where there is both open ocean water and also water that is optically complex because phytoplankton, suspended sediments and coloured dissolved organic matter also affect the measured signal. Such water can be found in all coastal regions and is a challenge to understand from space. Routine radiometry measurements are made at these locations, both above and below the water surface and fed into the Mermaid database, managed by ARGANS, UK. A unique flight pattern was developed by the Sen3Exp team that encompassed a wide range of observational configurations. Two overpasses over each site were carried out and an image over the coast that included the transition between land and ocean was also acquired, which will be important for understanding how the signal behaves in coastal zones. The campaign also took advantage of the fact that the MERIS 15 spectral bands can be reprogrammed. Thus, for several short periods during the campaign window, data were acquired using some of the new spectral bands planned for OLCI and provided some of the most realistic simulations possible of the data expected from Sentinel-3. With more than 60 people involved, the success of this technically and logistically complex campaign demonstrates the excellent cooperation between European scientists. Now comes the task of analysing the huge dataset collected during the campaign. An additional opportunity for data analysis is provided by the inclusion in of data collected from the Airborne Prism Experiment (APEX) imaging spectrometer during a parallel campaign in Switzerland and Belgium. In the meantime, Professors Moreno and Magnani agree that, "A large data archive has been generated that will help not only to provide important input for Sentinel-3 but will be valuable for future ESA missions."
jgundersen1@partners.orgBrain-computer interface begins new clinical trial for paralysisBrainGate2 research finds new home at Massachusetts General HospitalBOSTON  Scientists at the Massachusetts General Hospital (MGH) have initiated the BrainGate2 pilot clinical trial to expand restorative neurotechnology research for some patients with paralysis. This trial expands on previous research that explores methods that may help paralyzed patients control assistive technologies. The research, to be conducted jointly by physician researchers at MGH and neuroscientists and engineers at Brown University, has received approval from the hospital's Institutional Review Board (IRB) to begin recruiting patients.  John Donoghue, PhD, of Brown and the Providence VA Medical Center, and Leigh Hochberg, MD, PhD, of MGH, Brown, the VA and Harvard Medical School are leading this research to evaluate how people with spinal cord injury, brainstem stroke, muscular dystrophy, amyotrophic lateral sclerosis (ALS), or limb loss may be able to use brain signals to control assistive devices. "We are working to develop and test technologies that we hope will help patients with devastating illnesses that limit their ability to move or to speak," says Hochberg, a vascular and critical care neurologist at MGH, Brigham and Women's Hospital and Spaulding Rehabilitation Hospital.  "The goal of our research is to harness the brain signals that ordinarily accompany movement and to translate those signals into actions on a computer, like moving a cursor on the screen, or the movement of a robotic or prosthetic limb."Donoghue said the new trial is taking place at a time of great promise for neurotechnology research."We are entering a new age of neurotechnology," Donoghue says. "Our fundamental understanding of the nervous system, combined with advances in engineering may help people with brain and spinal cord injuries and diseases."A previous clinical trial run by an outside company, Cyberkinetics, Inc., together with researchers at MGH and Brown, demonstrated that the neural signals associated with the intent to move a limb can be "decoded" by a computer in real-time and used to operate external devices.  This device, called the BrainGate Neural Interface System, involved a sensor placed on a part of a study participant's brain called the motor cortex.  During research sessions, a computer was connected to the sensor through a port on the participant's head, allowing participants to control a computer cursor by simply thinking about the movement of their own paralyzed hand.  "We learned an incredible amount with the assistance of the first participants in the BrainGate trial, not only about how the motor cortex continues to work after paralyzing illness or injury, but also about how to harness these powerful intracortical signals for controlling computers and other assistive devices," Hochberg says.For financial reasons, Cyberkinetics stopped funding the trial and withdrew itself from the research.  The promising clinical trial continues, now based at Mass. General. A new academically-based Investigational Device Exemption (IDE) application, BrainGate2, was developed in 2008 to follow-up on research previously published in peer reviewed journals. The BrainGate2 pilot clinical trials will be directed by Dr. Hochberg at MGH with close collaboration with researchers at Brown University and the Providence VA Medical Center. BrainGate2 will expand on previous research, honing the hardware and software that decode the brain signals that are causing the cursor to move on a screen.This IDE is part of a larger research effort, the ultimate goals of which include "turning thought into action": developing point and click capabilities on a computer screen, controlling a prosthetic limb and a robotic arm, controlling functional electrical stimulation (FES) of nerves disconnected from the brain due to paralysis, and further expanding the neuroscience underlying the field of intracortical neurotechnology.  The research is focused not only on the ability to operate a computer, but also to assist people with ALS, spinal cord injury and stroke to control over their environment.  "Through ongoing development and testing, it is hoped that these technologies will eventually help to improve the communication, mobility and independence of people with severe paralysis," says Dr. Hochberg.People with these types of paralysis have at least two characteristics in common: a brain that wants to direct movement and a body that fails to respond accordingly.  Beyond the current clinical trial, the goal of the BrainGate research effort is to someday be able to provide a new pathway for brain signals to control external devices such as computers, or even one's own limbs that had been "disconnected" from the brain due to paralysis. 	The research is funded entirely by federal (NIH, VA) and philanthropic sources.Additional information about BrainGate2 can be accessed at www.braingate2.org. Inquires related to the research can be directed to clinicaltrials@braingate2.org. This research is based upon work supported in part by the Office of Research and Development, Rehabilitation R&D Service, Department of Veterans Affairs.Massachusetts General Hospital, established in 1811, is the original and largest teaching hospital of Harvard Medical School. The MGH conducts the largest hospital-based research program in the United States, with an annual research budget of more than $500 million and major research centers in AIDS, cardiovascular research, cancer, computational and integrative biology, cutaneous biology, human genetics, medical imaging, neurodegenerative disorders, regenerative medicine, systems biology, transplantation biology and photomedicine.Founded in 1764, Brown University is the seventh-oldest college in the United States and a member of the Ivy League. The faculty attracts over $130 million in research funding annually from government and private sources, including the Department of Health and Human Services, the National Science Foundation, the Department of Defense and Energy, the Department of Education, and a number of private, corporate and non-profit organizations.CAUTION: Investigational Device. Limited by Federal Law to Investigational Use.
mhisham@ntu.edu.sgNTU unveils green and fastest supercomputer in ASEAN in collaboration with leading IT giantsNanyang Technological University receives a significant boost in its research efforts with the installation of a green supercomputer at its new High Performance Computing Center on campusSingapore's main science and technology university, Nanyang Technological University (NTU), receives a significant boost in its research efforts with the installation of a green supercomputer at its new High Performance Computing (HPC) Centre on campus. This achievement is in partnership with Jardine OneSolution (2001) Pte Ltd, IBM (NYSE: IBM) - maker of nearly half of the world's 500 fastest computers, Intel, the world leader in silicon innovation, and Red Hat, the world's leading open source solutions provider. NTU's HPC Centre, expected to be operational in October 2009, will be based on the first IBM System x iDataplex cluster in the Association of Southsast Asian Nations (ASEAN) and the largest in Asia-Pacific. It is powered by the new Intel® Xeon® processor 5500 series that automatically adjusts to specified energy usage levels and speed data centre transactions, thus reducing electricity consumption. With its measured computing power (Rmax) of over 28 teraflops (trillion mathematical calculations per second), NTU's HPC system will be the fastest in ASEAN. This HPC Centre's supercomputer is currently placed at No. 267 of the world's most powerful supercomputers, according to the latest TOP500 List. It is also the 24th most energy efficient system on the Green500 list with 266.68 Mflops (millions of floating point operations per second) per watt. Today, advancement in research in a wide range of scientific fields is increasingly dependent on the generation and analysis of massive data sets and innovations in high performance computing, networking, storage and analytical capabilities. This initiative thus closes a gap in high performance computational capacity in the University, readying it to meet computational complexities encountered in a myriad of multidisciplinary research problems and to create new synergies and strengths in discoveries. With this HPC system, the impact will be extensive across disciplines, from developing future energy sources, studying global climate change, designing new materials, to understanding biological systems and physics of complex socio-economic systems, among others.   More can also be achieved in research such as in the modelling of volcanic activities, to understanding the earth's tectonic movements, research in the study of water treatment process, as well as the simulation of flight dynamics. "With growing HPC adoption worldwide, we are excited that the new facility will put NTU at the forefront of high performance computing.  By offering it as a central computing resource to the 2,800 faculty and researchers in the University, we cater to the varied computing needs demanded by the academic disciplines in the Institution and facilitate advancement of our many strategic research initiatives," said Professor Bertil Andersson, Provost, NTU. "IBM is actively engaged with universities throughout the world, working to put the latest technology into the hands of leading researchers.  We are delighted to partner with NTU on such a bold and technologically advanced HPC System," said Teresa Lim, Managing Director, IBM Singapore. "Designed to meet varying demands across a number of academic disciplines, the IBM system will enable NTU's faculty and researchers to work on larger, more complex problems and help find the answers to some of life's most perplexing problems." "The enterprise-class Intel® Xeon® processor 5500 series can play a key role in scientific discoveries by researchers who use supercomputers as their foundation for research, all whilst delivering great energy efficiency for reduced electricity costs," said Patrick Liew, Singapore Country Manager, Intel Technology Asia. "This HPC cluster by NTU will create opportunities to solve the world's most complex challenges and push the limits of science and technology." The IBM System x iDataplex cluster solution uses Quad Data Rate (QDR) InfiniBand as the Node Interconnect as well as the IBM System Storage DCS9900 Storage System. The Voltaire Infiniband QDR interconnect Director-class switches in the HPC system configuration makes NTU the first in Asia to utilise a leading-edge technology that couples input ports and output ports to realise significantly improved performance. In addition, the IBM System Storage DCS9900, a high performance storage system designed for the storage needs of highly scalable, data streaming applications, is arguably one of the fastest and densest storage solutions available today. IBM's full suite of HPC software will also be used to manage the full cluster, including IBM's General Parallel File System (GPFS), a workhorse file system that powers many of the world's largest supercomputer sites. Performance in the NTU's HPC system is further boosted by the Intel Xeon processor 5500 series which offers energy-efficiency features and more than three times the memory bandwidth than the dual-socket architecture. This translates to significant and immediate gains in application performance by up to three times without the need for further investment in software development or increased power and cooling support for the data centre.   The NTU HPC system will run on the industry's leading enterprise open source operating environment, Red Hat Enterprise Linux (RHEL). "Today, most of the world's HPC centres already run on RHEL and has proven to offer true mission-critical stability, operational flexibility, world-class performance, security, and stability," said Patrick Lim, ASEAN General Manager, Red Hat Asia Pacific. "Based on Top 500 supercomputer sites, Linux commands an almost ninety percent share for operating systems for High Performance Computing." Besides performance, with power and cooling the number one issue facing many HPC sites, the system at NTU will be one of the greenest HPC systems in the region.  NTU's HPC system maximises performance with a unique water-cooled technology - IBM's Rear Door Heat eXchanger for the iDataplex Rack. This liquid-cooled panel on the back of the unit will eliminate the need for computer-room air conditioners, allowing for room-temperature operation. The IBM Rear Door Heat eXchanger for iDataplex will also allow NTU to have a high-density data centre environment that will not increase cooling demands  and may actually lower them. 
john.verrico@dhs.govHomeland security's levee PLUGS pass a second test    It's a mean old levee, cause me to weep and moan     It's a mean old levee, cause me to weep and moan     Gonna leave my baby, and my happy homeThe lyrics to the 1929 blues classic "When the Levee Breaks" (the original recording can be found on the web) refer to the cataclysmic flood that began when heavy rains pounded the central basin of the Mississippi River in summer 1926. Swollen to capacity, the Mississippi broke out of its levee system in 145 places, flooding 17 million acres, and affecting an area the size of New England. Nearly a million people were displaced.The levee failures in New Orleans during Hurricane Katrina are, of course, fresher in the American mind.     If it keeps on rainin', levee's goin' to break     If it keeps on rainin', levee's goin' to break     And the water gonna come in, have no place to stayAaron Neville's song, Louisiana 1927, sung by Randy Newman, was alsoabout the 1926-1927 tragedy, and it became the theme song of theKatrina/Rita disaster.The challenge is to change that tune: to develop the technology to quickly seal a levee breach and reduce floodwaters through the opening within four to six hours of detectionbefore the water can do major damage.Enter Wil Laska of the Science & Technology Directorate (S&T), the research arm of the Department of Homeland Security. Laska has sought out innovative technologies from industry, academia, and government to meet this challenge. Any proposed system, he dictated, had to not only be capable of quickly closing breaches, but also be suitable for scenarios in which the breach may be difficult or impossible to reach with conventional construction equipment."The thing is," Laska deadpans, "there's an effective structural material that's readily available during floodswater."He found four technologies that met his requirements, and on Nov. 9, 2009, all of them passed their second test at the U.S. Department of Agriculture's Agriculture Research Service, Hydraulic Engineering Research Unit in Stillwater, Okla. The facility is used by the Army Corps of Engineers to test hydrology equipment and study water flow, dams and levees.The largest technology, proposed by the Army Corps of Engineers Engineering Research and Development Center in Vicksburg, Miss., is a large balloon or tubelight enough to be transported by helicopter and flexible enough to adapt to a wide range of environmental situations. When launched or dropped, the engineers hypothesized, the tube would in quick succession fill with water, float on the flood currents to the breach, and adhere to the breach in the earthen berm or levee that had failed.It worked.Dubbed the Portable Lightweight Ubiquitous Gasket (PLUG), the tube of non-stretch fabric is dropped into the floodwaters and an attached pump rapidly fills it to 80 percent capacitya bubble of air inside keeps the tube from sinking beneath the waters. Positioned upstream, flood currents pull it toward the breach. The incompressible nature of water and the unyielding fabric turn the tube into a rigid plug that conforms to the breach and seals it.Monday's PLUG demonstration was about 30 per cent larger than the ¼ scale model that was first successfully tested in September 2008. The Stillwater site is currently the only facility that can provide the water flows needed125 cubic feet per second for several minutes.While the PLUG system is designed specifically for narrow, deep breaches, several other solutions tailored for other types of levee breaches were also tested on Nov. 9:Oh cryin' won't help you, prayin' won't do no goodOh cryin' won't help you, prayin' won't do no goodWhen the levee breaks, mama, you got to loseEngineers could be on their way to writing a less bluesy version of a 90-year-old song:When the levee breaks, mama, you may need a PLUG.	BackgroundThere are roughly 14,000 miles of levees owned and maintained by the U.S. Army Corps of Engineers and an estimated 85,000 miles of privately owned and operated levees. Most are more than 50 years old, and many were built in agricultural areas now deeply embedded in the urban landscape.Levees fail for many reasons, not all of which are weather related. For instance, California's major concern is liquefaction of their levees during an earthquake. And some Midwestern levees have failed under sunny skies due to erosion caused by the long-term effects of previous high water and flood conditions.The intended primary customer of the PLUG would be local levee boards and State Emergency Management Agencies.
avogel@gatech.eduGTRI is developing protocols for testing effects of RFID systems on medical devicesRadio frequency identification (RFID) systems are widely used for applications that include inventory management, package tracking, toll collection, passport identification and airport luggage security. More recently, these systems have found their way into medical environments to track patients, equipment assets and staff members.However, there is currently no published standardized, repeatable methodology by which manufacturers of RFID equipment or medical devices can assess potential issues with electromagnetic interference and evaluate means to mitigate them.To resolve these concerns, the Georgia Tech Research Institute (GTRI) recently began developing testing protocols for RFID technology in the health care setting. The test protocol development is being overseen by AIM Global, the international trade association representing automatic identification and mobility technology solution providers, and also includes MET Laboratories, a company that provides testing and certification services for medical devices."A comprehensive set of test protocols, which are sufficiently precise to permit repeatable results, is required to understand if there is an interaction between various types of RFID systems and active implantable medical devices, electronic medical equipment, in vitro diagnostic equipment and biologics. Only after the protocols are developed will we be able to investigate the cause of any interactions, the result of any interactions, and ways manufacturers might eliminate or mitigate interactions," said Craig K. Harmon, president and CEO of Q.E.D. Systems and chairman of AIM Global's RFID Experts Group. This group is overseeing the Health Care Initiative and includes representatives from 40 organizations in the United States, Europe and Asia.GTRI researchers will test how RFID systems affect the function of implantable and wearable medical devices, such as pacemakers, implantable cardioverter defibrillators, neurostimulators, implantable infusion pumps and cardiac monitors. "The internal components, firmware and hardware of every company's devices are different, meaning that each device can respond differently to the same electromagnetic environment. Since there have been various preliminary tests and publications from different organizations indicating that there may or may not be issues with RFID system environments and these devices, it is important to standardize the way to test such devices," said Ralph Herkert, director of GTRI's Medical Device Test Center.Herkert and Gisele Bennett, director of GTRI's Electro-Optical Systems Laboratory, will evaluate and determine the best method for measuring whether interference takes place as a result of RFID emission in both active and passive RFID technologies covering the spectrum from low-frequency to ultra high-frequency. The researchers will test whether radio frequency-emitting devices cause any negative effects on the medical devices, and under what conditions these effects might occur. Testing will also determine whether specific medical devices are particularly susceptible to certain radio frequency identification characteristics and if any corrective actions can be taken to mitigate such susceptibility.Medical device testing is not new for GTRI, which established its Medical Device Test Center more than 14 years ago. The facility was created to enable manufacturers of implantable cardiac pacemakers and defibrillators to work with providers of electronic article surveillance (EAS) systems, used by retailers, libraries and other establishments to prevent theft and track inventory. The center's original mission was to help manufacturers improve compatibility between implantable medical devices and EAS systems that radiate electromagnetic energy. In 2006, GTRI expanded its operations and facilities to test new types of security and logistical systems (SLS), including RFID. To test the effects of RFID systems on medical devices, the researchers simulate real-world conditions by placing a medical device in a tank of saline solution that simulates the electrical characteristics of body tissue and fluid. The medical device is then exposed to different RFID technologies. Several tests are performed with the device placed in different orientations to represent how people typically interact with the emissions. "We think the testing procedure for RFID systems will be similar to the EAS system procedure, but there are a few more challenges with the RFID systems because a person doesn't always pass through a portal," noted Bennett, who is also a member of AIM Global's RFID Experts Group. "Medical devices can be affected by active tags with stronger signals or RFID systems reading passive tag signals."The test protocols developed by GTRI will be submitted to the U.S. Food and Drug Administration for concurrence, after which a worldwide certification program will be launched and other testing facilities will be invited to participate.Funding to develop these test guidelines is currently being provided by GTRI, but the researchers are actively looking for external funding."We have more than 35 years of experience at GTRI testing medical device interference and we think that testing the effects of RFID on medical devices is an important area to pursue," added Bennett.
bcooper@anl.govArgonne develops program for cyber security 'neighborhood watch'Cyber security team wins 2009 DOE innovation, technology achievement awardARGONNE, Ill. (July 16, 2009)  U.S. Department of Energy laboratories fight off millions of cyber attacks every year, but a near real-time dialog between these labs about this hostile activity has never existed  until now.Scientists at DOE's Argonne National Laboratory have devised a program that allows for Cyber Security defense systems to communicate when attacked and transmit that information to cyber systems at other institutions in the hopes of strengthening the overall cyber security posture of the complex."The Federated Model for Cyber Security acts as a virtual neighborhood watch program.  If one institution is attacked; secure and timely communication to others in the Federation will aide in protecting them from that same attack through active response," cyber security officer Michael Skwarek said. Prior to the development of the Federated Model for Cyber Security, the exchange of hostile activity was solely on the shoulders of the human element.  In cyber attacks, every second counts and the quicker that such information can be securely shared, will assist in strengthening others against similar attacks.  With millions of cyber security probes a day, the human element will not be successful alone."This program addresses the need for the exchange of hostile activity information, with the goal of reducing the time to react across the complex.  History has shown, hostile activity is often targeted at more than one location, and having our defenses ready and armed will assist greatly." Skwarek said.Currently, the program is capable of transmitting information regarding hostile IP addresses and domain names, and will soon be able to share hostile email address and web URLs to others in the Federation.The development of this program led to Skwarek along with Argonne's cyber security team members Matt Kwiatkowski, Tami Martin, Scott Pinkerton, Chris Poetzel, Gene Rackow and Conrad Zadlo winning the DOE's 2009 Cyber Security Innovation and Technology Achievement Award. The Federated Model for Cyber Security has proved to be an important cyber security and communication tool.  Use in the private sector, as well as in institutions with heavy collaborative efforts, can realize an operational gain by leveraging the power of sharing and learning from others on what they see and defend against on a daily basis.The U.S. Department of Energy's Argonne National Laboratory seeks solutions to pressing national problems in science and technology. The nation's first national laboratory, Argonne conducts leading-edge basic and applied scientific research in virtually every scientific discipline. Argonne researchers work closely with researchers from hundreds of companies, universities, and federal, state and municipal agencies to help them solve their specific problems, advance America's scientific leadership and prepare the nation for a better future. With employees from more than 60 nations, Argonne is managed by UChicago Argonne, LLC for the U.S. Department of Energy's Office of Science. 
ahopp@uh.eduUniversity of Houston's information security program recognized by NSA, DHSDesignation gives students better shot at advancement in high-stakes, high-dollar field of cyber securityWhile most of us give little thought to how our personal and professional data is secured or whether malicious or mischievous perpetrators are trying to get their hands on it, legions of information security professionals working behind the scenes do the worrying for us and try mightily to thwart threats. Industry insiders emphasize that, as new technologies and violators emerge at every turn, hiring the right kind of talent is becoming much more important, which makes a recent University of Houston designation by the National Security Agency and Department of Homeland Security much more significant. At the 13th Colloquium for Information Systems Security Education in Seattle last month, the university was named a national center of academic excellence in information assurance education. The designation means a lot to UH College of Technology instructional associate professor Edward Crowley, who headed up the rigorous application process. "At UH, the quest for academic excellence in information security began in 2002. Since then, the curriculum has continued to evolve and improve thanks, in part, to input from the National Security Agency, as well as the FBI, Secret Service and the Houston Police Department's computer forensics group," Crowley said.The need for these behind-the-scenes and battle-ready information security experts should not be underestimated, said Michael Gibson, who heads the information and logistics technology department. "You can do more damage with a computer than you can with bullets. Think about all of the systems that run our traffic, our power grid, our energy-distribution channels," he said.Crowley points to recent events such as Lockheed Martin's loss of F-35 fighter project data, Virginia's loss of personal health information, cyber attacks in Estonia and Georgia as well as the growing risk of identity theft.Last month the Partnership for Public Service, a Washington-based advocacy group focused on government service, issued a report detailing serious problems within the professional community charged with protecting the government's networks.   Its authors made several recommendations to the Obama administration, emphasizing that the safety of the nation requires building "a vibrant, highly trained and dedicated federal cyber-security work force."Anne M. Rogers, director of information safeguards for Waste Management in Houston, lauds UH's approach."One reason we have cyber security problems is that people have focused mainly on software features and functions without considering security. That led to a lot of buggy software  things built with inherent vulnerabilities. These systems may do wonderful things, but, if data leaks out or hackers easily get in and out of them, we lose," she said. "So, it's really important to build and assess for integrity, security and adequate control. The UH program brings this focus."UH's technology project management information systems security graduate degree program serves both UH students and non-degree-seekers who want information-assurance training. Five to 10 students complete the program each year, and Gibson expects the designation to increase demand for the curriculum.Many of those enrolled already are information technology professionals who aim to climb the corporate ladder or join the government work force at more advanced grades. Others, like alumnus Chad Van Zandt, move straight into the program after finishing undergraduate work.After getting his degree in information systems in 2004, Van Zandt enrolled in the technology project management graduate program and soon after began working as an intern at Houston's Gray Hat Research Corp. Upon graduation in 2006 ­­­­­­­­­­­­­­­­­­with his information assurance certification, he was promoted to executive director of educational services and consulting for the company in California."This is a relevant, cutting-edge program that helps shape its graduates to become formidable figures in the security technology field," he said. "To say the valuable knowledge and experience gained from this program has played a vital role in my career development would be an understatement."While career advancement is possible without the certification, Gibson said, those who do obtain it are more likely to enter into the work force at a higher rung and rise more quickly."If you look across the global regulatory world, there's less and less tolerance for inappropriate information handling," explained Rogers, whose company has hired two graduates of the UH program. "As everything goes electronic, they're hiring the best people to go to battle."Paul Williams, chief technology officer for Gray Hat Research Corp., said he looks for "the heroes of tomorrow" when hiring for his company and its clients, because  true information assurance requires getting ahead of the curve and seeing the big picture."What the industry is looking for is the geniuses  this middle layer where expertise is so lacking," he said. "At the high end, enterprise security really is rocket science. We have clients that have 50,000 computers in the world  in 21 time zones  and tens of thousands of employees. It's a million times harder to secure those computers than just one. They need people who will ask: 'How do we cost-effectively change the paradigm of this company so that we can do more with this money?' We're talking about using what you have today to mitigate the greatest risk."Elizabeth Anderson-Fletcher, associate vice president for research operations, said the designation supports faculty efforts to contribute to Department of Defense and Department of Homeland Security research programs."Examples of potential cyber security research projects would include the research and testing of procedures in the secure use of Internet-enabled supervisory control and data acquisition, or SCADA, systems that monitor, coordinate and control critical infrastructure in the energy sector.  In health care, cyber security becomes a major concern as we move to the digitization of patient medical records," she said.The four NSA-certified courses offered at UH include Principles of Information Security, Enterprise Security: Incident Response/Corp. Forensics, Applied Cryptography and Secure Communications, and Information Security Risk Analysis and Management. The training standards embedded in the courses are a part of the Information Assurance Courseware Evaluation (IACE) Program established by the Committee on National Security Systems (CNSS). Each institution designated as a center of excellence in information assurance education must recertify its courses and submit an application for renewal as a center of excellence every five years.	About the University of HoustonThe University of Houston, Texas' premier metropolitan research and teaching institution, is home to more than 40 research centers and institutes and sponsors more than 300 partnerships with corporate, civic and governmental entities.  UH, the most diverse research university in the country, stands at the forefront of education, research and service with more than 36,000 students.  About the College of TechnologyThe College of Technology educates leaders in innovation and global industry. With nearly 2,000 students, the college offers accredited undergraduate and graduate degrees in construction management technology, consumer science and merchandising, computer engineering technology, electrical power technology, logistics technology, network communications, human resources development and technology project management. It also offers specialized programs in biotechnology, surveying and mapping and digital media.For more information about UH, visit the university's Newsroom at http://www.uh.edu/news-events/.  To receive UH science news via e-mail, visit http://www.uh.edu/news-events/mailing-lists/sciencelistserv.php.  
lindsay.sheppard@sri.comResearch teams successfully operate multiple biomedical robots from numerous locationsNew software protocol enables interoperability among telesurgical systemsMENLO PARK, Calif. September 17, 2009 - Using a new software protocol called the Interoperable Telesurgical Protocol, nine research teams from universities and research institutes around the world recently collaborated on the first successful demonstration of multiple biomedical robots operated from different locations in the U.S., Europe, and Asia. SRI International operated its M7 surgical robot for this demonstration. In a 24-hour period, each participating group connected over the Internet and controlled robots at different locations. The tests performed demonstrated how a wide variety of robot and controller designs can seamlessly interoperate, allowing researchers to work together easily and more efficiently. In addition, the demonstration evaluated the feasibility of robotic manipulation from multiple sites, and was conducted to measure time and performance for evaluating laparoscopic surgical skills.New Interoperable Telesurgical ProtocolThe new protocol was cooperatively developed by the University of Washington and SRI International, to standardize the way remotely operated robots are managed over the Internet. "Although many telemanipulation systems have common features, there is currently no accepted protocol for connecting these systems," said SRI's Tom Low. "We hope this new protocol serves as a starting point for the discussion and development of a robust and practical Internet-type standard that supports the interoperability of future robotic systems."The protocol will allow engineers and designers that usually develop technologies independently, to work collaboratively, determine which designs work best, encourage widespread adoption of the new communications protocol, and help robotics research to evolve more rapidly. Early adoption of this protocol internationally will encourage robotic systems to be developed with interoperability in mind, and avoid future incompatibilities."We're very pleased with the success of the event in which almost all of the possible connections between operator stations and remote robots were successful. We were particularly excited that novel elements such as a simulated robot and an exoskeleton controller worked smoothly with the other remote manipulation systems," said Professor Blake Hannaford of the University of Washington.The demonstration included the following organizations:  	For more information regarding availability of the Interoperable Telesurgical Protocol, please visit: http://brl.ee.washington.edu/Research_Active/Interoperability/index.php/Main_PageFor visuals of this demonstration, please visit:  http://www.sri.com/news/	About SRI InternationalSilicon Valley-based SRI International is one of the world's leading independent research and technology development organizations. SRI, which was founded by Stanford University as Stanford Research Institute in 1946 and became independent in 1970, has been meeting the strategic needs of clients and partners for more than 60 years. Perhaps best known for its invention of the computer mouse and interactive computing, SRI has also been responsible for major advances in networking and communications, robotics, drug discovery and development, advanced materials, atmospheric research, education research, economic development, national security, and more. The nonprofit institute performs sponsored research and development for government agencies, businesses, and foundations. SRI also licenses its technologies, forms strategic alliances, and creates spin-off companies. In 2008, SRI's consolidated revenues, including its wholly owned for-profit subsidiary, Sarnoff Corporation, were approximately $490 million.
venere@purdue.eduNew nanolaser key to future optical computers and technologiesBecause the new device, called a "spaser," is the first of its kind to emit visible light, it represents a critical component for possible future technologies based on "nanophotonic" circuitry, said Vladimir Shalaev, the Robert and Anne Burnett Professor of Electrical and Computer Engineering at Purdue University.Such circuits will require a laser-light source, but current lasers can't be made small enough to integrate them into electronic chips. Now researchers have overcome this obstacle, harnessing clouds of electrons called "surface plasmons," instead of the photons that make up light, to create the tiny spasers.Findings are detailed in a paper appearing online Sunday (Aug. 16) in the journal Nature, reporting on work conducted by researchers at Purdue, Norfolk State University and Cornell University.Nanophotonics may usher in a host of radical advances, including powerful "hyperlenses" resulting in sensors and microscopes 10 times more powerful than today's and able to see objects as small as DNA; computers and consumer electronics that use light instead of electronic signals to process information; and more efficient solar collectors."Here, we have demonstrated the feasibility of the most critical component - the nanolaser - essential for nanophotonics to become a practical technology," Shalaev said.The "spaser-based nanolasers" created in the research were spheres 44 nanometers, or billionths of a meter, in diameter - more than 1 million could fit inside a red blood cell. The spheres were fabricated at Cornell, with Norfolk State and Purdue performing the optical characterization needed to determine whether the devices behave as lasers.The findings confirm work by physicists David Bergman at Tel Aviv University and Mark Stockman at Georgia State University, who first proposed the spaser concept in 2003."This work represents an important milestone that may prove to be the start of a revolution in nanophotonics, with applications in imaging and sensing at a scale that is much smaller than the wavelength of visible light," said Timothy D. Sands, the Mary Jo and Robert L. Kirk Director of the Birck Nanotechnology Center in Purdue's Discovery Park.The spasers contain a gold core surrounded by a glasslike shell filled with green dye. When a light was shined on the spheres, plasmons generated by the gold core were amplified by the dye. The plasmons were then converted to photons of visible light, which was emitted as a laser.Spaser stands for surface plasmon amplification by stimulated emission of radiation. To act like lasers, they require a "feedback system" that causes the surface plasmons to oscillate back and forth so that they gain power and can be emitted as light. Conventional lasers are limited in how small they can be made because this feedback component for photons, called an optical resonator, must be at least half the size of the wavelength of laser light.The researchers, however, have overcome this hurdle by using not photons but surface plasmons, which enabled them to create a resonator 44 nanometers in diameter, or less than one-tenth the size of the 530-nanometer wavelength emitted by the spaser."It's fitting that we have realized a breakthrough in laser technology as we are getting ready to celebrate the 50th anniversary of the invention of the laser," Shalaev said.The first working laser was demonstrated in 1960.The research was conducted by Norfolk State researchers Mikhail A. Noginov, Guohua Zhu and Akeisha M. Belgrave; Purdue researchers Reuben M. Bakker, Shalaev and Evgenii E. Narimanov; and Cornell researchers Samantha Stout, Erik Herz, Teeraporn Suteewong and Ulrich B. Wiesner.Future work may involve creating a spaser-based nanolaser that uses an electrical source instead of a light source, which would make them more practical for computer and electronics applications.The work was funded by the National Science Foundation and U.S. Army Research Office and is affiliated with the Birck Nanotechnology Center, the Center for Materials Research at Norfolk State, and Cornell's Materials Science and Engineering Department.Writer: Emil Venere, (765) 494-4709, venere@purdue.eduSource: Vladimir Shalaev, (765) 494-9855, shalaev@ecn.purdue.eduPurdue News Service: (765) 494-2096; purduenews@purdue.edu IMAGE CAPTION:Researchers have created the tiniest laser since its invention nearly 50 years ago. Because the new device, called a "spaser," is the first of its kind to emit visible light, it represents a critical component for possible future technologies based on "nanophotonic" circuitry. The color diagram (a) shows the nanolaser's design: a gold core surrounded by a glasslike shell filled with green dye. Scanning electron microscope images (b and c) show that the gold core and the thickness of the silica shell were about 14 nanometers and 15 nanometers, respectively. A simulation of the SPASER (d) shows the device emitting visible light with a wavelength of 525 nanometers. (Birck Nanotechnology Center, Purdue University)A publication-quality image is available at http://news.uns.purdue.edu/images/+2009/shalaev-spasers.jpg Abstract on the research in this release is available at: http://news.uns.purdue.edu/x/2009b/090817ShalaevSpasers.htmlSTORY AND PHOTO CAN BE FOUND AT:http://news.uns.purdue.edu/x/2009b/090817ShalaevSpasers.htmlNote to Journalists: Journalists may obtain a copy of the research paper by contacting Nature at press@nature.com  or calling (212) 726-9231.Purdue UniversityNews Service400 Centennial Mall Drive, Rm. 324West Lafayette, IN  47907-2016Voice:  765-494-2096FAX:  765-494-0401
carol.hughes@asu.edu'Breeding Bio Insecurity' argues for change in biodefense policyTEMPE, Ariz.  Biological warfare has shaped human conflict throughout history. But the deadly anthrax-letter mailings following the Sept. 11, 2001, terrorist attacks presented Americans with a threat new and terrifying. What if the resources spent to safeguard American citizens against terrorism have only made them more vulnerable?With their new book, "Breeding Bio Insecurity: How U.S. Biodefense is Exporting Fear, Globalizing Risk, and Making Us All Less Secure," Edward Sylvester, an Arizona State University professor, and Lynn Klotz, a senior science fellow at the Center for Arms Control and Non-Proliferation, investigate the implications of costly, complex and secretive U.S. biodefense policy.The book, released in October, offers readers facts and figures regarding the U.S. government's biodefense policy, and compels policymakers to justify spending and actions. The authors argue that the greatest external threat facing the U.S. comes from rogue nations conducting secret research rather than hypothetical scenarios in which people with basic skills weaponize deadly biomaterials."We have an urgent message that everyone needs to hear," says Sylvester, who teaches science writing, news writing, reporting and editing courses at ASU's Walter Cronkite School of Journalism and Mass Communication.Sylvester, who previously co-authored "The Gene Age" with Klotz, says: "'Bio Insecurity' started out as a series of conversations with Lynn, a scientist who is an expert in the field of biological security issues and one of my oldest friends. We became increasingly concerned that the government was taking the wrong direction in preparing against possible bioterrorism attacks in the years after Sept. 11."Terrorists are best known for stealing what they use to kill, everything from rifles to jetliners," says Sylvester. "The only realistic way for terrorists to get their hands on highly developed pathogen stocks to make such weapons is by stealing them, and the government was making that more likely by funding research into those pathogens at a rapidly increasing number of places around the country."The anthrax used against American citizens in the attacks after Sept. 11 was almost certainly stolen from Fort Detrick in Maryland, he notes."It was the extremely lethal Ames strain, cultured by well-trained scientists. It couldn't have been grown from a soil sample in someone's basement lab or a cave somewhere," Sylvester says.The number of high bio-security labs in the country has tripled in the last several years. The expansion of the biodefense program after Sept. 11 and its clandestine nature make the centers more susceptible to lethal accidents or theft. The book asserts that the only way to truly defend the country from bioterrorism is through multilateral activities, such as treaties, and international cooperation on defenses against all diseases.The future potential for biowarfare in the absence of such concerted efforts is truly ominous, Sylvester says. "When you realize the propensity of countries to bring whatever is most powerful into warfare and you combine that with the stunning possibilities for manipulating the living world, you enter a whole new world of dark possibilities," he says.Sylvester also is the author of three books on medical research: "Target: Cancer," "The Healing Blade: A Tale of Neurosurgery," and "Back From The Brink: How Crises Spur Doctors To New Discoveries About the Brain."ARIZONA STATE UNIVERSITY  (www.asu.edu)Walter Cronkite School of Journalism and Mass Communication  (http://cronkite.asu.edu)Tempe, Arizona USA
press_releases@the-jci.orgTherapeutic delivery of a gene to dysfunctional nervesMedical conditions that affect sensory nerves outside the brain and spinal cord are known as sensory neuronopathies. These conditions, which are extremely painful, include shingles and can be caused by anticancer drugs such as cisplatin. In many sensory neuronopathies, the nerves that are dysfunctional are those in a region of the body known as the dorsal root ganglion (DRG), and these conditions are particularly difficult to treat. However, Lawrence Chan and colleagues, at Baylor College of Medicine, Houston, have developed an approach to target therapeutic genes to nerves in the DRG, and used it to reduce sensory nerve dysfunction in a mouse model of Sandhoff disease, an inherited condition in which many nerves, including those in the DRG, are affected.The authors developed a system to generate helper-dependent adenoviruses that targeted only DRG nerves. These were used to deliver genes to DRG nerves in mice and found to be dramatically more efficient at gene delivery than nontargeted helper-dependent adenoviruses. In mice lacking the Hexb gene, which are consider a mouse model of Sandhoff disease, administration of DRG-targeted helper-dependent adenoviruses carrying the Hexb gene restored Hexb expression in DRG nerves and eliminated sensory nerve dysfunction. The authors hope this approach could be developed for treating different forms of DRG sensory neuronopathies.	TITLE: DRG-targeted helper-dependent adenoviruses mediate selective gene delivery for therapeutic rescue of sensory neuronopathies in miceAUTHOR CONTACT:Lawrence ChanBaylor College of Medicine, Houston, Texas, USA.Phone: (713) 798-4478; Fax: (713) 798-8764; E-mail: lchan@bcm.tmc.edu.View the PDF of this article at: https://www.the-jci.org/article.php?id=39038
aem1@psu.eduStimulus funds to further cyber security researchSafeguarding business applications and infrastructure from cyber threats is the aim of "Collaborative Research: Towards Self-Protecting Data Centers: a Systematic Approach," a project recently funded by the American Recovery and Reinvestment Act of 2009.	Peng Liu, associate professor of information sciences and technology, was one of three researchers who received more than $1 million thanks to the stimulus bill passed by the U.S. Congress earlier this year. The award was made to Liu: Sushil Jajodia; George Mason University and Meng Yu, Western Illinois University. The award totals more than $1 million for three years. Penn State's portion is $500,000.	"This grant will enable us to take a big stride forward towards building self-protecting and trustworthy information systems and data sets," Liu said. "This project will 'stand on the shoulders' of our recent research achievements in trusted recovery, self-healing information systems and intrusion-tolerant computing."	The research team aims to improve security consolidation to meet the top two requirements for modern data centers -- business continuity and information security. They will take a systematic approach that leverages the emerging virtual machine technologies to consolidate four areas of systems security research: microscopic intrusion analysis and detection, redundancy, automatic response and diversity-driven protection.	"We're very grateful to have this opportunity to address the important security consolidation problem faced by today's data centers," said Liu.	Broader impacts for this research include a significant advancement in reducing risk among business applications and information systems, increasing business continuity and delivering data assurance in the presence of severe cyber attacks.	Earlier this summer, Liu and his team received a $6.25 million Multidisciplinary University Research Initiative (MURI) award for his project, "Computer-Aided Human Centric Cyber Situation Awareness."	Liu will lead this project; which will further their previous research on cyber-awareness and how it can be used to improve cyber defense.
Holger.Vogt@iis.fraunhofer.deColor sensors for better visionThis release is available in German.The car of the future will have lots of smart assistants onboard  helping to park the car, recognize traffic signs and to warn the driver of blind spot hazards. Many driver assistance systems incorporate high-tech cameras which have to meet a wide range of requirements. They must be able to withstand high ambient temperatures and be particularly small, light and robust. What's more, they have to reliably capture all the required images and should cost as little as possible. Nowadays CMOS sensors are used for most in-car systems. These semiconductor chips convert light signals into electrical pulses and are installed in most digital cameras. At present, however, the sensors used for industrial and other special cameras are mostly color blind.Now researchers at the Fraunhofer Institute for Microelectronic Circuits and Systems IMS in Duisburg are adding some color to the picture. They have developed a new process for producing CMOS image sensors which enables the chips to see color. Normally the image sensors are produced on silicon wafers using a semiconductor technique, the CMOS process. "We have integrated a color filter system in the process," explains Prof. Dr. Holger Vogt, Deputy Director of the IMS. "In the same way as the human eye needs color-specific cone types, color filters have to be inserted in front of the sensors so that they can distinguish color." This job is handled by polymers dyed in the primary colors red, green and blue. Each pixel on the sensor is coated with one of the three colors by a machine which coats the sensor disk propels with a micrometer-thick polymer layer. Using UV light and a mask which is only transparent on the desired pixels, the dye is fixed at the requisite points and the rest is then washed off. In addition, the researchers have developed special microlenses which help the sensor to capture and measure the light more efficiently. With the aid of a transparent polyimide they create a separate lens for each individual pixel, which almost doubles the light-sensitivity of the image sensor.The optimized CMOS process not only makes it possible to cost-efficiently improve the performance of driver assistance systems. Endoscopes can also benefit from the new properties of CMOS image sensors. The researchers are presenting the CMOS process at the Vision trade fair from November 3 to 5 in Stuttgart (Hall 6, Stand 6D12).
mariangela.dacunto@esa.intTackling new Arctic challenges from spaceInternational scientists, researchers and decision makers met at the 'Space and the Arctic workshop' to identify the needs and challenges of working and living in the rapidly changing Arctic and to explore how space-based services can help to meet those needs.The workshop, held from 20 to 21 October in Stockholm, Sweden, was organised by the Swedish National Space Board and the Swedish Meteorological and Hydrological Institute together with ESA, EUMETSAT and the EC. The warmer climate, advances in technology and demand for natural resources are leading to increased human activity in the Arctic. This increase in activity, especially related to oil and gas production, changing fishery patterns and new shipping routes, provides new opportunities but also creates new risks to those working and living in the area and to the pristine and unique natural environment.  One of the highlights of the workshop was the 'Arctic Marine Transport and Space' presentation given by Dr Lawson Brigham of the University of Alaska Fairbanks that outlined the Arctic Marine Shipping Assessment (AMSA) report for 2009. The AMSA report, prepared by the Protection of the Arctic Marine Environment (PAME) Working Group on behalf of the Arctic Council, is designed to educate and inform people about the current state of Arctic marine use and future challenges. It focuses primarily on Arctic marine safety and marine environmental protection. "New space assets are crucial for improving marine communications in many regions of the Arctic Ocean in order to improve search and rescue and environmental response activities," Brigham said. "One key AMSA recommendation is the need for a comprehensive Arctic marine traffic awareness system; only space assets in the long-term can provide the coverage necessary to achieve effective monitoring and tracking of Arctic ships.""Improved space sensors measuring sea-ice thickness, mapping snow cover and tracking icebergs will be increasingly important to Arctic ship safety and route optimisation," he continued. "Continued satellite monitoring is also central to recording the retreat of sea ice and other changes to the cryosphere in a warming Arctic." In order to build the infrastructure needed in the Arctic to meet these challenges, workshop participants investigated ways space infrastructure could facilitate communication, environmental monitoring, early warning systems and navigation and vessel tracking in the area. The workshop was held under the auspices of the Swedish Presidency of the Council of the EU as part of a commitment to face the challenges of climate change and increased human activity. It focused on these main themes: climate change & environment; transport safety and security; and sustainable exploitation.The contributions from ongoing activities in European projects such as MyOcean, Polar View and Damocles, were presented to show how lessons learned in setting up operational services for the Baltic area could be applied in a new setting with similar information requirements for the Arctic. After presenting their experiences in the region, participants provided suggestions as to how operational space-based services could monitor and help adapt to climate change and ecosystem management, maintain safe transportation and ensure sustainable development in the vulnerable Arctic. At the end of the workshop, participants agreed a set of conclusions and recommendations as to how space technology could help Europe meet its objectives in the Arctic. ESA's ice mission, CryoSat, is scheduled to launch in February next year. CryoSat will monitor precise changes in the thickness of polar ice sheets and floating sea ice. The observations made over its three-year lifetime will help explain the connection between the melting of the polar ice and the rise in sea levels and how this is contributing to climate change.	The workshop conclusions are available at: http://esamultimedia.esa.int/docs/EarthObservation/Statement_final.pdf
hickeyh@uw.eduHousehold robots do not protect users' security and privacy, researchers sayPeople are increasingly using household robots for chores, communication, entertainment and companionship. But safety and privacy risks of information-gathering objects that move around our homes are not yet adequately addressed, according to a new University of Washington study.	It's not a question of evil robots, but of robots that can be misused."A lot of attention has been paid to robots becoming more intelligent and turning evil," said  co-author Tadayoshi Kohno, a UW assistant professor of computer science and engineering. "But there is a much greater and more near-term risk, and that's bad people who can use robots to do bad things."He and colleagues discovered security weaknesses in three robots currently on the market. They presented the findings last week at the International Conference on Ubiquitous Computing in Orlando, Fla. Co-authors are UW graduate students Tamara Denning, Cynthia Matuszek and Karl Koscher and UW affiliate faculty member Joshua Smith. During the past year the team ran tests to evaluate the security of three consumer-level robots: the WowWee Rovio, a wireless, buglike rolling robot marketed to adults as a home surveillance tool that can be controlled over the Internet and includes a video camera, microphone and speaker; the Erector Spykee, a toy wireless Web-controlled "spy" robot that has a video camera, microphone and speaker; and the WowWee RoboSapien V2, a more dexterous toy robot controlled over short distances using an infrared remote control. The concerns the researchers uncovered with the wireless robots include the fact that:The authors also identified scenarios in which a robot might physically harm its owner or the home environment. While the risks today are relatively small, researchers say they believe the risks will become more serious as robots become more widespread. "These are technologies that are being used in the home," noted Denning, a UW doctoral student in computer science and engineering. "The attacks here are very simple. But the consequences can be quite serious."   "In the future people may have multiple robots in the home that are much more capable and sophisticated," Denning added. "Security and privacy risks with future household robots will likely be more severe, which is why we need to start addressing robot security and privacy today."The robots the researchers studied were purchased in or before October 2008.  The researchers said they believe other robots now on the market offer a similar level of security. Owners of household robots can do some simple things to significantly increase their security, researchers said, such as turning on encryption for a home wireless network, and disabling Internet access to the robot's controls.Education is key, says co-author Matuszek, a UW doctoral student in computer science and engineering."Before they go and buy something people will typically go online and do some research," she said. "People know to look for small parts in children's toys, or look for lead paint. For products that combine more advanced technology and wireless capabilities, people should look at whether it protects privacy and security."  Researchers said they hope privacy will someday be on buyers' minds when they look at products, and that in the future electronic privacy and security could be included as a category in Consumer Reports and other product reviews. The work was funded by the National Science Foundation and an Alfred P. Sloan Foundation research fellowship.	For more information, contact Kohno at yoshi@cs.washington.edu or 206-685-4853 and Denning at tdenning@cs.washington.edu or 206-569-5992.More information on the project is at https://www.cs.washington.edu/research/security/robots/.
stefany@cc.gatech.eduGeorgia Tech launches experimental Green IT InitiativeRecycled HPC system to be used to develop sustainable power consumptionThe biggest challenge in computing today, some experts say, is not processing power, but power consumption. In 2007, the Environmental Protection Agency forecasted that as of 2011, data centers will be responsible for 2 percent of all power consumption in the U.S., and some predictions foresee those levels rising to almost 6 percent by 2020. Finally, there are numerous anecdotes about power demands caused by data centers, including partial brownouts when supercomputers are switched on and new data centers having to be moved to where cheap hydro-power is available, such as the Columbia River Gorge.Clearly, power consumption is not only an environmental concern, but also a productivity and security issue. If high-performance computing (HPC) centers are going to be able to run larger simulations and process more and more data, they must find a way to decrease their facilities' drain on the power grid. To help understand and reduce power consumption, the Georgia Institute of Technology has launched Green IT. The effort considers power consumption across the entire "energy stack," ranging from the power consumed by modern multi-core platforms, to the board and rack levels, to the entire data center. Corralling expertise from Georgia Tech's College of Computing, College of Engineering and Office of Information Technology, the consortium is a multidisciplinary effort that looks at how to build large-scale systems that use less power. The goal is to better understand where and how power is used, and to make it possible to coordinate power usage across different data center components, such as the cooling and the IT infrastructure."With experts from computer science looking at systems management, cloud computing and virtualization, and electrical engineers investigating chip design along with mechanical engineers working on cooling technologies, Georgia Tech is in a great position to help solve the power consumption problem," said Karsten Schwan, a professor in Georgia Tech's College of Computing. Often, research efforts like these must use simulated machines, with heaters substituting for computers; but the Green IT group will be using a large-scale commodity system, a 1,000-node IBM BladeCenter, to conduct its investigations.  The system was previously used by the Center for the Study of Systems Biology."Rather than junking the old machine, Georgia Tech decided that we could recycle it and use it for energy-efficient IT research along with a host of other uses," said Schwan.The GreenIT effort is led by Sudhakar Yalamanchili in Electrical and Computer Engineering and includes the following faculty members: Ada Gavrilovska, Ron Hutchins, Yogendra Joshi,  Hyesoon Kim,  Hsien-Hsin Lee,  Saibal Mukhopadhyay,  Santosh Pande, Calton Pu, Karsten Schwan,  Madhavan Swaminathan, Yorai Wardi, Marilyn Wolf and Jun Xu.This week, Georgia Tech is showcasing research activities in high-performance computing and the computational sciences at SC09. The conference takes place at the Oregon Convention Center in Portland, Oregon, Nov. 14-20. Researchers and staff will be on hand at Booth 132 to demonstrate and discuss Georgia Tech's latest research and activities in the field.
pberger@ieee.orgQuantum computer chips now 1 step closer to realityCOLUMBUS, Ohio -- In the quest for smaller, faster computer chips, researchers are increasingly turning to quantum mechanics -- the exotic physics of the small.The problem: the manufacturing techniques required to make quantum devices have been equally exotic.That is, until now.Researchers at Ohio State University have discovered a way to make quantum devices using technology common to the chip-making industry today.This work might one day enable faster, low-power computer chips. It could also lead to high-resolution cameras for security and public safety, and cameras that provide clear vision through bad weather.Paul Berger, professor of electrical and computer engineering and professor of physics at Ohio State University, and his colleagues report their findings in an upcoming issue of IEEE Electron Device Letters.The team fabricated a device called a tunneling diode using the most common chip-making technique, called chemical vapor deposition."We wanted to do this using only the tools found in the typical chip-makers toolbox," Berger said. "Here we have a technique that manufacturers could potentially use to fabricate quantum devices directly on a silicon chip, side-by-side with their regular circuits and switches."The quantum device in question is a resonant interband tunneling diode (RITD) -- a device that enables large amounts of current to be regulated through a circuit, but at very low voltages. That means that such devices run on very little power.RITDs have been difficult to manufacture because they contain dopants -- chemical elements -- that don't easily fit within a silicon crystal.Atoms of the RITD dopants antimony or phosphorus, for example, are large compared to atoms of silicon. Because they don't fit into the natural openings inside a silicon crystal, the dopants tend to collect on the surface of a chip."It's like when you're playing Tetris and you have a big block raining down, and only a small square to fit it in. The block has to sit on top," Berger said. "When you're building up layers of silicon, these dopants don't readily fit in. Eventually, they clump together on top of the chip."In the past, researchers have tried adding the dopants while growing the silicon wafer one crystal layer at a time -- using a slow and expensive process called molecular beam epitaxy, a method which is challenging for high-volume manufacturing. That process also creates too many defects within the silicon.Berger discovered that RITD dopants could be added during chemical vapor deposition, in which a gas carries the chemical elements to the surface of a wafer many layers at a time. The key was determining the right reactor conditions to deliver the dopants to the silicon, he found."One key is hydrogen," he said. "It binds to the silicon surface and keeps the dopants from clumping. So you don't have to grow chips at 320 degrees Celsius [approximately 600 degrees Fahrenheit] like you do when using molecular beam epitaxy. You can actually grow them at a higher temperature like 600 degrees Celsius [more than 1100 degrees Fahrenheit] at a lower cost, and with fewer crystal defects."Tunneling diodes are so named because they exploit a quantum mechanical effect known as tunneling, which lets electrons pass through thin barriers unhindered. In theory, interband tunneling diodes could form very dense, very efficient micro-circuits in computer chips. A large amount of data could be stored in a small area on a chip with very little energy required.Researchers judge the usefulness of tunneling diodes by the abrupt change in the current densities they carry, a characteristic known as "peak-to-valley ratio." Different ratios are appropriate for different kinds of devices. Logic circuits such as those on a computer chip are best suited by a ratio of about 2.The RITDs that Berger's team fabricated had a ratio of 1.85."We're close, and I'm sure we can do better," he said.He envisions his RITDs being used for ultra-low-power computer chips operating with small voltages and producing less wasted heat."Chip makers today are having a great difficulty boosting performance in each generation, so they pack chips with more and more circuitry, and end up generating a lot of heat," Berger said. "That's why a laptop computer is often too hot to actually sit atop your lap. Soon, their heat output will rival that of a nuclear reactor per unit volume.""That's why moving to quantum devices will be a game-changer."RITDs could form high-resolution detectors for imaging devices called focal plane arrays. These arrays operate at wavelengths beyond the human eye and can permit detection of concealed weapons and improvised explosive devices. They can also provide vision through rain, snow, fog, and even mild dust storms, for improved airplane and automobile safety, Berger said. Medical imaging of cancerous tumors is another potential application.His coauthors on the paper included Si-Young Park, and R. Anisha, both doctoral students in electrical engineering at Ohio State; and Roger Loo, Ngoc Duy Nguyen, Shotaro Takeuchi, and Matty Caymax, all of IMEC, an industrial research center in Belgium.This work was partially supported by the National Science Foundation.Contact: Paul R. Berger, (614) 247-6235; pberger@ieee.orgWritten by Pam Frost Gorder, (614) 292-9475; Gorder.1@osu.edu
bisanski@psychologicalscience.orgHush little baby ... linking genes, brain and behavior in childrenIt comes as no surprise that some babies are more difficult to soothe than others but frustrated parents may be relieved to know that this is not necessarily an indication of their parenting skills. According to a new report in Psychological Science, a journal of the Association for Psychological Science, children's temperament may be due in part to a combination of a certain gene and a specific pattern of brain activity.The pattern of brain activity in the frontal cortex of the brain has been associated with various types of temperament in children. For example, infants who have more activity in the left frontal cortex are characterized as temperamentally "easy" and are easily calmed down. Conversely, infants with greater activity in the right half of the frontal cortex are temperamentally "negative" and are easily distressed and more difficult to soothe.In this study, Louis Schmidt from McMaster University and his colleagues investigated the interaction between brain activity and the DRD4 gene to see if it predicted children's temperament. In a number of previous studies, the longer version (or allele) of this gene had been linked to increased sensory responsiveness, risk-seeking behavior, and attention problems in children. In the present study, brain activity was measured in 9-month-old infants via electroencephalography (EEG) recordings. When the children were 48 months old, their mothers completed questionnaires regarding their behavior and DNA samples were taken from the children for analysis of the DRD4 gene.The results reveal interesting relations among brain activity, behavior, and the DRD4 gene. Among children who exhibited more activity in the left frontal cortex at 9 months, those who had the long version of the DRD4 gene were more soothable at 48 months than those who possessed the shorter version of the gene. However, the children with the long version of the DRD4 gene who had more activity in the right frontal cortex were the least soothable and exhibited more attention problems compared to the other children.These findings indicate that the long version of the DRD4 gene may act as a moderator of children's temperament. The authors note that the "results suggest that it is possible that the DRD4 long allele plays different roles (for better and for worse) in child temperament" depending on internal conditions (the environment inside their bodies) and conclude that the pattern of brain activity (that is, greater activation in left or right frontal cortex) may influence whether this gene is a protective factor or a risk factor for soothability and attention problems. The authors cautioned that there are likely other factors that interact with these two measures in predicting children's temperament.For more information about this study, please contact: Louis A. Schmidt (schmidtl@mcmaster.ca)Psychological Science is ranked among the top 10 general psychology journals for impact by the Institute for Scientific Information. For a copy of the article "Linking Gene, Brain, and Behavior: DRD4, Frontal Asymmetry, and Temperament" and access to other Psychological Science research findings, please contact Barbara Isanski at 202-293-9300 or bisanski@psychologicalscience.org 
dbkane@ucsd.eduCalifornians -- and their cell phones -- will help computer scientists monitor air pollutionUniversity of California, San Diego computer scientists are creating a network of environmental sensors that will help you avoid air pollution hot spots in everyday lifeYou want to go for a run, but you don't want to run in polluted air that might aggravate your asthma. University of California, San Diego computer scientists are creating a network of environmental sensors that will help you avoid air pollution hot spots that exist exactly when you are planning your route. The system will provide up-to-the-minute information on outdoor and indoor air quality, based on environmental information collected by hundreds, and eventually thousands, of sensors attached to the backpacks, purses, jackets and board shorts of San Diegans going about daily life. This is "CitiSense"the vision of computer scientists from the UC San Diego Jacobs School of Engineering. The interdisciplinary team recently won a $1.5 million grant from the National Science Foundation (NSF) to solve the many technical challenges that stand in the way of applications that merge the cyber and physical worlds. "San Diego County has 3.1 million residents, 4,000 square miles, and only five official EPA air quality monitors. We know about the air quality in those exact spots but we know much less about the air quality in other places. Our goal is to give San Diegans up-to-the-minute environmental information about where they live, work and playinformation that will empower anyone in the community to make healthier choices," said William Griswold, the principal investigator on the grant and a professor in the Department of Computer Science and Engineering (CSE) at the UC San Diego Jacobs School of Engineering.The goal of CitiSense is to build and deploy a wireless network in which hundreds or thousands of small environmental sensors carried by the public rely on cell phones to shuttle information to central computers where it will be analyzed, anonymized and reflected back out to individuals, public health agencies and San Diego at large. At the same time, the sensor-wearing public will have the option to also wear biological monitors that collect basic health information, such as heart rate. This combination of sensors will enable the team's medical team to run exacting health science research projects, such as investigating how particular environmental pollutants affect human health. Dr. Kevin Patrick from UC San Diego's California Institute for Telecommunications and Information Technology (Calit2) and the UCSD School of Medicine will lead the medical efforts.Building a large-scale system that integrates sensors and other digital technologies into the physical world will require advances in a number of computer science areas including power management, privacy, security, artificial intelligence and software architecture. "It is a tremendous challenge to integrate a number of technologies and then deploy them outsidein the wild," said Griswold.Mobile phones and other handheld devices, for example, are traditionally designed to serve one personthe user. Including these electronics in advanced computing systems that have other priorities will require new power and workload management strategies. Computer science professor Tajana Simunic Rosing and her graduate students are developing systems to ensure that the phones and other mobile devices serving as stepping stones between environmental sensors and the centralized computing infrastructure will not drop calls or suffer other hits to performance. Rosing's team is also investigating how sensors fixed in the environmentrather than carried around by the general publicmight be powered by solar, wind, or vibrational energy instead of batteries. In addition, the computer scientists are considering how these fixed sensors might rely on nearby handheld devices to send environmental information to central computers. Capturing high quality data from sensors in uncontrolled environments is another challenge the computer scientists face. "Sensors will differ. Sensors will fail. People will breathe on them. And so there is the question of how you get good data in these conditions. We have to find a way to process the data to remove the noise," said Griswold, who noted that computer science professor Sanjoy Dasgupta and a team of student researchers is using statistical artificial intelligence (AI) to do just this. Computer science professor Hovav Shacham will lead a team focused on security and privacy issues, which are particularly challenging given the limited computational power of sensors and other embedded devices. Software architecture and cyberinfrastructure are additional areas in which breakthroughs will be needed. The computer scientists are developing new approaches to writing code for software systems that are open and flexible yet private and secure.For example, if someone develops a new application for monitoring carbon dioxide, the computer scientists want to be able to drop the application into the system and have it not only workbut interact with existing systems in terms of data, power management and workflow. Computer science professors Ingolf Krueger and William Griswold are leading these efforts. In part, they are building on Krueger's previous work in service-oriented architecture, which can keep various componentslike machine learning, power management and security codemuch more separate than in traditional software systems, where functional elements are often so woven into the source code that it is difficult to quickly update any one aspect of the software."We are addressing major problems of the day of tremendous social, environmental and economic importance. When you attach the science and engineering to the problems of the day, it drives the research in a very exciting way," said Griswold.The National Science Foundation grant is entitled: CitiSense - Adaptive Services for Community-Driven Behavioral and Environmental Monitoring to Induce Change.The researchers are grateful to Qualcomm, Inc (http://www.qualcomm.com/) which is donating funds for the cell phones needed for the project and Seacoast Science, Inc. (http://www.seacoastscience.com/), which is providing expertise on chemical sensors.Principal Investigator: William Griswold, a professor in the Department of Computer Science and Engineering at the UC San Diego Jacobs School of Engineering.The Co-Principal Investigators: Ingolf Krueger, Sanjoy Dasgupta, Tajana Simunic Rosing, and Hovav Shacham are professors in the Department of Computer Science and Engineering at the UC San Diego Jacobs School of Engineering; Kevin Patrick is a UCSD School of Medicine/Calit2 professor.
evelyn.brown@nist.govWho are you? Mobile ID devices find out using NIST guidelinesA new publication that recommends best practices for the next generation of portable biometric acquisition devicesMobile IDhas been published by the National Institute of Standards and Technology (NIST).Devices that gather, process and transmit an individuals biometric datafingerprints, facial and iris imagesfor identification are proliferating. Previous work on standards for these biometric devices has focused primarily on getting different stationary and desktop systems with hardwired processing pathways to work together in an interoperable manner. But a new generation of small, portable and versatile biometric devices are raising new issues for interoperability.The proliferation of smaller devices including advanced personal digital assistants (PDAs), ultra-portable personal computers and high-speed cellular networks has made portable biometric systems a reality, computer scientist Shahram Orandi says. While the portable systems have made leaps and bounds in terms of capability, there are still intrinsic limitations that must be factored into the big picture to ensure interoperability with the larger, more established environments such as desktop or large server-based systems.The new mobile biometric devices allow first responders, police, the military and criminal justice organizations to collect biometric data with a handheld device on a street corner or in a remote area and then wirelessly send it to be compared to other samples on watch lists and databases in near real-time. Identities can be determined quickly without having to take a subject to a central facility to collect his or her biometrics, which is not always possible.Soldiers are beginning to use these devices to control access to secured areas, and first responders can use them to ensure that only approved workers are on-site during an incident or investigation.Special Publication 500-280: Mobile ID Device Best Practice Recommendation Version 1 offers guidelines to help ensure that, if followed, mobile and stationary systems will work together. It was developed by NIST researchers working with first responders, criminal justice agencies, the military, industry and academia.For example, most current law enforcement applications require capturing all 10 fingerprints from an individual. Desktop fingerprint scanners provide a large scanning areaa platenthat can capture all 10 fingers in a fast, three-step process. Most portable devices, however, have platens that are a fraction of the size of a desktop scanner. The Mobile ID best practices publication provides guidelines that allow for the capture of all 10 fingerprints on a scanner with a smaller platen using a two-fingers-at-a-time approach.	The publication is available at http://fingerprint.nist.gov/mobileid/MobileID-BPRS-20090825-V100.pdf.
caroline_barnhill@ncsu.eduSTEM gets greener: Promoting critical thinking using renewable energy technologyCan building model cars really help create the next generation of electric vehicle designers and engineers? Researchers at North Carolina State University think so. Through a recent grant, they will develop a curriculum that uses real-world applications of renewable energy technologies to teach science, technology, engineering and mathematics  known as STEM concepts.The project, Green Research for Incorporating Data in the Classroom (GRIDc), uses renewable energy technologies as a learning tool to foster cognitive skill development and promote critical thinking and problem solving. This not only teaches new "green" sciences, but allows students to make practical informed decisions  such as comparing energy sources in electric vehicles."Students have a general knowledge of concepts such as wind and solar power, but they generally do not understand the pros and cons of different types of  renewable energy systems," says Dr. Bill DeLuca, associate professor of technology education at NC State and principal investigator on the project. "Through this research, we are able to provide them data so they can understand and decide for themselves how well these systems work  and in what sorts of applications."The $400,000 National Science Foundation (NSF) grant builds upon a previous $200,000 NSF grant that collected data from renewable energy technologies at the N.C. Solar Center to teach undergraduate courses. As a small part of the initial grant, the team held workshops for middle and high school students. The immense success of the workshops led to the development of STEP (Sustainable Transportation Electrification Program), a program partially funded by GRIDc, which  includes two pilot electric vehicle competitions. Ten N.C. middle schools will hold contests to develop remote-controlled, battery-operated cars. Six N.C. high schools will also build the model cars, in addition to solar charging stations to operate the model vehicles."We know there is a big misconception among people about electric vehicles  like the differences between hybrid and electric vehicles. And, with President Obama's announcement of $2.4 billion for the electrification of transportation, we need a well-trained workforce and informed consumers  it's a paradigm shift for American society," says Dr. Pam Carpenter, a project coordinator at the N.C. Solar Center and co-principal investigator. "What we've found is that these students are actually able to go home and educate their parents about renewable energy."Carpenter adds that GRIDc was one of 17 projects recently featured at the NSF technology showcase on Capitol Hill. Working with DeLuca and Carpenter is a team of researchers at NC State including Dr. Len Annetta, associate professor of science education, Dr. Aaron Clark, associate professor of graphic communications, and Dr. Joseph DeCarolis, assistant professor of environmental engineering. The initial grant funded the development of a monitoring system that provides students with energy and power readings from  multiple renewable energy technologies at the Solar Center. These technologies include photovoltaics, wind turbines, solar thermal, and hydrogen fuel cell systems, which can be referenced against meteorological data such as wind speed, the sun's irradiance, ambient temperature, and module temperature. All of these variables can affect the performance of the photovoltaic system. The second phase of the grant, which launched in September, will expand data collection by adding sensors to wind turbines in North Carolina's Outer Banks. Additional data on  plug-in hybrid electric vehicles will come from Progress Energy and Advanced Energy, as well as from charging stations for electric vehicles."We're teaching students about renewable energy, but we're doing it in a way that complements the material they are already learning in the classroom  such as how to make graphs, look at trend analyses or work with different data types," DeLuca says. "So part of our job is working with the teachers to help them incorporate this information into their curriculum."
verena.kraeusel@iwu.fraunhofer.deElectromagnetic fields as cutting toolsThis release is available in German.Squealing tires and the crunch of impact  when an accident occurs, the steel sheets that form a motor vehicle's bodywork must provide adequate impact protection and shield its passengers to the greatest extent possible. But the strength of the steels that are used throw up their own challenges, for example when automobile manufacturers have to punch holes in them for cable routing. Struggling to pierce the hard steel, mechanical cutting tools rapidly wear out. And because they also leave some unwanted material on the underside of the steel (burr, as the experts call it), additional time has to be spent on a finishing process. One possible alternative is to use lasers as cutters, but they require a great deal of energy, which makes the entire process time-consuming and costly.Working together with a number of partners including Volkswagen, researchers at the Fraunhofer Institute for Machine Tools and Forming Technology IWU in Chemnitz have come up with another way to make holes in press-hardened steel bodywork. Dr. Verena Kräusel, head of department at the IWU, explains: "The new method is based on electromagnetic pulse technology (EMPT), which was previously used primarily to expand or neck aluminum tubes. We've modified it to cut even hard steels. Whereas a laser takes around 1.4 seconds to cut a hole, EMPT can do the job in approximately 200 milliseconds  our method is up to seven times faster." Another advantage is that it produces no burr, thus doing away with the need for a finishing process. Stamping presses become superfluous, and no costs arise from the need to replace worn-out parts.The pulse generators comprise a coil, a capacitor battery, a charging device and high-current switches. When the switch closes, the capacitors discharge via the coil within a matter of microseconds, producing a high pulsed current. The coil converts the energy stored in the capacitors into magnetic energy. To be able to use this process to cut steel, the researchers simply had to modify the coil to ensure the resulting electromagnetic field is strong enough: the pressure with which the field hits the steel must be so high that it forcibly expels the material from the sheet. "The impact pressure on the steel is approximately 3,500 bar, which equates to the weight of three small cars on a single fingernail," says Kräusel. PSTproducts GmbH in Alzenau provided the original EMPT system. With regard to the customer demands the researchers develop now the coils for various cutting geometries.
matt_shipman@ncsu.eduUnderstanding mechanical properties of silicon nanowires paves way for nanodevicesSilicon nanowires are attracting significant attention from the electronics industry due to the drive for ever-smaller electronic devices, from cell phones to computers. The operation of these future devices, and a wide array of additional applications, will depend on the mechanical properties of these nanowires. New research from North Carolina State University shows that silicon nanowires are far more resilient than their larger counterparts, a finding that could pave the way for smaller, sturdier nanoelectronics, nanosensors, light-emitting diodes and other applications.It is no surprise that the mechanical properties of silicon nanowires are different from "bulk"  or regular size  silicon materials, because as the diameter of the wires decrease, there is an increasing surface-to-volume ratio. Unfortunately, experimental results reported in the literature on the properties of silicon nanowires have reported conflicting results. So the NC State researchers set out to quantify the elastic and fracture properties of the material."The mainstream semiconductor industry is built on silicon," says Dr. Yong Zhu, assistant professor of mechanical engineering at NC State and lead researcher on this project. "These wires are the building blocks for future nanoelectronics." For this study, researchers set out to determine how much abuse these silicon nanowires can take. How do they deform  meaning how much can you stretch or warp the material before it breaks? And how much force can they withstand before they fracture or crack? The researchers focused on nanowires made using the vapor-liquid-solid synthesis process, which is a common way of producing silicon nanowires.Zhu and his team measured the nanowire properties using in-situ tensile testing inside scanning electron microscopy. A nanomanipulator was used as the actuator and a micro cantilever used as the load sensor. "Our experimental method is direct but simple," says Qingquan Qin, a Ph.D. student at NC State and co-author of the paper. "This method offers real-time observation of nanowire deformation and fracture, while simultaneously providing quantitative stress and strain data. The method is very efficient, so a large number of specimens can be tested within a reasonable period of time."As it turns out, silicon nanowires deform in a very different way from bulk silicon. "Bulk silicon is very brittle and has limited deformability, meaning that it cannot be stretched or warped very much without breaking." says Feng Xu, a Ph.D. student at NC state and co-author of the paper, "But the silicon nanowires are more resilient, and can sustain much larger deformation. Other properties of silicon nanowires include increasing fracture strength and decreasing elastic modulus as the nanowire gets smaller and smaller."The fact that silicon nanowires have more deformability and strength is a big deal. "These properties are essential to the design and reliability of novel silicon nanodevices," Zhu says. "The insights gained from this study not only advance fundamental understanding about size effects on mechanical properties of nanostructures, but also give designers more options in designing nanodevices ranging from nanosensors to nanoelectronics to nanostructured solar cells."The study, "Mechanical Properties of Vapor-Liquid-Solid Synthesized Silicon Nanowires," was co-authored by Zhu, Xu, Qin, University of Michigan (UM) researcher Wei Lu and UM Ph.D. student Wayne Fung. The study is published in the Nov. 11 issue of Nano Letters, and was funded by grants from the National Science Foundation and NC State.
susanwbrenner@yahoo.comDistributed securityA new sharing approach to online securityCould an entirely new approach to online security, based on distributed sanctions, help prevent cybercrime, fraud and identity theft? A report in the International Journal of Intercultural Information Management suggests it could.Susan Brenner of the University of Dayton School of Law, in Dayton, OH, and Leo Clarke of the Thomas M. Cooley School of Law, in Lansing, MI, suggest that government could control cybercrime by requiring anyone accessing cyberspace to employ reasonable security measures but without infringing on civil liberties.Modern criminal law has its origins in the industrial revolution of the nineteenth century and does not meet the needs of the current digital age, Brenner and Clarke explain. Currently, sanctions against criminals, such as prison sentences, fines, and the freezing of assets rely on a system that can entrap the criminals in order to bring them to justice.As such, the law repeatedly fails to tackle the modern phenomenon of cybercrime, where criminals operate virtually through distributed networks and exploit a multitude of loopholes in software and hardware, and as ever through social engineering. There are many reasons that the old legal system fails to combat cybercrime, but primarily it is because the criminal can be anywhere in the world, all they need is an internet connection to commit many thousands of crimes at once.The researchers say that a new paradigm is now needed to cope with this changing landscape of criminal activity. This new paradigm cannot rely on sanctions, but must instead turn the distributed nature of cybercrime on its head.They suggest that a new model must shift the focus of law enforcement from reaction and punishment to deterrence and prevention and to do so requires something akin to community policing but in the virtual world. Individuals must recognize that they are their own front line defense against cybercrime, but with the critical community structures that can exist on the internet they are not alone in building and maintaining their defenses, the researchers explain.Fundamentally, Brenner and Clarke argue, a new generation of cybercrime prevention laws would require citizens, organizations, and companies to identify and obtain the tools necessary to prevent cybercrime, to install these tools and keep them updated, and to use them in an effective manner to prevent identity theft, anonymous email relaying, and the expansion of zombie networks of infected computers.However, they do not provide a prescription for deciding which tools are effective or how this might be policed. Nevertheless, by inverting the usual prevention and sanction approach, the individuals' expectation of law enforcement becomes their responsibility to avoid being a victim."Combatting cybercrime through distributed security" in Int. J. Intercultural Information Management, 2009, 1, 259-274
ghunka@aftau.orgSensing disasters from spaceTel Aviv University's 'Earth binoculars' see our planet through an astral lensOne small step for mankind is now a leap for averting natural and man-made disasters on earth. New Tel Aviv University technology combines sophisticated sensors in orbit with sensors on the ground and in the air to create a "Hyperspectral Remote Sensor" (HRS).  It can give advance warnings about water contamination after a forest fire, alert authorities of a pollution spill long before a red flag is raised on earth, or tell people in China where a monsoon will strike. Prof. Eyal Ben-Dor of Tel Aviv University's Department of Geography describes his team's HRS technology as a combination of physical, chemical and optical disciplines.  "When a devastating forest fire hits the Hollywood Hills, for example, we can see from space how the mineralogy of the soil has changed," he explains.  "Because of these changes, the next rainstorm may wash out all the buildings or leach contaminants into the soil.  With our new tool, we can advise on how to contain the pollutants after the fire, and warn if there is a risk for landslides." Details on new applications of this technology were presented recently in several leading journals including Soil Science Society of American Journals, Soil Science Journal and the International Journal of Remote Sensing.Putting a price on dirtHRS provides information useful to property developers as well. It can offer a soil profile map with detailed information for contractors, farmers or vintners interested in making major land purchase deals or managing existing ones. It can also indicate where water runoff should be directed and what minerals may be lacking in a given parcel of land. "Water is an expensive commodity today," says Prof. Ben Dor. "Knowing how to better manage water resources is a top priority for states like California, and our new tool could help them do that."Today, it can take years before authorities can detect chemicals that can compromise our health. For example, about 90% of all gas stations leak contaminants into the soil, says Prof. Ben-Dor. His new HRS can monitor gas stations and identify problematic areas. "Our space sensors combined with ground measurements and GPS data will be able to detect and map hydrocarbon contamination in real time.  Within a year, we'll be able to identify these problematic areas far more quickly than with traditional methods," he says. Seeing where Earth takes the heatThe HRS simultaneously acquires hundreds of optical images, each from a different frequency, that enable a "spectral assessment" from distances high in the air via airplanes and in orbit using satellites.  This raw data is then processed by Prof. Ben Dor and his team to yield sophisticated thematic maps. "These are not regular maps at all," says Prof. Ben-Dor. "We are combining properties from the physical, chemical and optical worlds, using all the latest technologies available from these fields. Ours is one of a few leading teams in the world exploring this novel way of mapping earth."These "soil maps" supply a bigger picture. Water bodies and sediment runoff in California.  A small soil patch in a California forest after a fire.  Impending monsoons and floods in China.  Contaminants surrounding a factory.  All these can literally be "seen" from space with the HRS. Previous research by Prof. Ben-Dor, which focused on mapping urban heat islands from space, is now regularly used by cities and urban planners to develop projects such as "green" roofs or new parks. American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
pressoffice@epsrc.ac.ukPioneering cyber-security center to transform crime preventionThese are some of the research projects that will be the focus of a major UK cyber-security centre launched today.  The new £25 million Centre for Secure Information Technologies (CSIT), based at Queen's University Belfast, is funded by the Engineering and Physical Sciences Research Council (EPSRC), the Technology Strategy Board, Queen's University Belfast and a range of partner organisations. CSIT is unique because it brings together, under one roof, cutting-edge expertise in data security, network security, wireless network enabled systems and surveillance intelligence systems. Harnessing this expertise, CSIT will help pioneer the concept of 'converged security'  the use of IT systems to improve people's physical security while protecting the systems themselves to ensure they can't be hacked into. Professor John McCanny, Principal Investigator at CSIT, says:"CSIT has an excellent technology platform based on world-leading expertise at the University and its Institute of Electronics, Communications and Information Technology (ECIT). Our starting points are mission-driven projects for which we have identified end goals. CSIT also has a strong entrepreneurial ethos. We're confident that we'll be able to fast-track the development of marketable applications of our technologies to the benefit of UK industry and the wider economy." CSIT is one of the first Innovation and Knowledge Centres, (IKCs), to be established in the UK. Research project detailed outlines [photos available from the EPSRC Press Office]:  Ultra-powerful processors will make internet surfing much saferInnovative content-processor technology that analyses internet traffic in real time, enabling threatening behaviour to be identified and stopped immediately.Within 3 to 5 years, groundbreaking computer hardware being developed at CSIT could begin to make a major contribution to foiling cyber-crime and protecting internet users from paedophiles, harassment and other online threats.CSIT is working on a new type of content-processor technology that, unlike current hardware used by network providers, can inspect and analyse internet traffic in real time  enabling risky or threatening online behaviour to be pinpointed and stopped before any harm is done. The key to this capability will be the new processors' capacity to process data between 100 and 10,000 times faster than existing hardware and software-based content-processing solutions. Although the internet is now a mainstream medium for communication, self-expression and information dissemination, the phenomenal volume of online traffic makes it impossible to police, control and manage using today's technology. With literally billions of e-mail addresses and millions of websites in use, even advanced firewalls and similar measures offer only limited protection. Quite simply, current data processing hardware can't analyse traffic fast enough to enable every suspicious online conversation, virus-bearing e-mail and request to visit a 'bad' website, for example, to be detected and blocked automatically and immediately. The internet can therefore be a dangerous environment, especially for children and anyone who isn't computer literate.  "Because conventional processor technology can only deal with information character by character, it's far too slow to analyse internet traffic in real time," says project leader Dr Sakir Sezer. "We're developing parallel processors which can be scaled to process up to 32 characters (256-bit) at once, making real-time inspection of huge data volumes possible for the first time ever. Network providers will be able to install and use this technology to provide much better protection for internet users, as well as advanced user experience (i.e. quality of service), and efficient utilisation/management of network resources."To maximise the value of this leading-edge processor technology, Dr Sezer's team also aim to optimise the rulesets that enable processing hardware to decide, based on the nature of the internet traffic, which website requests to block, which word sequences may indicate threatening behaviour, which traffic may be generated by malicious software (malware, adware, spyware, botware) and which unsolicited e-mails may carry damaging content (viruses, worms, spam)."The combination of next-generation content-processor technology and more sophisticated rulesets will improve internet security beyond recognition, ensuring more threats and attacks are prevented or mitigated at a much earlier stage," Dr Sezer comments. "That means less online bullying and harassment, less identity theft, fewer viruses and less internet misuse in general for users to contend with."Dr Sezer's team integrates leading-edge expertise in: internet traffic/threat mining; policy/ruleset definition (including legal issues); technology development (harnessing hardware and software skills); and system-level security/network issues. They are also working in close consultation with equipment manufacturers, security vendors and network operators in the UK and US to ensure the project meets market needs and delivers real-world benefits as soon as possible.  Intelligent CCTV analysis could cut crime on public transportIntelligent computer technology that recognises suspicious behaviour in live internet-enabled CCTV feeds from buses and trains, allowing control room staff to intervene and prevent assaults, thefts and other incidents.Innovative computer technology that automatically and accurately interprets behaviour in live CCTV feeds could be deployed in around 5 years to help protect transport users from potential assault, theft and other offences. The technology will underpin a unique system which not only alerts CCTV control room staff that an incident might be about to occur, but also equips them to intervene and prevent it from happening.Building on Queen's University Belfast's EPSRC-funded ISIS (Integrated Sensor Information System) project which developed the initial system, CSIT will now take the work forward in two key areas: increasing the sophistication of the data analysis techniques which the system incorporates; and equipping the system with the artificial intelligence needed to connect small-scale events, draw conclusions about their significance and prioritise information displayed in CCTV control rooms.        Initially designed for use on-board buses, the purpose of the system is to automatically analyse video and audio information captured by every camera/sensor in a CCTV network and instantly relay the feeds it judges most suspicious or threatening to four screens in the control room.      The controller can then decide what action(s), if any, to take. Options could include: alerting the police car nearest the bus; sending the relevant feed to a small screen in the bus driver's cab (the driver can then activate a recorded warning saying a police car is on its way); linking up the bus with the police car so that the police can issue a warning via a video screen on the bus; or the controller can themselves speak to suspicious individuals via the video screen.      Although recent years have seen massive investment in CCTV systems across the UK, their impact has been very limited in terms of preventing crime. This is mainly because most CCTV control rooms are flooded with information from multiple cameras/sensors, making it difficult for controllers to pinpoint feeds showing situations that are likely to develop into criminal incidents."Our system will prioritise feeds but still ensure it's the controller who makes the decisions as to what action to take", says research director Dr Paul Miller. "The system will instantly give every live feed a score, based on factors such as time of day, crime statistics for the location, a threat assessment of the people shown and so on. This score will determine where each feed is placed in the queue for the controller's attention."Specially developed algorithms, software and hardware will all play a vital role in the system. Because data will be transmitted around the system via the internet, the team includes experts in secure ad-hoc wireless networking to ensure that the system can cope with the high volumes of data moving about and can not be hacked into. The team is also working in close collaboration with the Applied Criminology Centre at the University of Huddersfield, which is providing crime statistics for bus routes.      "We aim to develop a system which helps to make crime-free buses, trains, stations and airports a reality," says Dr Miller. "Ultimately it could be adapted to protect many other kinds of critical infrastructure too." New processor promises big benefits in combating computer virusesA high-speed data processor that will provide unprecedented protection from internet-borne malware (e.g. viruses) distributed by cyber-criminals.As early as next year, a high-speed data processor now coming to market could start providing households and businesses with unprecedented protection from viruses and other internet-borne malware.Developed by Titan IC Systems, a Queen's University Belfast spin-out company with close links to CSIT, the RXP (Regular Expression Processor) is capable of investigating all internet traffic on a PC or other internet-linked appliance, identifying malware, spam or unusual behaviour (e.g. fraud) with unrivalled accuracy.The RXP is unique because, unlike other anti-malware technology, it analyses all internet traffic in real time and allows the customer (e.g. a network provider) to describe the characteristics of viruses they want to stop, as these characteristics may change. Both of these capabilities are key to improving the level of protection provided.Dovetailing with these capabilities is the RXP's capacity to process huge volumes of data very quickly, thanks to the fact that it can process multiple characters simultaneously, rather than sequentially as is the case with conventional content processors. It will therefore be able to cope with the massive explosion of internet data predicted in the years ahead and with growing demand for faster broadband speeds, as people increasingly watch TV via broadband, for example."In the next few years, demand on the internet could grow ten or a hundred times over", says Dr Godfrey Gaston, Titan IC Systems' CEO and CSIT Director. "The RXP provides the processing power needed to analyse this tidal wave of data, thus providing a secure internet experience."Keeping viruses, worms, spyware and so on at bay is vital because the aim of such malware is no longer just to disrupt or damage appliances and networks. Today, cyber-criminals also use malware to gather personal, financial and other data about individuals and organisations  data that can be used to commit fraud, identity theft and other serious offences.The RXP is designed to be incorporated in internet routers and gateways operating at local exchange or street level. Smaller versions are also planned for deployment in individual properties, delivering a 'defence in depth' internet security solution. Working prototypes of the RXP are currently being evaluated by companies in the network equipment manufacturing sector, and a product launch is envisaged within around 6-12 months. Each RXP will be customised to the particular needs of each purchasing organisation. The RXP could also have potential applications in other fields, such as DNA profiling.      As Titan IC Systems continues to develop the RXP and other new products, it plans to work in close co-operation with CSIT, to the benefit of both parties."The link with CSIT will help the company stay right at the cutting edge," says Dr Gaston. "Collectively, CSIT and Titan IC Systems provide a dynamic environment for knowledge transfer and the commercialisation of leading-edge ideas."    The Engineering and Physical Sciences Research Council (EPSRC) is the UK's main agency for funding research in engineering and the physical sciences. The EPSRC invests around £850 million a year in research and postgraduate training, to help the nation handle the next generation of technological change. The areas covered range from information technology to structural engineering, and mathematics to materials science. This research forms the basis for future economic development in the UK and improvements for everyone's health, lifestyle and culture. EPSRC also actively promotes public awareness of science and engineering. EPSRC works alongside other Research Councils with responsibility for other areas of research. The Research Councils work collectively on issues of common concern via Research Councils UK. Website address for more information on EPSRC: www.epsrc.ac.uk/  Queen's University Belfast is a member of the Russell Group of the UK's 20 leading research-intensive universities and an international centre of academic excellence rooted at the heart of Northern Ireland. The University has a broad academic profile which covers a wide range of disciplines, from medicine, law and engineering to the humanities, social sciences, science and agriculture. In recent years it has emerged as a major player on the international scene in areas ranging from wireless technology to poetry, cancer studies to climate change and from pharmaceuticals to sonic arts. Queen's was one of the first UK universities to recognise the importance of bringing research excellence to the marketplace and has created around 50 spin-out companies, employing more than 900 people. It is the leading higher education institution in the UK for turnover from its spinout companies."The Technology Strategy Board is a business-led executive non-departmental public body, established by the government. Its role is to promote and support research into, and development and exploitation of, technology and innovation for the benefit of UK business, in order to increase economic growth and improve the quality of life. It is sponsored by the Department for Business, Innovation and Skills (BIS). For more information please visit www.innovateuk.orgIf you'd like to arrange interviews with the academics involved please contact the Press Office at Queen's University Belfast: Brian Arlow, e-mail: brianarlow@mac.com, tel 02891 470 700, mob: 0786 0289 143 or Lisa McElroy, e-mail: lisa.mcelroy@qub.ac.uk  For images contact: The EPSRC Press Office on 01793 444404, E-mail: pressoffice@epsrc.ac.ukImage details and captions:CSIT1.jpg: 'The building where CSIT is based at Queen's University Belfast' G. gaston.jpg: 'Dr Godfrey Gaston'Sakir1 and Sakir2.jpgs:  'Dr Sakir Sezer' P.Miller.jpg: 'Dr Paul Miller'
caisen@iupui.eduAttacking emerging health risks through innovative health information technologyCDC Grant creates Indiana Center of Excellence in Public Health InformaticsINDIANAPOLIS    Researchers from Indiana University and the Regenstrief Institute, with its world-renowned medical informatics research group and regional health information exchange, have been awarded a $4.8 million grant by the Centers for Disease Control and Prevention to create the Indiana Center of Excellence in Public Health Informatics, one of only four such centers in the nation.  The five-year award builds upon the unique capabilities of the Indiana Network for Patient Care to securely exchange health information when and where needed for purposes of health care treatment.  INPC, developed by Regenstrief physician-researchers, currently allows medical providers across the state to securely obtain patients' medical histories, providing information critical to patient care. Nowhere else in the nation can this be done.The new center also brings together the expertise of  the Polis Center, a national leader in community-based and public health research and applications using geographic information technologies; the Indiana State Health Department; the Marion County (Ind.) Health Department;  the IU School of Medicine's Department of Public Health; the Department of Geography in the School of Liberal Arts at Indiana University-Purdue University Indianapolis; IUPUI's Center for Health Geographics, and a unique data visualization group at IU- Bloomington."We are very excited to draw upon the broad expertise of these diverse groups," said Shaun Grannis, M.D., Regenstrief Institute investigator and IU School of Medicine assistant professor of family medicine. "With the addition of their input plus our extensive work in informatics and biosurveillance, we can leverage Regenstrief's strengths in truly novel ways to improve the health of our community and eventually the nation. And by building on existing proven technology already used for clinical health care, we minimize development costs and rapidly implement technology that delivers real-world value to public health." Dr. Grannis is director of the new center. The new multidisciplinary center is the first to take this comprehensive approach to expand and develop innovative public health information tools to improve patient care.  Areas of initial work by the center include:Much of this work will utilize DOCS4DOCS®, a clinical messaging service developed by Regenstrief's health-care information technology professionals and operated by the Indiana Health Information Exchange, one of the nation's most respected health information exchange organizations. Currently DOCS4DOCS delivers more than five million messages with information, such as laboratory or other test results, critical to patient care."The two-way communication model we have developed to send critical data such as lab test results to public health officials and to convey public health alerts to doctors in a fashion that is seamlessly integrated into their work flow will increasingly be the model for bi-directional public health data exchange," said Dr. Grannis.The other Centers of Excellence in Public Health Informatics established by the CDC are located at the University of Utah, the University of Pittsburgh, and Harvard Pilgrim Health Care.
peter.vietti@navy.milCNN's 'Vital Signs' to feature US Navy-funded technologyEnhanced digital breast imagingA breakthrough technology adapted for breast cancer detection based in part on research originally sponsored by the Office of Naval Research (ONR) is set to air July 23 during a CNN International news segment on Vital Signs, a program hosted by Dr. Sanjay Gupta. The technology, referred to by the U.S. Navy as enhanced digital imaging, developed out of the Navy's need to improve capabilities to detect, localize and classify underwater mines. It later served as the point of departure for research conducted by the Naval Undersea Warfare Center, Division Newport (NUWC), part of the Naval Sea Systems Command (NAVSEA), designed to enhance the discrimination of data in complex underwater environments. Researchers at NUWC later patented the enhanced digital imaging process.The NUWC's Technology Transfer program, also known as T2, made possible the transfer, commercialization and transition of the digital enhancement technology.  The innovative concept was adapted by software developer, Advanced Image Enhancement (AIE), to provide improved enhancement of digital mammography images, thereby increasing the potential for improved reliability in the early detection of breast cancer.The Vital Signs July program series is reporting on select medical advances born from military research, both on the battlefield out of necessity and other sources, highlighting how innovations eventually are applied in emergency rooms and medical procedures around the world. AIE technology complements existing radiology devices by yielding more detailed images, especially by uncovering the often faint cancerous lesions in dense and hard-to-detect breast tissue. It is expected to be commercially available in the late summer 2009.	The news feature segment will air on the following:The Office of Naval Research (ONR) manages science and technology research for the Navy and Marine Corps.   ONR sponsors basic and applied research in oceanography, advanced materials, sensors, robotics, biomedical science and technology, electronics, surveillance, mathematics, manufacturing technology, information science, advanced combat systems, and technologies for ships, submarines, aircraft, ground vehiclesand much more.   For information about ONR's programs, go to http://www.onr.navy.mil.
ghunka@aftau.orgProtecting your virtual privacyTel Aviv University recommends that lawmakers take a closer look at digital and Internet securityThe details of your personal life, such as grocery purchases and pizza topping preferences, are collected every day ― online and by club and discount cards from the gym, department store and supermarket. Though this data seems innocent enough, when it's put together it can tell a whole lot about your health, finances and behavior. That information, a Tel Aviv University researcher reminds us, could eventually be used against you.  Dr. Michael Birnhack of TAU's Faculty of Law and Prof. Niva Elkin-Koren from the University of Haifa recently completed a comprehensive study on information privacy laws in Israel and found compelling reasons for lawmakers everywhere to take notice. "Our research from Israel can serve as a case study of the shortcomings of a comprehensive data protection program," says Dr. Birnhack.  "It's not just sites like Facebook and Twitter that should cause concern," he continues. "It's all the trivial things that are collected about us that we're not protected against."Your digital dossierThe process can be seductive: information collected by websites has benefits, too. Based on previous purchase and search queries, Amazon can recommend books for readers "just like you." But in the wrong hands, similar information collected by Web sites and discount card companies could be used by health insurance organizations to boost premiums or by employers trying to figure out how many sick days you'll be taking each year. It could even make or break your chances of landing that new job, Dr. Birnhack says. A health insurance provider doesn't need to see your medical records to understand the state of your family's health. It can learn just as much by looking at your grocery bill. "If you use a discount card at a supermarket, information on your purchases is added to a database. If you shop for halal or kosher products, your religion can be inferred, and the purchases of fatty or gluten-free foods can provide an indicator of your family's overall health." Federal legislation in the U.S. regulates for some 15 different kinds of specific data sets, such as health data and credit histories, but not for information collected by club and discount cards or by commercial Web sites. And it's more difficult to write a law to secure confidentiality in those areas, says Dr. Birnhack. "Unless there are specific laws in place, this personal digital information is up for grabs. It can be bought and sold between governments and private companies, which can then conduct data mining and analysis on it and sell the results to third parties," he explains.Like Europe, Canada has a universal informational privacy policy, but U.S. data collection and dissemination regulation is more limited. Justice system lawyers are currently debating the issue of informational privacy, and Dr. Birnhack suggests that they look to Canada's law as a good way to protect privacy. "Canada has the best data protection regime in the world," he says. "It's very powerful."Reading the fine printIn conducting their research, Birnhack and Elkin-Koren examined close to 1,400 Israeli websites and their privacy statements and attempted to discern whether or not the sites complied with the law. They then reported their findings reported on the Social Science Research Network (SSRN) website in a paper available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1456968. Even though Israeli law requires them to do so, a significant number of sites don't state that they are collecting this information, while a majority of popular commercial sites reserve the right to change their privacy policies at any time. This means that data is up for grabs. "Legislators should be aware of how easy it is to collect personal information about citizens to start building more protective laws," Dr. Birnhack concludes.  	American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
Morris.692@osu.eduNew way to make sensors that detect toxic chemicalsCOLUMBUS, Ohio -- Ohio State University researchers have developed a new method for making extremely pure, very small metal-oxide nanoparticles.They are using this simple, fast, and low-temperature process to make materials for gas sensors that detect toxic industrial chemicals (TICs) and biological warfare agents.The researchers described their work in a recent issue of the journal Materials Chemistry and Physics.Patricia Morris, associate professor of materials science and engineering at Ohio State, leads a team of researchers who develop solid materials that can detect toxic chemicals.The challenge, she said, is to design a material that reacts quickly and reliably to a variety of chemicals, including TICs, when incorporated into a sensor."These are sensors that a soldier could wear on the battlefield, or a first responder could wear to an accident at a chemical plant," Morris said.The material under study is nickel oxide, which has unusual electrical properties. Other labs are studying nickel oxide for use in batteries, fuel cells, solar cells, and even coatings that change color.But Morris, along with Ohio State doctoral student Elvin Beach, is more interested in how nickel oxide's electrical conductance changes when toxic chemicals in the air settle on its surface. Beach applies a thin coating of the material onto microelectro-mechanical systems (made in a similar fashion to computer chips), with a goal of identifying known toxic substances.The design works on the same general principle as another, much more familiar sensor."The human nose coordinates signals from hundreds of thousands of sensory neurons to identify chemicals," Beach said. "Here, we're using a combination of electrical responses to identify the signature of a toxic chemical."The key to making the sensor work is how the nickel oxide particles are made. Beach and Morris have devised a new synthesis method that yields very small particles -- which give the sensor a large surface area to capture chemical molecules from the air -- and very pure particles -- which enable the sensor to detect even very small quantities of a substance.Each particle of nickel oxide measures only about 50 atoms across -- that's equivalent to five nanometers (billionths of a meter).Beach described the synthesis method in very simple terms."Basically, you mix everything together in a pressure vessel, pop it in the oven, rinse it off and it's ready to use," he said.Of course, for the process to go smoothly, the researchers have to meet specific conditions of temperature and pressure, and leave the material in the pressure cooker for just the right amount of time. For this study, they set the pressure cooker to around 225 °C. They found they can make the particles in as little as 12 hours, but no more than 24 hours."Too short a time, and the nickel oxide doesn't form -- too long and it reduces to metallic nickel," Beach explained.After he removes the nickel oxide from the pressure cooker, he washes it in a common solvent called methyl ethyl ketone to free up the nanoparticles.At that point, the material is ready to use. Most other synthesis methods require another additional step -- a high-temperature heat treatment.Starting with a microsensor silicon chip array provided by collaborators at the National Institute of Standards and Technology (NIST), Beach adds a layer of particles using a device called a picoliter drop dispenser. A picoliter is a trillionth of a liter.He describes the dispenser as a kind of inkjet printer that places a droplet of a liquid suspension containing particles onto a surface -- in this case, the chips.According to Morris, this is the first time that nickel oxide nanoparticles have been applied in this way.But to Beach, the most important "first" to come out of the study is their discovery of the reaction pathway -- that is, the various chemical steps that take place inside the pressure cooker during the synthesis of the material.Now that the researchers know the reaction pathway, they can devise ways to add chemical dopants to the nanoparticles. Dopants would change the function of the sensor -- for instance, to speed up the response rate.A one-gram batch of nickel oxide nanoparticles costs about $5.00 to make; one chip carries four nanograms (billionths of a gram) of material, so each sensor costs only pennies to fabricate.Other applications could include exhaust or pollution monitoring and air quality monitoring.Collaborators on the project include Steve Semancik and Kurt Benkstein at NIST. Study coauthors include: Krenar Shqau, an Ohio State postdoctoral researcher; Samantha Brown, then an undergraduate student visitor from Northwestern University who will return to Ohio State this fall to pursue her doctorate in Chemistry; and Steven Rozeveld at Dow Chemical Co., who helped Beach produce electron microscope images of the nanoparticles.This work is funded by the National Science Foundation and Ohio State University.Contact: Patricia Morris, (614) 247-8873; Morris.692@osu.eduElvin Beach, Beach.110@osu.eduWritten by Pam Frost Gorder, (614) 292-9475: Gorder.1@osu.edu..
hboffin@eso.orgWorld's fastest and most sensitive astronomical camera"The performance of this breakthrough camera is without an equivalent anywhere in the world. The camera will enable great leaps forward in many areas of the study of the Universe," says Norbert Hubin, head of the Adaptive Optics department at ESO. OCam will be part of the second-generation VLT instrument SPHERE. To be installed in 2011, SPHERE will take images of giant exoplanets orbiting nearby stars.A fast camera such as this is needed as an essential component for the modern adaptive optics instruments used on the largest ground-based telescopes. Telescopes on the ground suffer from the blurring effect induced by atmospheric turbulence. This turbulence causes the stars to twinkle in a way that delights poets, but frustrates astronomers, since it blurs the finest details of the images.Adaptive optics techniques overcome this major drawback, so that ground-based telescopes can produce images that are as sharp as if taken from space. Adaptive optics is based on real-time corrections computed from images obtained by a special camera working at very high speeds. Nowadays, this means many hundreds of times each second. The new generation instruments require these corrections to be done at an even higher rate, more than one thousand times a second, and this is where OCam is essential."The quality of the adaptive optics correction strongly depends on the speed of the camera and on its sensitivity," says Philippe Feautrier from the LAOG, France, who coordinated the whole project. "But these are a priori contradictory requirements, as in general the faster a camera is, the less sensitive it is." This is why cameras normally used for very high frame-rate movies require extremely powerful illumination, which is of course not an option for astronomical cameras.OCam and its CCD220 detector, developed by the British manufacturer e2v technologies, solve this dilemma, by being not only the fastest available, but also very sensitive, making a significant jump in performance for such cameras. Because of imperfect operation of any physical electronic devices, a CCD camera suffers from so-called readout noise. OCam has a readout noise ten times smaller than the detectors currently used on the VLT, making it much more sensitive and able to take pictures of the faintest of sources."Thanks to this technology, all the new generation instruments of ESO's Very Large Telescope will be able to produce the best possible images, with an unequalled sharpness," declares Jean-Luc Gach, from the Laboratoire d'Astrophysique de Marseille, France, who led the team that built the camera."Plans are now underway to develop the adaptive optics detectors required for ESO's planned 42-metre European Extremely Large Telescope, together with our research partners and the industry," says Hubin.Using sensitive detectors developed in the UK, with a control system developed in France, with German and Spanish participation, OCam is truly an outcome of a European collaboration that will be widely used and commercially produced.More informationThe three French laboratories involved are the Laboratoire d'Astrophysique de Marseille (LAM/INSU/CNRS, Université de Provence; Observatoire Astronomique de Marseille Provence), the Laboratoire d'Astrophysique de Grenoble (LAOG/INSU/CNRS, Université Joseph Fourier; Observatoire des Sciences de l'Univers de Grenoble), and the Observatoire de Haute Provence (OHP/INSU/CNRS; Observatoire Astronomique de Marseille Provence).OCam and the CCD220 are the result of five years work, financed by the European commission, ESO and CNRS-INSU, within the OPTICON project of the 6th Research and Development Framework Programme of the European Union. The development of the CCD220, supervised by ESO, was undertaken by the British company e2v technologies, one of the world leaders in the manufacture of scientific detectors. The corresponding OPTICON activity was led by the Laboratoire d'Astrophysique de Grenoble, France. The OCam camera was built by a team of French engineers from the Laboratoire d'Astrophysique de Marseille, the Laboratoire d'Astrophysique de Grenoble and the Observatoire de Haute Provence. In order to secure the continuation of this successful project a new OPTICON project started in June 2009 as part of the 7th Research and Development Framework Programme of the European Union with the same partners, with the aim of developing a detector and camera with even more powerful functionality for use with an artificial laser star. This development is necessary to ensure the image quality of the future 42-metre European Extremely Large Telescope.ESO, the European Southern Observatory, is the foremost intergovernmental astronomy organisation in Europe and the world's most productive astronomical observatory. It is supported by 14 countries: Austria, Belgium, the Czech Republic, Denmark, France, Finland, Germany, Italy, the Netherlands, Portugal, Spain, Sweden, Switzerland and the United Kingdom. ESO carries out an ambitious programme focused on the design, construction and operation of powerful ground-based observing facilities enabling astronomers to make important scientific discoveries. ESO also plays a leading role in promoting and organising cooperation in astronomical research. ESO operates three unique world-class observing sites in Chile: La Silla, Paranal and Chajnantor. At Paranal, ESO operates the Very Large Telescope, the world's most advanced visible-light astronomical observatory. ESO is the European partner of a revolutionary astronomical telescope ALMA, the largest astronomical project in existence. ESO is currently planning a 42-metre European Extremely Large optical/near-infrared Telescope, the E-ELT, which will become "the world's biggest eye on the sky".ContactsNorbert Hubin, Mark DowningESOPhone: +49 89 3200 6517, +49 89 3200 6389E-mail: nhubin (at) eso.org, mdowning (at) eso.orgJean-Luc GachLaboratoire d'Astrophysique de Marseille (LAM), FrancePhone: +33 4 95 04 41 19E-mail: jean-juc.gach (at) oamp.frPhilippe FeautrierLaboratoire d'Astrophysique de Grenoble (LAOG), FrancePhone: +33 4 76 63 59 81E-mail: Philippe.Feautrier (at) obs.ujf-grenoble.fr
walters1@andrew.cmu.eduCarnegie Mellon researchers find social security numbers can be predicted with public informationPITTSBURGHCarnegie Mellon University researchers have shown that public information readily gleaned from governmental sources, commercial data bases, or online social networks can be used to routinely predict most  and sometimes all  of an individual's nine-digit Social Security number. 	Project lead Alessandro Acquisti, associate professor of information technology and public policy at Carnegie Mellon's H. John Heinz III College, and Ralph Gross, a post-doctoral researcher at the Heinz College, have found that an individual's date and state of birth are sufficient to guess his or her Social Security number with great accuracy. The study findings will appear this week in the online Early Edition of the Proceedings of the National Academy of Science, and will be presented on July 29 at the BlackHat 2009 information security conference in Las Vegas. Additional information about the study and some of the issues it raises is available at http://www.ssnstudy.org. 	The predictability of Social Security numbers is an unexpected consequence of seemingly unrelated policies and technological developments that, in combination, make Social Security numbers obsolete for authentication purposes, according to Acquisti and Gross. Because many businesses use Social Security numbers as passwords or for other forms of authentication  a use not anticipated when Social Security was devised in the 1930s  the predictability of the numbers increases the risk of identity theft. ID theft cost Americans almost $50 billion in 2007 alone. The Social Security Administration could mitigate this vulnerability by assigning numbers to people based on a randomized scheme, but ultimately an alternative means of authenticating identities must be adopted, the authors conclude.	"In a world of wired consumers, it is possible to combine information from multiple sources to infer data that is more personal and sensitive than any single piece of original information alone," said Acquisti, a researcher in the Carnegie Mellon CyLab. Information that once was useful to make public may now be too available. An example is the Social Security Administration's Death Master File, a public database with Social Security numbers, dates of birth and death, and states of birth for every deceased beneficiary. Its purpose is to prevent impostors from assuming the Social Security numbers of deceased people. But Acquisti and Gross found that analyzing the death file enabled them to detect statistical patterns that would help them predict Social Security numbers of the living.	These statistical patterns can help narrow guesses of an individual's Social Security number, when combined with that person's date and state of birth. Birth information can be obtained from various sources, including commercial databases, public records (such as voter registration lists) and the millions of profiles that people publish about themselves on social networks, personal Web sites and blogs.	The statistical patterns and the birth information can be used to predict Social Security numbers because the Social Security Administration's methods for assigning numbers, based in part on geography, are well-known. For most individuals born nationwide since 1989, Social Security numbers are assigned shortly after birth, making those numbers easier to predict.	Acquisti and Gross tested their prediction method using records from the Death Master File of people who died between 1973 and 2003. They could identify in a single attempt the first five digits for 44 percent of deceased individuals who were born after 1988 and for 7 percent of those born between 1973 and 1988. They were able to identify all nine digits for 8.5 percent of those individuals born after 1988 in fewer than 1,000 attempts. Their accuracy was considerably higher for smaller states and recent years of birth: for instance, they needed 10 or fewer attempts to predict all nine digits for one out of 20 SSNs issued in Delaware in 1996. Sensitive details of the prediction strategy were omitted from the article.	 "If you can successfully identify all nine digits of an SSN in fewer than 10, 100 or even 1,000 attempts, that Social Security number is no more secure than a three-digit PIN," the authors noted. 	When the researchers tested their method using birth dates and hometowns that students had self-reported on popular social networking sites, the results were almost as good despite the inaccuracies typical of social network data. Enrollment records were used to confirm the accuracy of the predictions, though the researchers did not receive confirmation of any individual Social Security number, but only aggregate measures of accuracy.	 "Dramatically reducing the range of values wherein an individual's Social Security number is likely to fall makes identity theft easier," Gross said. A fraudster who knows just the first five digits of an individual's number might use a phishing email to trick the person into revealing the last four digits. Or, a fraudster could use networks of compromised computers, or "botnets," to repeatedly apply for credit cards in a person's name until hitting the correct nine-digit sequence. 	Future Social Security numbers could be made more secure by switching to a randomized assignment scheme, but protecting people who already have been issued numbers is harder, the researchers said. Given the ease with which Social Security numbers can be predicted  particularly the first five digits and particularly for the millions of Americans born since 1988  legislative and policy initiatives aimed at removing the numbers from public exposure, or redacting their first five digits, may be well-meaning but misguided, Acquisti added.	"Given the inherent vulnerability of Social Security numbers, it is time to stop using them for verifying identities and redirect our efforts toward implementing secure, privacy-preserving authentication methods," Acquisti said. Methods to consider include two-factor authentication, similar to the PIN number/card combinations used for bank accounts, and digital certificates.		Students Ioanis Alexander Biternas, Ihn Aee Choi, Jimin Lee and Dhruv Deepan Mohindra assisted Acquisti and Gross in the study. The Heinz College (http://www.heinz.cmu.edu) includes the School of Information Systems and Management and the School of Public Policy and Management and its faculty and students bring expertise to bear on issues of information security and policy and information systems, as well as public policy, arts and health care management.	The National Science Foundation, the U.S. Army Research Office, Carnegie Mellon CyLab and the Berkman Faculty Development Fund provided support for this research.	About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion comprehensive campaign, titled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty, students and innovative research, and enhance the physical campus with equipment and facility improvements. For more about Carnegie Mellon, visit http://www.cmu.edu/about/.
pberzins@stevens.eduStevens Institute of Technology to host Cyber Security Policy Conference, Jan. 19-20Focus on national and global policy at Stevens' new Center of DC OperationsHOBOKEN, N.J. & WASHINGTON, D.C. - In January in our nation's capital, Stevens Institute of Technology will host a unique conference that will closely examine national and global cyber security policy. 	In an age where so much data and communication is centered on the Internet, it is vitally important that governments, commercial and corporate entities along with private citizens are working towards securing these critical infrastructure systems. 	Stevens, a New Jersey-based technological university, has number of funded academic and research programs dedicated to cyber security and will lead the Global Cyber Security Policy Conference, January 19-20, 2010, at its new facility in the Reagan Building in Washington, D.C. 	The conference will take a holistic and whole-of-society view of the implications of working with and controlling for the betterment of society what has become known as the Cyberspace Domain. 	"The three major segments of Government must collaborate in a way that the public can understand and support in order to preserve the best aspects of the Internet and to overcome burgeoning threats to our privacy, critical infrastructure, and potentially to our very way of life," said Dr. Joseph Mitola III, Vice President for the Research Enterprise, Stevens Institute of Technology.	The conference comes at a time when President Barak Obama hopes to "initiate a national public awareness and education campaign to promote cyber security."	Conference participants will adopt a CEO perspective on cyber security issues and will hear from policy decision makers in both government and industry with greater clarity in evaluating tradeoffs among cyber security mandates and other objectives of their respective missions. 	All points of view from panelists and participants alike will be thoughtfully considered and fully reflected in the resulting plain-talk Report to the Congress and the American People: The 2010 Cybersecurity Policy Guidebook.	The full agenda, a list of speakers and registration information may be found at http://www.stevens.edu/cyberpolicy/About Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value. Stevens offers baccalaureates, master's and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students with about 250 full-time faculty. Stevens' graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.For the latest news about Stevens, please visit www.StevensNewsService.com.
lisa.mcelroy@qub.ac.ukUK's £30M center for cyber security opens at Queen'sThe United Kingdom's lead center for cyber security research is to be opened today at Queen's University BelfastThe United Kingdom's lead centre for cyber-security research is to be opened today at Queen's University Belfast.The £30 million Centre for Secure Information Technologies (CSIT) at Queen's will create 80 new positions and become the UK's principal centre for the development of technology to counter malicious 'cyber-attacks'. Leading edge research that will help keep crime off the internet, combat anti-social behaviour and street crime and safeguard the trustworthiness of information stored electronically, both at home and in the workplace, are just some of the key areas addressed by the new Centre. CSIT will also provide a timely boost to the UK's economy, aiding job creation and strategically positioning UK industry at the forefront of the global communication and information security sector, predicted to grow to around £50 billion in 2011.The Centre is one of the first Innovation and Knowledge Centres (IKCs) created in the UK. Funders include the Engineering and Physical Sciences Research Council and the Technology Strategy Board. In addition, to date over 20 organisations have committed to support CSIT's work over the next five years. They include industrial partners such as BAe Systems and Thales UK as well as government agencies and international research institutes.The Centre will bring together research specialists in complementary fields such as data encryption, network security systems, wireless enabled security systems and intelligent video analysis.Queen's University Vice-Chancellor Professor Peter Gregson said: "The opening of CSIT at Queen's, is the most bold and exciting development the United Kingdom has seen to date in terms of information security. "Cyber-security is a global issue that affects us all. 97 per cent of business in the UK now relies on the internet and other IT systems. By coupling the pioneering research undertaken at CSIT with economic development, Queen's will secure the UK's position in cyberspace."Today is also one of the most significant and defining moments in the history of Queen's University Belfast. The funding to create this Centre is the largest ever package made available for academic research in Northern Ireland."The successful bid for this Centre by Professor John McCanny and his colleagues, against other world-class institutions in the UK, highlights how the research work at Queen's in this crucial sector has become truly internationally recognised. CSIT will become a vital reference point for all businesses working in this field and beyond."The attendance at today's launch of some of the most respected national and international figures in the field of cyber-security, including Larry Rohrbough, Chief Executive of TRUST, the United States' major centre in the area of cyber-security at the University of California at Berkeley, highlights the importance of the new Centre to the global communications and IT industries. Professor John McCanny, CSIT principal investigator, believes the new centre is set to become globally recognised thanks to the breadth and depth of its technological capabilities and because it represents a new international paradigm for innovation: "It is really only now that the international community is beginning to tackle cyber security in a co-ordinated way. Our work at CSIT is therefore of fundamental importance at this critical time in the development of the Internet and related technologies."CSIT has an excellent technology platform based on world-leading expertise at Queen's and its already existing Institute for Electronics, Communications and Information Technology (ECIT) in the Northern Ireland Science Park. Our approach to exploiting the commercial and economic benefits of these strengths represents a major advance on how UK universities have attempted this in the past."The approach adopted within CIST contrasts with the more conventional way academic research is undertaken. Our starting points tend to be larger "mission-driven" projects involving sizeable teams for which ambitious and challenging end goals have been identified."In addition, CSIT has a strong entrepreneurial ethos with a novel commercialisation process built into our management structure. This enables our researchers to work effectively alongside potential customers and specialists from industry and other academic institutions. We are confident that in this way, we will be able to fast track the development of marketable applications of our technologies to the benefit of UK industry and the wider economy as a whole," added Professor McCanny. One of CSIT's fundamental challenges is to develop systems to be deployed at the core of next generation computer and telecoms networks to provide much higher levels of protection than is possible with the Internet security tools installed on today's PCs.Making this a reality will require significant advances in high-performance network and content processing technology - two research areas for which Queen's University Belfast has earned an international reputation.Building on this work, CSIT is developing powerful processors capable of screening huge volumes of data - equivalent to the Internet traffic produced by over 10,000 households - for malicious content and behavior in real time.The processors are the most advanced of their kind and are ideally suited for use at the heart of sophisticated systems controlled by complex sets of rules. These can be written to prevent identity theft and fraud or to protect children from on-line grooming. When an on-line security risk or crime is identified, they will be capable of triggering an immediate response.By bringing together a wide range of security related research fields and technologies under one roof, CSIT is also aiming to pioneer the convergence of network, data and physical security through the development of new technologies and systems. This work is expected to lead to significant improvements in the effectiveness of CCTV technology in combating anti-social behaviour and street crime.  Currently, while much criminal activity is captured by the UK's four million CCTV cameras, very little is observed in real time because of the high cost of employing sufficient numbers of people to monitor activity on screens. This means that while the data they generate may be used to help prosecute offenders, it is of little value in preventing a crime before it occurs. CSIT aims to tackle this problem by using innovative hardware and software designed to analyse CCTV camera data in real time. CSIT's research also covers specific areas which have been identified as being of strategic national security importance in major reports produced recently by the British and American governments. 
mariangela.dacunto@esa.intWater cycle conference makes a big splashEarth has a limited amount of water that recycles itself in what is called the 'water cycle'. Climate change, weather and human life are highly affected by changes in this continuous, interconnected cycle.Observing and monitoring the key variables governing the global water cycle is essential to our understanding of the Earth's climate, forecasting our weather, predicting floods and droughts, and improving water management for human use. Recent advances in Earth observation (EO) satellite technologies have made it possible to survey several of these variables from space. In the coming years an increasing number of EO missions will provide an unprecedented capacity to observe Earth's surface, its interior and the atmosphere, opening a new era in EO and water cycle science. To discuss the challenges and opportunities resulting from this coming increasing capacity, nearly 200 scientists from more than 30 countries gathered at ESA's EO for Water Cycle Science Conference held at the agency's Earth Observation Centre (ESRIN) in Frascati, Italy, 18-20 November, to assess the state-of-the-art instruments and scientific developments used in characterising global water cycle variability and to identify the main needs in modelling and data assimilation to improve our knowledge of water cycle science and our ability to quantify future changes in water cycle variables.  "The conference provided a good overview of current research capabilities and activities in the area of space borne observations of the water cycle," said Peter J. van Oevelen, Director of the Global Energy and Water Cycle Experiment (GEWEX) International Project Office. "The plenary discussions helped identify current and future critical gaps and omissions and, as such, provide valuable input in the strategy and future directions of global climate research coordinative programmes such as the Global Energy and Water Cycle Experiment of the World Climate Research Programme." Presentations focused on novel space missions, precipitation, clouds and water vapour, water levels and surface waters, turbulent energy fluxes and evapotranspiration, floods and droughts, modelling the water cycle, and soil moisture, among others. Round table discussions focused on the main gaps and scientific challenges ahead to better observe, monitor and characterise the different components of the water cycle in view of improving our ability to cope with water management and governance in a world where water is more and more at the centre of international policy and conflicts. The conference also offered an excellent opportunity to the Soil Moisture and Ocean Salinity (SMOS) Lead Investigator, Yann Kerr, to unveil the first data sent to Earth by the satellite, launched on 2 November. SMOS, an ESA Earth Explorer, is the first satellite designed to map sea surface salinity and to monitor soil moisture on a global scale. The first data received was acquired as part of the initial functional verification test following the instrument switch-on on 17 November. Although the image content could not be interpreted at the time, it proved the instrument was in good shape. Data provided by SMOS will be important for weather and climate modelling, water resource management, agriculture planning, ocean currents and circulation studies and forecasting hazardous events such as floods.The experts also discussed the challenges and opportunities in water cycle science in order to reduce uncertainties in water-related climate change impacts and adaptation strategies in water resources. The conference recommendations represent a solid scientific roadmap that outlines the main priorities for the development of novel and robust global geo-information data products, improved models and effective data assimilation systems. The three-day workshop was organised by ESA, GEWEX, the European Geosciences Union and International Society for Photogrammetry and Remote Sensing. Earlier this year, ESA launched, as part of its new Support To Science Element programme, the Water Cycle Multi-Mission Observation Strategy (WACMOS) project in collaboration with GEWEX to support the development of novel techniques to study the water cycle with satellites. At the conference, the WACMOS team showed the first preliminary results of the projects addressing key elements of the water cycle such as global evapotranspiration, soil moisture, clouds and water vapour. WACMOS, carried out by a international team of experts led by the International Institute for Geo-Information Science and Earth Observation, aims to develop the first global map of evapotranspiration exploiting the synergies using the Medium Resolution Imaging Spectrometer and the Advanced Along-Track Scanning Radiometer instruments on ESA's Envisat satellite. Also, the Vienna University of Technology, in partnership with the Vrjge University of Amsterdam, aims to develop the first global multi-decade soil moisture data set merging passive and activate microwave sensors, such as the scatterometer on ESA's ERS-1 and ERS-2. The project also explores advanced clouds and water vapour synergic products. This project, as well as the conference results, represents an ESA contribution to support the international coordination effort carried out by GEWEX to better understand, characterise and forecast the global water cycle. 
etaylor@anl.govArgonne technology enables high-speed data transferGridFTP, a protocol developed by researchers at Argonne National Laboratory, has been used to transfer unprecedented amounts of data over the Department of Energy's (DOE) Energy Sciences Network (ESnet), which provides a reliable, high-performance communications infrastructure to facilitate large-scale, collaborative science endeavors.The Argonne-developed system proved key to enabling research groups at Oak Ridge National Laboratory in Tennessee and the National Energy Research Scientific Computing Center in California to move large data sets between the facilities at a rate of 200 megabytes per second.The deployment of GridFTP at the two computing facilities is part of a major project to optimize wide-area network data transfers between sites hosting DOE leadership-class computers.According to Ian Foster, co-director of the Globus Alliance project responsible for designing GridFTP, large-scale data transfer places an enormous burden on networks. "Conventional protocols have proven unable to handle the increasing demand of large-scale data transfer," he said. "The result has been delays in obtaining data, or even lost data as the network becomes overwhelmed. GridFTP changes that."As large-scale collaborative science projects become increasingly common, the need to transfer unprecedented amounts of data is becoming critical. Having GridFTP on ESnet will enable the sharing of data between supercomputer centers in disciplines such as climate modeling and nuclear physics that require secure, robust, high-speed bulk data transfer."Our goal is to enable the scientists to rapidly move large-scale data sets between supercomputer centers as dictated by the needs of the science," said Eli Dart, a network engineer for ESnet, which is managed by Lawrence Berkeley National Laboratory. "High-performance networking has become critical to science due to the size of the data sets and the wide scope of collaboration characteristic of today's large science projects such as climate research and high energy physics."GridFTP offers several advantages over other data transfer systems. For example, with Secure Copy, or scp, bulk transfer of a 33-gigabyte dataset between the two remote hosts could take up to eight hours. With GridFTP, almost 20 times that amount of data can be transferred in the same amount of time. And, unlike the transfer application FTP, GridFTP uses multiple data channels for improving the transfer speed."The data tsunami problem has been a major bottleneck to scientific advancement," said Raj Kettimuthu, technical lead and technology coordinator of the GridFTP project at Argonne. "With GridFTP computational scientists can analyze their simulated and derived data in real time."More information on GridFTP is available at www.globus.org/grid_software/data/gridftp.php.More information on ESNet is available at www.es.net/.Argonne National Laboratory seeks solutions to pressing national problems in science and technology. The nation's first national laboratory, Argonne conducts leading-edge basic and applied scientific research in virtually every scientific discipline. Argonne researchers work closely with researchers from hundreds of companies, universities, and federal, state and municipal agencies to help them solve their specific problems, advance America 's scientific leadership and prepare the nation for a better future. With employees from more than 60 nations, Argonne is managed by UChicago Argonne, LLC for the U.S. Department of Energy's Office of Science.
mark.esser@nist.govUp-scale: Frequency converter enables ultra-high sensitivity infrared spectrometryIn what may prove to be a major development for scientists in fields ranging from forensics to quantum communications, researchers at the National Institute of Standards and Technology (NIST) have developed a new, highly sensitive, low-cost technique for measuring light in the near-infrared range. The technique can measure the spectrum of the specific wavelengths of near infrared light used widely in telecommunications as well as the very weak infrared light at single-photon levels given off by fragile biomaterials and nanomaterials. They described their results in a recent issue of Optics Express.*A single photon detector is the key device needed to build highly sensitive instruments for measuring spectra. For the past 30 years, scientists have made steady progress increasing the efficiency and sensitivity of visible and ultraviolet photon detectors while methods for detecting elusive single photons in the near-infrared (NIR) range have faltered. The methods presently in use are too static-laden, inefficient and slow, or depend on superconducting detectors, which require expensive, low-temperature operating environments. The NIST group, Lijun Ma, Oliver Slattery and Xiao Tang, wanted to develop a way to use existing detectors such as avalanche photodiode detectors (APD), which work very well for detecting visible light and are widely used, but are ineffective for the detection of NIR.Their approach was to adapt a technique developed two years ago at NIST for quantum cryptography that up converts photons at one frequency to a higher frequency. The technique promotes the infrared photons up to the visible range using a strong, tunable laser. During the frequency conversion process, the narrow-band pump laser scans the infrared signal photons and converts only those that have the desired polarization and wavelength to visible light. Once converted to visible light, the signal photons are easily detected by commercially available APDs. According to Tang, the new system enables the measurement of spectra with sensitivity of more than 1,000 times that of common commercial optical spectral instruments.Our key achievement here was to reduce the noise, but our success would not have been possible without the many years of work by others in this field, says Tang. We hope that our discovery will open doors for researchers studying diseases, pharmaceuticals, secure communications and even solving crimes. We are very excited to make this technology available to the larger scientific community.	* L. Ma, O. Slattery and X. Tang. Experimental study of high sensitivity infrared spectrometer with waveguide-based up-conversion detector. Optics Express. Vol. 17, No. 16. Aug. 3, 2009.
swaney@andrew.cmu.eduCarnegie Mellon CyLab to join new industry consortiumResearch universities study cybersecurity	PITTSBURGHCarnegie Mellon University's CyLab will join Northrop Grumman Corp. (NGC), a leading global security company, and two other research universities to form the Cybersecurity Research Consortium to address the nation's most critical cyber threats. 	The research initiative, launched today at the National Press Club in Washington, D.C., is designed to accelerate the transfer of technology from the lab to commercial use. 	"In this consortium, researchers from Carnegie Mellon CyLab will work side-by-side with Northrop Grumman researchers to address critical real-world challenges by transitioning and further developing CyLab technologies," said CyLab Technical Director Adrian Perrig, a professor of electrical and computer engineering and engineering and public policy at Carnegie Mellon. 	In addition to Northrop Grumman and Carnegie Mellon CyLab, the new consortium will include The Massachusetts Institute of Technology (MIT) Computer Science and Artificial Intelligence Lab (CSAIL) and Purdue University's Center for Education and Research in Information Assurance and Security (CERIAS). 	The universities were chosen for their long-term, leading-edge research in cybersecurity and their national standing in this important arena.	"Carnegie Mellon developed the first federally funded cybersecurity program, the CERT-CC (Computer Emergency Response Team Coordination Center), in 1989 in response to the well-known Internet worm incident the previous year. As a result of this first major cybersecurity incident, CyLab has grown to be one of the largest cybersecurity academic research centers in the world," said Robert Brammer, chief technology officer, Northrop Grumman Information Systems. 	Carnegie Mellon CyLab is a cross-disciplinary, university-wide research program dedicated to cybersecurity, privacy and dependability. It involves six Carnegie Mellon colleges, and includes more than 50 faculty members and 130 graduate students, as well as numerous partners in industry and government. Several research centers within CyLab also focus on cutting-edge research, including the Trustworthy Computing Center, the Biometrics Center, the Usable Privacy and Security Lab (CUPS) and the Mobility Research Center. In the past decade CyLab research has contributed to innovations in mobile ad-hoc network security, sensor network security and trustworthy computing. It also has contributed to an increased understanding and usability of privacy and security tools.	Consortium members will coordinate research projects, share information, develop curricula and author case studies, and provide broader learning experiences for students and the global defense community.  About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven colleges and schools benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion fundraising campaign, entitled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty and students and innovative research, and enhance the physical campus with equipment and facility improvements. 
comms.office@qub.ac.ukQueen's research could help protect frontline troopsA team of researchers at Queen's University Belfast's Centre for Secure Information Technologies (CSIT) is working to develop futuristic communications systems that could help protect frontline troops. Building on work completed recently for the UK Ministry of Defence, the project is aimed at investigating the use of arrays of highly specialised antennas that could be worn by combat troops to provide covert short-range person-to-person battleground communications.The project could lead to the development of advanced wireless systems that would enable small squads of soldiers to share real-time video, covert surveillance data and tactical information with each other via helmet mounted visors. The equipment would bring major benefits to members of the armed forces by providing high levels of situational awareness in hostile environments as well as helping to preserve the element of surprise in close encounters with an enemy. Details of the project appear in the most recent edition of IEEE Communications Magazine - one of the most authoritative international academic publications in the field. According to lead researcher, Dr Simon Cotton of CSIT's Radio Communications Research Group, it is the seventh article the team has published on the topic in leading academic journals since the beginning of 2009. "This is a major achievement and underlines the fact that the group is now a recognised international leader in the area of Body Area Networks (BANs). Our paper in IEEE Communications Magazine is also the first to be published on Body-to-Body Networks (BBNs)," says Dr Cotton."Through our work, we aim to overcome some formidable challenges as the proposed wireless devices will be expected to operate in a range of environments much more exacting than those encountered in civilian life. "Despite this, they still need to be extremely reliable, efficient and resilient to 'jamming' or interception and decryption by enemy forces "Our job is to help make them a reality by modelling how the devices would work in real life; how the signals would be transmitted to and from the body of each user and what types of antennas would be required to allow them to function properly."To do this, we are modelling specific combat scenarios using state-of-the-art animation normally used to create computer games."We believe that ultimately this work will lead directly to the development of new applications not only for the military but also for the emergency services and the sports and entertainment markets," adds Dr Cotton.NOTES TO EDITORS: William Scanlon is available for interview. Please telephone 028 90 97 3091 to arrange.CSIT (www.csit.qub.ac.uk/)Officially opened in September 2009, the £30 million Centre for Secure Information Technologies (CSIT) at Queen's University Belfast has been set up to exploit the university's international research expertise in high performance data and network security and intelligent surveillance. The Centre is one of the first Innovation and Knowledge Centres (IKCs) created in the UK. Funders include the Engineering and Physical Sciences Research Council and the Technology Strategy Board. In addition, to date, 20 organisations have committed to support CSIT's work over the next five years. They include industrial partners such as BAE Systems and Thales UK as well as government agencies and international research institutes.Radio Communications Research Group (www.ee.qub.ac.uk/radio/)The seven body-centric communications research papers published so far this year were written by researcher, Dr Simon Cotton of the Radio Communications Research Group at Queen's. To mark the achievement, Dr Cotton was presented with a special award for outstanding performance by Professor John McCanny, Head of the School of Electronics, Electrical Engineering and Computer Science at Queen's University Belfast.TechnologyThe Radio Communications Research Group project commissioned by the UK Ministry of Defence is based on devices operating at 60 GHz. These offer a number of distinct advantages over competing lower frequency technologies including increased stealth and reduced risk of interference. In addition, devices operating at this frequency enable greater miniaturisation. This facilitates the construction of wearable smart antenna arrays capable of electrically steering highly focused beams of electromagnetic energy in chosen directions. Similar low-cost 60 GHz technology is being employed for commercial applications such as high speed transfer of high definition television signals within the home.For further information, please contact Lisa McElroy, Tel: 028 90 97 5384 or email comms.office@qub.ac.uk
hboffin@eso.orgStellar family portrait takes imaging technique to new extremesNoted for harbouring Eta Carinae  one of the wildest and most massive stars in our galaxy  the impressive Carina Nebula also houses a handful of massive clusters of young stars. The youngest of these stellar families is the Trumpler 14 star cluster, which is less than one million years old  a blink of an eye in the Universe's history. This large open cluster is located some 8000 light-years away towards the constellation of Carina (the Keel).A team of astronomers, led by Hugues Sana, acquired astounding images of the central part of Trumpler 14 using the Multi-conjugate Adaptive optics Demonstrator (MAD, [2]) mounted on ESO's Very Large Telescope (VLT). Thanks to MAD, astronomers were able to remove most of the blurring effects of the atmosphere and thus obtain very sharp images. MAD performs this correction over a much larger patch of the sky than any other current adaptive optics instrument, allowing astronomers to make wider, crystal-clear images.Thanks to the high quality of the MAD images, the team of astronomers could obtain a very nice family portrait. They found that Trumpler 14 is not only the youngest  with a refined, newly estimated age of just 500 000 years  but also one of the most populous star clusters within the nebula. The astronomers counted about 2000 stars in their image, spanning the whole range from less than one tenth up to a factor of several tens of times the mass of our own Sun. And this in a region which is only about six light-years across, that is, less than twice the distance between the Sun and its closest stellar neighbour!The most prominent star is the supergiant HD 93129A, one of the most luminous stars in the Galaxy. This titan has an estimated mass of about 80 times that of the Sun and is approximately two and a half million times brighter! It makes a stellar couple  a binary star  with another bright, massive star. The astronomers found that massive stars tend to pair up more often than less massive stars, and preferably with other more massive stars.The Trumpler 14 cluster is undoubtedly a remarkable sight to observe: this dazzling patch of sky contains several white-blue, hot, massive stars, whose fierce ultraviolet light and stellar winds are blazing and heating up the surrounding dust and gas. Such massive stars rapidly burn their vast hydrogen supplies  the more massive the star, the shorter its lifespan. These giants will end their brief lives dramatically in convulsive explosions called supernovae, just a few million years from now.A few orange stars are apparently scattered through Trumpler 14, in charming contrast to their bluish neighbours. These orange stars are in fact stars located behind Trumpler 14. Their reddened colour is due to absorption of blue light in the vast veils of dust and gas in the cloud.The technology used in MAD to correct for the effect of the Earth's atmosphere over large areas of sky will play a crucial role in the success of the next generation European Extremely Large Telescope (E-ELT).Notes[1] Telescopes on the ground suffer from a blurring effect introduced by atmospheric turbulence. This turbulence causes the stars to twinkle in a way that delights poets but frustrates astronomers, since it smears out the fine details of the images. However, with adaptive optics techniques, this major drawback can be overcome so that the telescope produces images that are as sharp as theoretically possible, i.e. approaching conditions in space. Adaptive optics systems work by means of a computer-controlled deformable mirror that counteracts the image distortion introduced by atmospheric turbulence. It is based on real-time optical corrections computed at very high speed (several hundreds of times each second) from image data obtained by a wavefront sensor (a special camera) that monitors light from a reference star.[2] Present adaptive optics systems can only correct the effect of atmospheric turbulence in a very small region of the sky  typically 15 arcseconds or less  the correction degrading very quickly when moving away from the reference star. Engineers have therefore developed new techniques to overcome this limitation, one of which is multi-conjugate adaptive optics. MAD uses up to three stars instead of one as references to remove the blur caused by atmospheric turbulence over a field of view thirty times larger than that available to existing techniques (ESO PR 19/07 - http://www.eso.org/public/outreach/press-rel/pr-2007/pr-19-07.html).More InformationThis research has been presented in a paper submitted to Astronomy and Astrophysics ("A MAD view of Trumpler 14", by H. Sana et al.).The team is composed of H. Sana, Y. Momany, M. Gieles, G. Carraro, Y. Beletsky, V. Ivanov, G. De Silva and G. James (ESO). H. Sana is now working at the Amsterdam University, The Netherlands.ESO, the European Southern Observatory, is the foremost intergovernmental astronomy organisation in Europe and the world's most productive astronomical observatory. It is supported by 14 countries: Austria, Belgium, the Czech Republic, Denmark, France, Finland, Germany, Italy, the Netherlands, Portugal, Spain, Sweden, Switzerland and the United Kingdom. ESO carries out an ambitious programme focused on the design, construction and operation of powerful ground-based observing facilities enabling astronomers to make important scientific discoveries. ESO also plays a leading role in promoting and organising cooperation in astronomical research. ESO operates three unique world-class observing sites in Chile: La Silla, Paranal and Chajnantor. At Paranal, ESO operates the Very Large Telescope, the world's most advanced visible-light astronomical observatory. ESO is the European partner of a revolutionary astronomical telescope ALMA, the largest astronomical project in existence. ESO is currently planning a 42-metre European Extremely Large optical/near-infrared Telescope, the E-ELT, which will become "the world's biggest eye on the sky".LinksMore info: adaptive optics public web page - http://www.eso.org/public/astronomy/technology/adaptive_optics.htmlResearch paper  http://staff.science.uva.nl/~hsana/research_tr14.htmlContactsHugues SanaAmsterdam University, The NetherlandsPhone: +31 20 525 8496Email: H.Sana@uva.nlYuri BeletskyESO, ChilePhone: +56 5543 5311E-mail: ybialets@eso.orgEnrico MarchettiESO, GarchingPhone: +49 89 3200 6458E-mail: emarchet@eso.org
bspice@cs.cmu.eduCarnegie Mellon expands mobile learning project in IndiaNokia supports new phase of researchPITTSBURGHCarnegie Mellon University today announced the expansion of its Mobile & Immersive Learning for Literacy in Emerging Economies (MILLEE) project, which will gauge the effectiveness of mobile phone-based games for teaching English lessons to students in rural India. This new phase of the research project is made possible, in part, by a grant and a supply of 450 mobile phones provided by Nokia Research Center in Palo Alto, Calif.Even today's low-end mobile phones exceed the capabilities of the original IBM personal computer and thus could become important learning tools as wireless carriers extend their services into previously underdeveloped regions, said Carnegie Mellon's Matthew Kam, assistant professor in the School of Computer Science's Human-Computer Interaction Institute. He and his colleagues in the MILLEE project have spent six years designing educational games for mobile phones that are relevant to  the culture of rural India. During the next two years, MILLEE will conduct a controlled study involving 800 children in 40 villages in the state of Andhra Pradesh."With Nokia's assistance, we will be able to conduct field research that is more extensive and more rigorous than we could previously," Kam said. "Our previous, smaller studies have shown that students have significant gains in learning when they use these games. By aiming to replicate these results in a much larger study, we anticipate that we can understand how to design and develop phone-based games to improve educational prospects for billions of people throughout the developing world.""Nokia is pleased to contribute to the MILLEE project," said John Shen, lab director for the Nokia Research Center in Palo Alto. "The opportunities to use the mobile phone as an educational tool are unlimited, and Carnegie Mellon's vision in this area is very aligned with that of Nokia's."Kam and his students already are working on games that support a new English curriculum adopted by Andhra Pradesh schools this summer. Their goal is to have at least six and as many as a dozen games ready to deploy next year. If their field study is successful, they hope to establish partnerships with a growing number of content developers in India who are currently focused on educational programs for desktop computers."We're trying to build an entire ecosystem of educational content developers around mobile learning, which does not currently have a critical mass," Kam said. Shabnam Aggarwal, the project manager, will be relocating to India where she will oversee the research project and develop ties with content developers, wireless carriers and groups interested in implementing mobile learning programs. Aggarwal, who earned a degree in electrical and computer engineering at Carnegie Mellon, has experience on Wall Street and with non-governmental literacy programs in southeast Asia.Despite their small screens and low computing power by today's standards, mobile phones could become a major educational resource as wireless carriers and mobile phone manufacturers move aggressively to extend mobile phone penetration among rural consumers, Kam said. And if the educational benefits of mobile phones can be demonstrated convincingly, he added, those consumers will have an additional motivation for investing in mobile phone service, which will further spur mobile phone adoption.The MILLEE project currently focuses on lessons that improve English skills because English is a power language in India and much of the developing world. Students must learn English, Kam explained, if they ever hope to pursue higher education or aspire to be part of the middle class. Yet, in many areas of the developing world, schoolteachers struggle to speak English and have difficulty helping their students to speak the language. Leonora Anyango-Kivuva, a veteran language instructor who speaks seven languages and holds a doctorate in international and development education from the University of Pittsburgh, contributes to MILLEE's curriculum development efforts. Another collaborator is Maxine Eskenazi, associate teaching professor in Carnegie Mellon's Language Technologies Institute, who is an expert in both second language acquisition and speech technologies.Kam mentors a group of 20 students who comprise the backbone of the MILLEE project. They include Ph.D. student Derek Lomas who runs weekly design workshops and Ph.D. student Anuj Kumar who performs field trials with local partners. Graduate student Geeta Shroff conducts statistical analysis on mobile phone usage in rural India to identify improvements to the game designs. Computer science undergraduate Manoj Dayaram developed the foundational code for the upcoming games. Alex Kowalski, Gino Mancuso, Andrew Ngan, Daniel Rhim and Kyle Sondrock are developing three MILLEE games as their senior project in the Information Systems Program in the College of Humanities & Social Sciences; Raja Sooriamurthi, associate teaching professor, is their adviser. Anshul Chaurasia, Denny George, Chetan Gupta, Amol Jain, Manish Lohani, Nikhil Marathe and Kishan Patel  undergraduates at the Dhirubhai Ambani Institute of Information and Communication Technology in India  contribute to the game development and play testing as volunteers.In addition to the expansion in India, MILLEE is working with Sue-Mei Wu, chair of Chinese learning initiatives in the Pittsburgh Science of Learning Center, a joint effort of Carnegie Mellon and Pitt, to expand its work to underserved regions in China. Mobile phone games will be tested in China to see if they can improve literacy in Mandarin. MILLEE is also being expanded to Kenya, where the games will be used to teach English.For more information on MILLEE, visit http://www.millee.org/. About Carnegie Mellon: Carnegie Mellon (www.cmu.edu) is a private, internationally ranked research university with programs in areas ranging from science, technology and business, to public policy, the humanities and the fine arts. More than 11,000 students in the university's seven schools and colleges benefit from a small student-to-faculty ratio and an education characterized by its focus on creating and implementing solutions for real problems, interdisciplinary collaboration and innovation. A global university, Carnegie Mellon's main campus in the United States is in Pittsburgh, Pa. It has campuses in California's Silicon Valley and Qatar, and programs in Asia, Australia and Europe. The university is in the midst of a $1 billion fundraising campaign, titled "Inspire Innovation: The Campaign for Carnegie Mellon University," which aims to build its endowment, support faculty, students and innovative research, and enhance the physical campus with equipment and facility improvements.
natash.richardson@epsrc.ac.ukResearch puts police gun detectors a step closerScientists have developed a prototype scanner designed to help police identify criminals carrying guns and knivesScientists have developed a prototype scanner designed to help police identify criminals carrying guns and knives without putting themselves in the line of attack.The new technology uses electro magnetic  waves in order to pick up 'reflections' from concealed guns, gun barrels or knives without the need to be close to the subject. It also uses 'neural network' technology - as used in automatic number plate recognition systems - to identify the weapon and ignore everyday items carried by the subject. The new device is non-intrusive - no image of the subject's body is produced.  The research, funded by the Engineering and Physical Sciences Research Council (EPSRC), is sponsored and supported by the Metropolitan Police and the Home Office Scientific Development Branch.Catherine Coates, EPSRC head of innovation, said: "This represents a great return on public research investment. This device could save lives and free up valuable policing time currently taken up with gun and knife detection."Professor Nick Bowring, from Manchester Metropolitan University and lead investigator on the project said: "This is a world first and a success for British science. This device means UK police will be able to lead the way in accurate mobile gun and knife detection without putting themselves in the line of attack".Stuart Ibbotson, Metropolitan Police head of engineering, said:  "We are still at early stages and a way off deploying operational capability yet, but so far results are very encouraging.  "This kind of device would be of great service to officers, helping them to catch people carrying guns and knives without putting themselves in increased danger. It could also help to target stop and search to further increase its effectiveness."Tests are currently being carried out by the Metropolitan Police Operational Technology Department to see how the scanner could work in practice.  If successful, the finished product could be available to police forces within 2 years.The project also involved researchers from Manchester University, Newcastle University and Queen Mary University of London.  Notes for editors:  The Engineering and Physical Sciences Research Council (EPSRC) is the UK's main agency for funding research in engineering and the physical sciences. The EPSRC invests more than £740 million a year in research and postgraduate training, to help the nation handle the next generation of technological change. www.epsrc.ac.ukFor further information contact:Professor Nick Bowring, Manchester Metropolitan University.N.Bowring@mmu.ac.uk+44 161 2476271EPSRC Press Officepressoffice@epsrc.ac.uk01793 44 444404Out of hours: 0776 889 4281
uwe.krengel@iwes.fraunhofer.deLaunch of first operating system for smart grid home automationKassel. December 16, 2009   More than 40% of the final energy consumption in Germany is related to buildings. Heating, cooling, domestic hot water supply and the operation of electrical appliances are the areas with the highest demand  electric vehicles are expected to become increasingly important in this context. The Open Gateway Energy Management Alliance (OGEMA) provides an open software platform for energy management which links the customer's loads and generators to the control stations of the power supply system and includes a cus-tomer display for user-interaction. In this way end customers will be able to automatically observe the future variable price of electricity and shift energy consumption to times when the price is low. All developers and involved parties can turn their ideas for more efficient energy usage by automation into software for the gateway platform.Many people talk about saving energy, but when it actually comes down to it most peo-ple are not prepared to adjust the heating up and down according to room usage or to search for energy guzzlers within the house as this is all too cumbersome. As well as sav-ing energy, shifting energy consumption according to supply is also becoming more and more important. With an increasing share of wind power and photovoltaics as well as de-centralized generators such as combined-heat-and-power units, the large conventional power plants are decreasingly capable of controlling the electrical supply system. "Already today electricity is for free on the German Energy Exchange at times when large power plants have to be derated due to high feed-in from wind power. Using automated load-shifting, private households and small business should also benefit from such favorable electricity prices", explains Dr. Philipp Strauß, head of the Division of System Engineering und Grid Integration at the Fraunhofer Institute for Wind Energy and Energy System Technology (IWES) in Kassel, Germany.Together with partners from the Model City Mannheim project (i.a. MVV Energie and IBM Deutschland) and SmartHouse/SmartGrid (i.a. ECN, The Netherlands), Fraunhofer IWES is developing technology to assist the user in smarter energy consumption through taking over as much work for him or her as possible. In order to further develop and promote this concept the Open Gateway Energy Management Alliance (OGEMA) is being founded by Fraunhofer IWES and all interested parties are invited to participate. Similar to success-ful open source projects such as Linux or the web browser Firefox, anyone will be able to turn ideas into software for the gateway platform  also those not participating in the OGEMA Alliance. Similar to novel mobile phones, a multitude of applications (apps") are to be developed within a short period of time. These apps will cover the differing re-quirements of private households, super markets, small businesses as well as public insti-tutions such as schools and hospitals and help to tap potential for energy efficiency which is not accessed today.The developers of driver software for connecting the gateway to devices and energy sys-tems within the building as well as to the control stations of the energy suppliers can also use the open interfaces provided by OGEMA. Similar to the operating system on a PC, the gateway brings together applications and hardware. It also acts as a firewall between the private area of the customer on the one hand and the public internet and public energy supply networks on the other hand. It thus takes into account the need for data privacy and for security against external manipulation. "Shifting energy into times of high generation and saving energy should not only save money, but should also be fun", stresses Dr. David Nestle, head of the Decentralized En-ergy Management group at Fraunhofer IWES. For this reason OGEMA will open up a vari-ety of new opportunities for the user  e.g. controlling single heating radiators precisely over time following to the requirements of the users or adapting the operation of electri-cal appliances according to the generation of the customer's photovoltaic plant. The floor is again open to the ideas of developers and providers of applications.Fraunhofer IWES is currently developing a first version of the OGEMA software, which is to be made public and available for download on the OGEMA home page at the begin-ning of 2010 (http://www.ogema-alliance.org). Further information on the technical con-cept can already be found under this link. In the framework of the E-Energy research pro-gram, which is funded jointly by the German Ministry of Economics (BMWi) and the Ger-man Ministry of Environment (BMU), with a total budget of approx. 140 million euros, OGEMA was presented at the yearly E-Energy congress on November 26-27, 2009 in Ber-lin. OGEMA technology is also developed and used in the project SmartHouse/SmartGrid funded by the European Commission.Further InformationE-Energy-program & projects: http://www.e-energy.de/E-Energy-congress: http://www.e-energy.de/de/jahreskongress.php (in German)Project Model City Mannheim: http://www.modellstadt-mannheim.de/ (in German)Project SmartHouse/SmartGrid: http://www.smarthouse-smartgrid.eu/OGEMA-Alliance: http://www.ogema-alliance.orgFraunhofer IWESThe Fraunhofer Institute for Wind Energy and Energy System Technology IWES was founded on January 1, 2009 and consists of the former Fraunhofer Center for Wind-energy and Marine Technology CWMT in Bremerhaven and the Institute for Solar Energy Power Supply Systems Technology ISET e.V. in Kassel. Research areas cover the complete wind energy spectrum and the integration of all renewables into the energy supply sys-tem. At the moment around 220 scientists, employees and students are employed. The annual budget is currently approx. 15 million euros. Fraunhofer IWES is one of 60 Insti-tutes belonging to the Fraunhofer-Gesellschaft, which is the largest organization for ap-plied research in Europe.
kirchweger@salk.eduRising above the dinAttention makes sensory signals stand out amidst the background noise in the brainLA JOLLA, CAThe brain never sits idle. Whether we are awake or asleep, watch TV or close our eyes, waves of spontaneous nerve signals wash through our brains. Researchers at the Salk Institute for Biological Studies studying visual attention have discovered a novel mechanism that explains how incoming sensory signals make themselves heard amidst the constant background rumblings so they can be reliably processed and passed on. "We live with the illusion that our visual system processes all the information that is available in the visual scene in a single glimpse," says John H. Reynolds, Ph.D., an associate professor in the Systems Neurobiology Laboratory at the Salk Institute and senior author of the current study. "In reality, there is far too much detail in a typical scene for the visual system to take it in all at once. So our perception of the world around us is in a sense pieced together from what we pay attention to." Researchers had known for some time that paying attention to visual details increases the firing rate of neurons tuned for attended stimulus. Until now, it was assumed that these attention-dependent increases in neural activity were the primary cause of the improvement in perceptual discrimination that we experience when we focus a sensory stimulus.  The findings of the Salk researchers, published in the September 24, 2009 issue of the journal Neuron, reveal that the uptick in the firing rate is only a small part of the story. "What we found is that attention also reduces background activity," says postdoctoral researcher and first author Jude Mitchell, Ph.D. "We estimate that this noise reduction increases the fidelity of the neural signal by a factor that is as much as four times as large as the improvement caused by attention-dependent increases in firing rate.  This reduction in noise may account for as much as 80% of the attention story."When light hits the retina, visual information is translated into a cascade of nerve impulses sending signals deep into the brain. It is here, in the brain's visual cortex, which resides in the occipital lobe at the back of the skull, that these signals are interpreted and give rise to perception. But the visual system has limited capacity and cannot process everything that falls onto the retina. Instead, the brain relies on attention to bring details of interest into focus so it can select them out from background clutter. In their study, Reynolds, Mitchell, and former graduate student Kristy Sundberg asked whether attention, which so efficiently tunes out external distractions, does the same for the internal racket. Attention generally increases the firing rate of responsive neurons: The stronger the stimulus, the more impulses are sent per second, which improves the quality of the signal somewhat. "It's a little bit like turning up the volume from very low to high on a stereo," says Reynolds. "You are not hearing it very clearly at low volume not only because the signal is weak but because ambient noise is masking the stimulus. As you increase the volume, the signal becomes clearer."But even under the most controlled laboratory conditions, the responses evoked by identically repeated stimuli vary from trial to trial. "Neurons are very noisy computing devices," says Mitchell. "Each neuron receives input from thousands of neurons and needs to distinguish the incoming information from the background noise."If each neuron produced random noise that is independent from what its neighbor neuron is doing, the brain cell on the receiving end could simply pool all incoming signals and average out the noise. Reynolds compares it to diversifying risk in a stock portfolio: "If you have a portfolio of stocks whose prices vary independently, you can reduce fluctuations by dividing your investment among a large pool of stocks." Unfortunately, for neurons this option is off the table since most of the brain's background noise originates in waves of spontaneous nerve signals that undulate across a large population of brain cells. Says Mitchell, "These fluctuations can't be simply averaged out since they are shared across the neural population."  To extend the investment analogy, say you put your money into a pool of real estate investments. Your portfolio is subject to fluctuations in the real estate market  the correlated fluctuations in the values of individual investments  no matter how big the pool.  But an interesting thing happened when the researchers measured the activity of a large population of visual neurons in animals trained to play a simple video game that required rapt attention to a visual stimulus on the screen. The internal fluctuations or shared noise quieted down, increasing the visibility of the incoming sensory information.  "Attention is an essential part of perception," says Reynolds. "Brain disorders in which attention fails therefore have devastating effects. Gaining insight into the neural mechanisms of attention is essential if we are to understand the causes of these perceptual deficits and find ways to treat them. By revealing a major new attentional mechanism, Jude has taken a major step toward understanding the neural mechanisms of conscious awareness."This work was supported in part by the National Institutes of Health and the National Science Foundation. About the Salk Institute for Biological StudiesThe Salk Institute for Biological Studies is one of the world's preeminent basic research institutions, where internationally renowned faculty probe fundamental life science questions in a unique, collaborative, and creative environment. Focused both on discovery and on mentoring future generations of researchers, Salk scientists make groundbreaking contributions to our understanding of cancer, aging, Alzheimer's, diabetes, and cardiovascular disorders by studying neuroscience, genetics, cell and plant biology, and related disciplines.Faculty achievements have been recognized with numerous honors, including Nobel Prizes and memberships in the National Academy of Sciences. Founded in 1960 by polio vaccine pioneer Jonas Salk, M.D., the Institute is an independent nonprofit organization and architectural landmark.
dwayne.c.brown@nasa.govNASA spacecraft provides first view of our place in the galaxyWASHINGTON -- NASA's Interstellar Boundary Explorer, or IBEX,spacecraft has made it possible for scientists to construct the firstcomprehensive sky map of our solar system and its location in theMilky Way galaxy. The new view will change the way researchers viewand study the interaction between our galaxy and sun.The sky map was produced with data that two detectors on thespacecraft collected during six months of observations. The detectorsmeasured and counted particles scientists refer to as energeticneutral atoms.The energetic neutral atoms are created in an area of our solar systemknown as the interstellar boundary region. This region is wherecharged particles from the sun, called the solar wind, flow outwardfar beyond the orbits of the planets and collide with materialbetween stars. The energetic neutral atoms travel inward toward thesun from interstellar space at velocities ranging from 100,000 mph tomore than 2.4 million mph. This interstellar boundary emits no lightthat can be collected by conventional telescopes.The new map reveals the region that separates the nearest reaches ofour galaxy, called the local interstellar medium, from ourheliosphere -- a protective bubble that shields and protects oursolar system from most of the dangerous cosmic radiation travelingthrough space."For the first time, we're sticking our heads out of the sun'satmosphere and beginning to really understand our place in thegalaxy," said David J. McComas, IBEX principal investigator andassistant vice president of the Space Science and EngineeringDivision at Southwest Research Institute in San Antonio. "The IBEXresults are truly remarkable, with a narrow ribbon of bright detailsor emissions not resembling any of the current theoretical models ofthis region."NASA released the sky map image Oct. 15 in conjunction withpublication of the findings in the journal Science. The IBEX datawere complemented and extended by information collected using animaging instrument sensor on NASA's Cassini spacecraft. Cassini hasbeen observing Saturn, its moons and rings since the spacecraftentered the planet's orbit in 2004.The IBEX sky maps also put observations from NASA's Voyager spacecraftinto context. The twin Voyager spacecraft, launched in 1977, traveledto the outer solar system to explore Jupiter, Saturn, Uranus andNeptune. In 2007, Voyager 2 followed Voyager 1 into the interstellarboundary. Both spacecraft are now in the midst of this region wherethe energetic neutral atoms originate. However, the IBEX results showa ribbon of bright emissions undetected by the two Voyagers."The Voyagers are providing ground truth, but they're missing the mostexciting region," said Eric Christian, the IBEX deputy missionscientist at NASA's Goddard Space Flight Center in Greenbelt, Md."It's like having two weather stations that miss the big storm thatruns between them."The IBEX spacecraft was launched in October 2008. Its scienceobjective was to discover the nature of the interactions between thesolar wind and the interstellar medium at the edge of our solarsystem. The Southwest Research Institute developed and leads themission with a team of national and international partners. Thespacecraft is the latest in NASA's series of low-cost, rapidlydeveloped Small Explorers Program. NASA's Goddard Space Flight Centermanages the program for the agency's Science Mission Directorate atNASA Headquarters in Washington.The Cassini-Huygens mission is a cooperative project of NASA and theEuropean and Italian Space Agencies. NASA's Jet Propulsion Laboratoryin Pasadena, Calif., provides overall management for Cassini and theVoyagers for the Science Mission Directorate.To view the sky map and for more information about IBEX, visit:http://www.nasa.gov/ibexFor more information about other NASA science missions on the Web,visit:http://www.nasa.govVisuals from the NASA Science Update: http://www.nasa.gov/mission_pages/ibex/allsky_visuals.html
frazieef@wfu.eduAnts vs. worms:  New computer security mimics natureIn the never-ending battle to protect computer networks from intruders, security experts are deploying a new defense modeled after one of nature's hardiest creatures  the ant.Unlike traditional security devices, which are static, these "digital ants" wander through computer networks looking for threats, such as "computer worms"  self-replicating programs designed to steal information or facilitate unauthorized use of machines. When a digital ant detects a threat, it doesn't take long for an army of ants to converge at that location, drawing the attention of human operators who step in to investigate.The concept, called "swarm intelligence," promises to transform cyber security because it adapts readily to changing threats."In nature, we know that ants defend against threats very successfully," explains Professor of Computer Science Errin Fulp, an expert in security and computer networks. "They can ramp up their defense rapidly, and then resume routine behavior quickly after an intruder has been stopped. We were trying to achieve that same framework in a computer system."Current security devices are designed to defend against all known threats at all times, but the bad guys who write malware  software created for malicious purposes  keep introducing slight variations to evade computer defenses. As new variations are discovered and updates issued, security programs gobble more resources, antivirus scans take longer and machines run slower  a familiar problem for most computer users.Glenn Fink, a research scientist at Pacific Northwest National Laboratory (PNNL) in Richland, Wash., came up with the idea of copying ant behavior. PNNL, one of 10 Department of Energy laboratories, conducts cutting-edge research in cyber security.Fink was familiar with Fulp's expertise developing faster scans using parallel processing  dividing computer data into batches like lines of shoppers going through grocery store checkouts, where each lane is focused on certain threats. He invited Fulp and Wake Forest graduate students Wes Featherstun and Brian Williams to join a project there this summer that tested digital ants on a network of 64 computers. Swarm intelligence, the approach developed by PNNL and Wake Forest, divides up the process of searching for specific threats. "Our idea is to deploy 3,000 different types of digital ants, each looking for evidence of a threat," Fulp says. "As they move about the network, they leave digital trails modeled after the scent trails ants in nature use to guide other ants. Each time a digital ant identifies some evidence, it is programmed to leave behind a stronger scent. Stronger scent trails attract more ants, producing the swarm that marks a potential computer infection."In the study this summer, Fulp introduced a worm into the network, and the digital ants successfully found it. PNNL has extended the project this semester, and Featherstun and Williams plan to incorporate the research into their master's theses. Fulp says the new security approach is best suited for large networks that share many identical machines, such as those found in governments, large corporations and universities.Computer users need not worry that a swarm of digital ants will decide to take up residence in their machine by mistake. Digital ants cannot survive without software "sentinels" located at each machine, which in turn report to network "sergeants" monitored by humans, who supervise the colony and maintain ultimate control. 
kfb@mbari.orgNew robot travels across the seafloor to monitor the impact of climate change on deep-sea ecosystemsMOSS LANDING, CA  Like the robotic rovers Spirit and Opportunity, which wheeled tirelessly across the dusty surface of Mars, a new robot spent most of July traveling across the muddy ocean bottom, about 40 kilometers (25 miles) off the California coast. This robot, the Benthic Rover, has been providing scientists with an entirely new view of life on the deep seafloor. It will also give scientists a way to document the effects of climate change on the deep sea. The Rover is the result of four years of hard work by a team of engineers and scientists led by MBARI project engineer Alana Sherman and marine biologist Ken Smith.About the size and weight of a small compact car, the Benthic Rover moves very slowly across the seafloor, taking photographs of the animals and sediment in its path. Every three to five meters (10 to 16 feet) the Rover stops and makes a series of measurements on the community of organisms living in the seafloor sediment. These measurements will help scientists understand one of the ongoing mysteries of the oceanhow animals on the deep seafloor find enough food to survive.Most life in the deep sea feeds on particles of organic debris, known as marine snow, which drift slowly down from the sunlit surface layers of the ocean. But even after decades of research, marine biologists have not been able to figure out how the small amount of nutrition in marine snow can support the large numbers of organisms that live on and in seafloor sediment.The Benthic Rover carries two experimental chambers called "benthic respirometers" that are inserted a few centimeters into the seafloor to measure how much oxygen is being consumed by the community of organisms within the sediment. This, in turn, allows scientists to calculate how much food the organisms are consuming. At the same time, optical sensors on the Rover scan the seafloor to measure how much food has arrived recently from the surface waters.MBARI researchers have been working on the Benthic Rover since 2005, overcoming many challenges along the way. The most obvious challenge was designing the Rover to survive at depths where the pressure of seawater is about 420 kilograms per square meter (6,000 pounds per square inch). To withstand this pressure, the engineers had to shield the Rover's electronics and batteries inside custom-made titanium pressure spheres.To keep the Rover from sinking into the soft seafloor mud, the engineers outfitted the vehicle with large yellow blocks of buoyant foam that will not collapse under extreme pressure. This foam gives the Rover, which weighs about 1,400 kilograms (3,000 pounds) in air, a weight of only about 45 kilograms (100 pounds) in seawater.Other engineering challenges required less high-tech solutions. In constructing the Rover's tractor-like treads, the design team used a decidedly low-tech materialcommercial conveyor belts. After watching the Benthic Rover on the seafloor using MBARI's remotely operated vehicles (ROVs), however, the researchers discovered that the belts were picking up mud and depositing it in front of the vehicle, where it was contaminating the scientific measurements. In response, the team came up with a low-tech but effective solution: they removed the heads from two push brooms and bolted them onto the vehicle so that the stiff bristles would clean off the treads as they rotated.The team also discovered that whenever the Rover moved, it stirred up a cloud of sediment like the cloud of dust that follows the character "Pig-Pen" in the Charlie Brown comic strip. This mud could have affected the Rover's measurements. To reduce this risk, the engineers programmed the Rover to move very, very slowlyabout one meter (3 feet) a minute. The Rover is also programmed to sense the direction of the prevailing current, and only move in an up-current direction, so that any stirred-up mud will be carried away from the front of the vehicle.In its basic configuration, the Benthic Rover is designed to operate on batteries, without any human input. However, during its month-long journey this summer, the Rover was connected by a long extension cord to a newly-completed underwater observatory. This observatory, known as the Monterey Accelerated Research System (MARS), provided power for the robot, as well as a high-speed data link back to shore.According to Sherman, "Hooking up the Rover to the observatory opened up a whole new world of interactivity. Usually when we deploy the Rover, we have little or no communication with the vehicle. We drop it overboard, cross our fingers, and hope that it works." In this case, however, the observatory connection allowed MBARI researchers to fine tune the Rover's performance and view its data, videos, and still images in real time. Sherman recalls, "One weekend I was at home, with my laptop on the kitchen table, controlling the vehicle and watching the live video from 900 meters below the surface of Monterey Bay. It was amazing!"Later this fall, the Rover will be sent back down to the undersea observatory site in Monterey Bay for a two-month deployment. Next year the team hopes to take the Rover out to a site about 220 km (140 miles) offshore of Central California. They will let the Rover sink 4,000 meters down to the seafloor, where it will make measurements on its own for six months. The team would also like to take the Rover to Antarctica, to study the unique seafloor ecosystems there. The Rover may also be hooked up to a proposed deep-water observatory several hundred miles off the coast of Washington state.In addition to answering some key questions of oceanography, the Benthic Rover will help researchers study the effects of climate change in the ocean. As the Earth's atmosphere and oceans become warmer, even life in the deep sea will be affected. The Benthic Rover, and its possible successors, will help researchers understand how deep-sea communities are changing over time.Just as the rovers Spirit and Opportunity gave us dramatic new perspectives on the planet Mars, so the Benthic Rover is giving researchers new perspectives of a dark world that is in some ways more mysterious than the surface of the distant red planet.For an on-line version of news release, with accompanying images, please see: http://www.mbari.org/news/news_releases/2009/rover/rover-release.html
daniel.parry@nrl.navy.milNRL sensor observes first light(Washington, DC  Dec. 2, 2009)  The Special Sensor Ultraviolet Limb Imager (SSULI) developed by NRL's Spacecraft Engineering Department and Space Science Division, launched October 18, 2009 on the U.S. Air Force Defense Meteorological Satellite Program (DMSP) F18 (flight 18) satellite, observed first light on December 1, 2009. In a sample airglow profile (Figure 1) the spectral emission features in the data are clean and show no anomalies. "The SSULI team is very excited to continue with early orbit testing and begin the calibration and validation process with this instrument," said Andrew Nicholas, SSULI principal investigator, NRL Space Science Division.Offering global observations, that yield near real-time altitude profiles of the ionosphere and neutral atmosphere, over an extended period of time, SSULI makes measurements from the extreme ultraviolet (EUV) to the far ultraviolet (FUV) over the wavelength range of 80 nanometers (nm) to 170 nm with 1.5 nm resolution. SSULI data products, once calibrated and validated, will be used operationally at the Air Force Weather Agency (AFWA) as standalone operational data products and also as inputs into operational Space Weather models.The Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA. 
sue.knapp@dartmouth.eduDartmouth College researchers help set security standards for the InternetHANOVER, NH  Dartmouth researchers who were pioneers in Public Key Infrastructure (PKI)  a system that secures and authenticates computer communications  are now playing leading roles establishing Internet standards and guidelines for security. Secure Internet activity requires being able to prove who you are. Security experts agree that the traditional approach of passwords is not always effective. PKI and public key cryptography solve these problems, and Dartmouth researchers are leading the way in helping organizations deploy PKI. A new system developed at Dartmouth called PRQP, which stands for PKI Resource Query Protocol, is now in the pipeline with the Internet Engineering Task Force (IETF) to become the universal way to easily implement PKI-enhanced computing security. "PKI labors under the misconception that it's difficult," says Scott Rea, senior PKI architect at Dartmouth. "PKI is most successful when it runs under the covers or in the background." And that's what it does on a lot of commercial websites that accept credit card numbers, ensuring security behind-the-scenes using PKI or "certificate authority" technology.Dartmouth's Institute for Security, Technology, and Society (ISTS) has received funding from the Department of Homeland Security to explore ways to make PKI more user-friendly, for individuals and for businesses of all sizes. That's how PRQP was born. "PRQP, very simply, provides a more distributed system for PKI; it works in a way to get trustworthy references in order to verify the PKI certificates of individuals or servers," says Massimiliano "Max" Pala, research fellow with ISTS and the Open Certificate Authority Lab director. In other words, as PKI becomes ubiquitous, IT professionals need PQRP, which provides a standard way to operate PKI efficiently, and therefore ensures a consistent and robust measure of security. And, according to Pala and Rea, adoption of PKI is growing, and there is a deliberate program to bring more and more organizations into the PKI fold. Consortiums have been established, grouped around common themes, so that all members within each group can trust each other's PKI certificates. For example, there are eight organizations now in the Higher Education group, or "bridge," which includes colleges and universities. It's called HEBCA, which stands for Higher Education Bridge Certificate Authority, and Rea serves as director of the HEBCA Operating Authority and secretary of the HEBCA Policy Management Authority.There are also bridges for federal employees and contractors, pharmaceutical companies and researchers, and one for defense and aerospace companies and contractors. All four existing bridge organizations have formed a "federation" to trust everyone within these networks, and there are varying levels of security, because PKI is customizable. Among all four bridges, approximately 15 million certificates have been issued (mainly to individuals, but servers and other network devices can also carry certificates). That figure is expected to double in the next 12-18 months. At Dartmouth alone there are 34,000 active certificates and about 1,500 server certificates issued from the Dartmouth PKI. "It's rewarding to see the real-world impact that PKI researchers and practitioners like Scott and Max are having," says Sean Smith, associate professor of computer science and ISTS faculty affiliate. "It's also great to see the institutional support that Dartmouth gives to technological innovation  and in bringing this new technology to the higher ed community at large." Smith co-founded Dartmouth's PKI laboratory in 2000. Research Director of ISTS Denise Anthony sees the role of Dartmouth as one of mentor or parent when it comes to PKI and PRQP. "Dartmouth faculty members and researchers led by Sean Smith have been at the forefront of PKI technology for more than 9 years," says Anthony. "Our students, grad students, and post-docs have learned about this emerging technology since it was born. And we continue to be involved as PKI and PQRP go global and become the standard way to deploy inter-operable computing security." Anthony is also an associate professor and chair of sociology at Dartmouth. Dartmouth has a long history of pushing the computing envelope, from hosting the first demonstration of remote computing using standard phone lines in 1940 to convening the conference in 1956 that coined the term Artificial Intelligence to being the home of the birthplace of the BASIC computing language and the Dartmouth Time Sharing System. Dartmouth was also one of the first institutions of higher education to deploy a wireless network and converge computing, voice, and television on its data network. TIMELINE:2000 -ISTS is established at Dartmouth2000 -Prof. Sean Smith, Adjunct Prof. Ed Feustel, and Punch Taylor from Computing Services establish Dartmouth's PKI Lab with funding from Internet2 and AT&T 2002 -PKI Lab receives additional funding from the Mellon Foundation2002 -The first PKI Research Workshop is hosted by the National Institute of Standards and Technology (NIST). Sean Smith is the founding program chair, and this annual meeting continues to this day as the Symposium on Identity and Trust on the Internet.2003 -Dartmouth's PKI begins issuing certificates 2004 -Dartmouth receives EDUCAUSE funding to establish HEBCA 2004 -Sean Smith receives NSF CAREER Award; provides funding to work to bridge the gap between information infrastructure technology and people's trust requirements2005 Dartmouth Root Certificate Authority (CA) cross-certified with HEBCA2005 -HEBCA cross-certified with the Federal Bridge CA prototype2005 -PKI Lab teams with Sun Microsystems' OpenSolaris Project2006 -ISTS begins contributing to HEBCA funding2006 -The PQRP was first proposed by Massimiliano Pala to IETF 2007 -ISTS takes over management of the HEBCA operations 2007 -US Higher Education Root CA (USHER)  an Internet2 initiative, is created at Dartmouth2009 -Formation of the Four Bridges Forum (4BF) connecting all the current bridge CAs
maria.callier@afosr.af.milThe future of electricity may be found in environmentally friendly, thermoelectric cellsThe Air Force Office of Scientific Research and the National Science Foundation are funding research that may result in a military turbine aircraft that for the first time ever will produce its own electricity from exhaust heat generated from thermo electricity. Dr. Daryoosh Vashaee and a team of co-researchers at Oklahoma State University's Helmerich Advanced Technology Research Center in Tulsa are using thermo electric nanotechnology to investigate the conversion of waste heat into electricity. Up to this point, thermo electricity has not been used extensively beyond space and cooling applications because it could not be produced efficiently. However, the scientists' efforts in Oklahoma may soon change that and thermo electric technology may be heralded by the Air Force in a way that no other eco-friendly energy source has, because it has non- toxic emissions. Vashaee and his co-researchers are examining thermo electric versus infrared technology, which is what the Air Force is currently using. The latter requires liquid nitrogen to cool down the infrared cells. Thermo electricity, on the other hand, would not make that necessary and it would also be inexpensive. "The new thermo electric sensors also provide a means to make high performance infrared detectors that are structurally simple and small, suitable for being used in military missions," said Vashaee. Vashaee noted that the next step is to develop thermo electric modules that can be used for power generation for Air Force aircraft, solar, thermal cells and waste heat recovery systems used in industry. ABOUT AFOSR: The Air Force Office of Scientific Research (AFOSR), located in Arlington, Virginia, continues to expand the horizon of scientific knowledge through its leadership and management of the Air Force's basic research program. As a vital component of the Air Force Research Laboratory (AFRL), AFOSR's mission is to discover, shape, and champion basic science that profoundly impacts the future Air Force. 
julie.carter@csiro.auE-Noses: Testing their mettle against fly nosesCSIRO scientists have made a breakthrough in efforts to extend the sensory range of 'electronic noses'Scientists from CSIRO's Food Futures Flagship have made a breakthrough in efforts to extend the sensory range of 'electronic noses' (e-noses) by developing a system for comparing their performance against the much-superior nose of the common house fly.	"Although e-noses already have many uses  such as detecting spoilage in the food industry and monitoring air quality  they are not as discriminating as biological noses," according to CSIRO scientist, Dr Stephen Trowell.	"Our efforts to improve e-noses recently received a boost following our development of a new system which enables us to compare technical sensors with biological sensors.	"We looked at how the most common type of e-nose sensors  metal oxide or 'MOx' receptors  sample the air around them.  This is a critical factor in the performance of all noses.  We then compared it with the performance of odorant receptors from the common house fly, Drosophila.	"We already know that fly receptors, unlike most other bioreceptors, are not very specific. Even so, it really surprised us how much narrower the responses of the MOx sensors were than the biological ones.  We also found that the fly bioreceptors outperformed the MOx sensors in their levels of independence.  The fly seems to make a range of broadly tuned receptors that are independent of each other and human engineers haven't yet worked out how to do this.	"These results, published today in the science journal PLoS ONE, will help in the design of better e-noses and help us understand better how biological systems work," Dr Trowell said.	Bio-benchmarking approaches such as this could also be applied to other classes of electronic nose sensors. The CSIRO research team is looking to collaborate with developers of solid-state chemical sensors in the search for more effective devices.	This research is part of a much larger project developing an improved electronic nose, the Cybernose®, for use in the wine industry. Using insect receptors, the Cybernose will detect volatiles and contaminants in grapes and wine, thus allowing winemakers to improve their wines. When completed, the Cybernose will have wide application for detecting ripeness and spoilage in a range of foods as well as other applications such as detecting explosives.	The comparisons between the fly's receptors and those of the e-nose were made possible by recent descriptions of how odorant receptors function in Drosophila, which was the first insect to have its genome described. It was this new knowledge of the fly's genome that made the fly odorant receptor work possible.  	Amalia Z Berna, Alisha R Anderson, Stephen C Trowell. 2009. Bio-benchmarking of electronic nose sensors. PLoS ONE http://dx.plos.org/10.1371/journal.pone.0006406
ahfell@ucdavis.eduSecuring military wireless networksCreating secure, mobile wireless networks for the military is the aim of a $35.5 million, 10-year grant from the U.S. Army Research Laboratory to a group of universities including UC Davis. The research project, which is unclassified, will also likely have benefits in civilian use.The new grant will create the Communication Networks Research Center, one of four centers in a new Collaborative Technology Alliance for Network Science run by the Adelphi, Md.-based Army Research Laboratory.The project's overall goal is to conduct basic scientific research and create foundational theories on wireless networks that are capable of supporting a mix of highly mobile individual soldiers, ground vehicles, airborne platforms, unmanned aerial vehicles, robotics, and unattended ground sensor networks; that can withstand interference and jamming; and that can reconfigure themselves rapidly as needed.UC Davis researchers will focus on the science behind creating secure and trusted networks, said Prasant Mohapatra, professor and chair of computer science at UC Davis and principal investigator for the UC Davis portion of the project.Mohaptra said UC Davis' share of the total grant will likely be about $7 to $8 million.Security and trustworthiness is an emerging issue in wireless networks generally, Mohapatra said. Military applications are even more complex."How much do you trust sources? How much do you trust information that is relayed back to you? Are there intruders on your network listening to information or planting misinformation? These are problems we want to address," Mohapatra said.Penn State is the lead institution in the Communication Networks Research Center, with Rensselaer Polytechnic Institute, UC Davis, UC Santa Cruz and the City University of New York as general members. A number of other universities and BBN Technologies, a Cambridge, Mass.-based high technology company, will be involved as subcontractors.Other UC Davis faculty taking part in the project are Professor Karl Levitt and Associate Professor Felix Wu of the Department of Computer Science; Associate Professor Raissa D'Souza, Department of Computer Science and Department of Mechanical and Aeronautical Engineering; and Assistant Professor Qing Zhao, Department of Electrical and Computer Engineering.
lbrooks@rsna.orgSmart phones allow quick diagnosis of acute appendicitisCHICAGO  Radiologists can accurately diagnose acute appendicitis from a remote location with the use of a handheld device or mobile phone equipped with special software, according to a study presented today at the annual meeting of the Radiological Society of North America (RSNA). "The goal is to improve the speed and accuracy of medical diagnoses, as well as to improve communications among different consulting physicians," said the study's lead author, Asim F. Choudhri, M.D., fellow physician in the Division of Neuroradiology at Johns Hopkins University in Baltimore. "When we can make these determinations earlier, the appropriate surgical teams and equipment can be assembled before the surgeon even has the chance to examine the patient." Appendicitis, or inflammation and infection of the appendix, is a medical emergency requiring surgical removal of the organ. Undiagnosed or left untreated, the inflamed appendix will rupture, causing toxins to spill into the abdominal cavity and potentially causing a life-threatening infection. Appendicitis can occur at any age but is most common in people between the ages of 10 and 30, according to the National Institutes of Health.     Typically, a patient arriving at the emergency room with suspected appendicitis will undergo computed tomography (CT) and a physical examination. If a radiologist is not immediately available to interpret the CT images or if consultation with a specialist is needed, diagnosis is delayed, increasing the risk of rupture. Transmitting the images over a mobile device allows for instant consultation and diagnosis from a remote location. It can also aid in surgical planning."This new technology can expedite diagnosis and, therefore, treatment," Dr. Choudhri said.For the study performed at the University of Virginia in Charlottesville, CT examinations of the abdomen and pelvis of 25 patients with pain in the right lower abdomen were reviewed over an encrypted wireless network by five radiologists using an iPhone G3 equipped with OsiriX Mobile medical image viewing software. All of the patients had surgical confirmation or follow-up evaluations to confirm whether or not they had appendicitis. "The scans can be read in full resolution with very little panning, and the software allows the reader to zoom and adjust the contrast and brightness of the image," Dr. Choudhri said. "The radiologist is evaluating actual raw image data, not snapshots." Fifteen of the 25 patients were correctly identified as having acute appendicitis on 74 (99 percent) of 75 interpretations, with one false negative. There were no false positive readings. In eight of the 15 patients who had appendicitis, calcified deposits within the appendix were correctly identified in 88 percent of the interpretations. All 15 patients had signs of inflammation near the appendix that were correctly identified in 96 percent of interpretations, and 10 of the 15 had fluid near the appendix, which was correctly identified in 94 percent of the interpretations. Three abscesses were correctly identified by all five readers. "The iPhone interpretations of the CT scans were as accurate as the interpretations viewed on dedicated picture archiving and communication system (PACS) workstations," Dr. Choudhri said.Dr. Choudhri pointed out that patient privacy concerns would have to be addressed before any handheld mobile device could be considered practical for clinical use, but noted that this technique has great potential for improving emergency room care."We hope that this will result in improved patient outcomes, as evidenced by decreased rates of ruptured appendicitis, shorter hospital stays and fewer complications," he said.Co-authors are Thomas M. Carr III, M.D., Christopher P. Ho, M.D., James R. Stone, M.D., Ph.D., Spencer B. Gay, M.D., and Drew L. Lambert, M.D.	Note: Copies of RSNA 2009 news releases and electronic images will be available online at RSNA.org/press09 beginning Monday, Nov. 30.RSNA is an association of more than 44,000 radiologists, radiation oncologists, medical physicists and related scientists committed to excellence in patient care through education and research. The Society is based in Oak Brook, Ill. (RSNA.org)Editor's note: The data in these releases may differ from those in the printed abstract and those actually presented at the meeting, as researchers continue to update their data right up until the meeting. To ensure you are using the most up-to-date information, please call the RSNA Newsroom at 1-312-949-3233.For patient-friendly information on CT, visit RadiologyInfo.org. 
maria.callier@afosr.af.milAFOSR funds super-fast, secure computingAir Force Office of Scientific Research(AFOSR)-supported physicists at the University of Michigan are developing innovative components for quantum, or super-fast, computers that will improve security for data storage and transmission on Air Force systems. According to Professor Duncan Steel, lead researcher from the University of Michigan, the long-term goal of this basic research is to push the frontier of modern electronics and optics into the realm of quantum behavior, where more complex computing problems can be solved at faster speeds. To accomplish this goal, Steel and his team have started by exploring ways to optically create and maintain quantum coherence using a single electron or hole in a quantum dot structure. Maintaining a constant electrical charge for an extended period of time in a solid-state nanostructure, like a quantum dot, is likely key to long-term success. Steel's next challenge has been investigating how to manipulate the electrical charge to perform basic computing tasks. In this phase of his research, he has focused on "spin and phase" -- quantum properties of the electron and hole that can be used to transport and store information. By demonstrating well-defined "spin and phase," the researchers have been better able to control and maintain information. Building on this conceptual model, the team was able to move on to more complex applications needed for quantum computing. "State-of-the-art frequency, stabilized lasers and the advanced laser control system, developed with AFOSR support will make optical control possible from this time forth," said Steel. With their collaborators at the Naval Research Laboratory, the researchers have successfully learned how to optically control and measure the spin of an extra electron in a quantum dot. In the process of conducting this AFOSR-funded research, Steel and his co-researchers, including former AFOSR-funded researcher L.J. Sham at the University of California at San Diego, have also advanced the understanding of optical control. With a new knowledge of how "spin and phase" information is lost and how that loss can be reduced, they have been able to demonstrate an increase in quantum storage time. With the use of ultra-fast laser technology, this increase will allow for over a million quantum operations to take place before information is lost. "The most significant aspect of this milestone is there has been widespread concern that semiconductors, while extremely successful in modern electronics and photonics would not be successful in quantum computing if they lead to loss of coherence of the electron spin," said Steel. "Now the problem is, in principle, no longer an issue because of the research we have completed." By funding this collaborative research with the University of Michigan, AFOSR continues to promote scientific milestones for the Air Force. Note:Dr. Steel's research is also being supported by the Army Research Office, National Science Foundation, Office of Naval Research, Intelligence Advanced Research Projects Activity Laboratory for Physical Sciences and Defense Advanced Research Projects Agency. ABOUT AFOSR: The Air Force Office of Scientific Research (AFOSR), located in Arlington, Virginia, continues to expand the horizon of scientific knowledge through its leadership and management of the Air Force's basic research program. As a vital component of the Air Force Research Laboratory (AFRL), AFOSR's mission is to discover, shape, and champion basic science that profoundly impacts the future Air Force. 
presse@zv.tum.deSensor biochips could aid in cancer diagnosis and treatmentMunich researchers develop new test process for cancer drugsIt is very difficult to predict whether a cancer drug will help an individual patient: only around one third of drugs will work directly in a given patient. Researchers at the Heinz Nixdorf Chair for Medical Electronics at the Technische Universitaet Muenchen (TUM) have developed a new test process for cancer drugs. With the help of microchips, they can establish in the laboratory whether a patient's tumor cells will react to a given drug. This chip could help in future with the rapid identification of the most effective medication for the individual patient.Cancer is the second most common cause of death in the Western world. According to the German Cancer Research Center in Heidelberg, approximately 450,000 people develop cancer every year in Germany. Although the doctors who treat cancer have numerous cancer drugs at their disposal today, the treatment must be precisely tailored to the patient and the type of cancer in question to be as effective as possible. If it takes a second or third try to find a drug that works, the patient loses valuable time in which the tumor can continue to grow.In the future, miniature laboratories could provide the fast help required here. A lab-on-a-chip is a device -- made of glass, for example -- that is just a few millimeters across and has bioelectronic sensors that monitor the vitality of living cells. The chips sit in small wells, known as microtiter plates, and are covered with a patient's tumor cells. A robot changes the culture fluid in each well containing a chip at intervals of just a few minutes. The microsensors on the chip record, among other things, changes in the acid content of the medium and the cells' oxygen consumption; photographs of the process are also taken by a microscope fitted underneath the microtiter plate. All of the data merge in a computer that is connected to the system, and which provides an overview of the metabolic activity of the tumor cells and their vitality.The robots and microtiter plates are kept in a chamber which, through precisely regulated temperature and humidity, provides an environment similar to that of the human body, and which also protects the tumor cells against external influences that can falsify the test results.After the tumor cells have been able to divide undisturbed for a few hours, the robot applies an anti-cancer substance. If their metabolic activity declines over the next day or two, the active substance was able to kill the tumor cells and the drug is effective. Using the microchips, twenty-four active substances or combinations of active substances can be tested simultaneously in this way.The gain in time for the patient is not the only positive factor here. Dr. Helmut Grothe, a scientist from the Heinz Nixdorf Chair at the TUM, explains: "Treatment with an ineffective cancer drug sometimes leads to the development of resistance to other drugs in the patient." Such resistance on the part of the tumor cells can also be identified at an early stage with the help of the sensor chip.Another advantage of the system is its automation. The robot works faster and more accurately than any human could. Hence, the test results can be obtained quickly, which, in turn, saves on costs. Furthermore, the possibility of testing tumor cells with several active substances simultaneously facilitates the search for effective substances for individually tailored cancer treatment. Pharmaceutical companies may also be able to use the sensor chip in the development of new drugs in future.As part of another research project, the scientists at the Heinz Nixdorf Chair are also developing a sensor chip that is intended to control tumor growth. The chip, which would be implanted once in the vicinity of the tumor, could release cancer drugs or pain medication only when the tumor grows. The release of the active substances would be controlled by electrical impulses. This sensor system could be used in the treatment of inoperable tumors, for example pancreatic tumors.In the past, microchips have also been developed at the TUM's Heinz Nixdorf Chair for use in other research fields. For example, one such system is used in the analysis of the water quality of streams and rivers. In this case, a small pump removes samples at regular intervals from the water body and channels it via a pipe to a sensor chip on which algae have been planted. As algae are highly sensitive to toxic substances, they are particularly suitable for water analysis and react to the minutest impurities: their metabolic activity, which is measured on the basis of their oxygen production, declines in less than one minute in the presence of toxins. The data recorded by this sensor system are transmitted via mobile telephone to a computer that raises the alarm. The TUM research team won the E.ON Environmental Award, presented by the Bavarian energy company of the same name, for this system in 2008.Contact:Dr.-Ing. Helmut GrotheHead of Technology DevelopmentHeinz-Nixdorf Chair for Medical ElectronicsTechnische Universitaet MuenchenTel:  +49.89.289.22949E-mail:  grothe@tum.de
heike.mertsching@igb.fraunhofer.deArtificial liver for drug testsThis release is available in German.If you have hay fever, headaches or a cold, it's only a short way to the nearest chemist. The drugs, on the other hand, can take eight to ten years to develop. Until now animal experiments have been an essential step, yet they continue to raise ethical issues. "Our artificial organ systems are aimed at offering an alternative to animal experiments," says Professor Heike Mertsching of the Fraunhofer Institute for Interfacial Engineering and Biotechnology IGB in Stuttgart. "Particularly as humans and animals have different metabolisms. 30 per cent of all side effects come to light in clinical trials." The test system, which Professor Mertsching has developed jointly with Dr. Johanna Schanz, should in future give pharmaceutical companies greater security and shorten the path to new drugs. Both researchers received the "Human-centered Technology" prize for their work."The special feature, in our liver model for example, is a functioning system of blood vessels," says Dr. Schanz. "This creates a natural environment for cells." Traditional models do not have this, and the cells become inactive. "We don't build artificial blood vessels for this, but use existing ones  from a piece of pig's intestine." All of the pig cells are removed, but the blood vessels are preserved. Human cells are then seeded onto this structure  hepatocytes, which, as in the body, are responsible for transforming and breaking down drugs, and endothelial cells, which act as a barrier between blood and tissue cells. In order to simulate blood and circulation, the researchers put the model into a computer-controlled bioreactor with flexible tube pump, developed by the IGB. This enables the nutrient solution to be fed in and carried away in the same way as in veins and arteries in humans. "The cells were active for up to three weeks," says Dr. Schanz. "This time was sufficient to analyze and evaluate the functions. A longer period of activity is possible, however." The researchers established that the cells work in a similar way to those in the body. They detoxify, break down drugs and build up proteins. These are important pre-conditions for drug tests or transplants, as the effect of a substance can change when transformed or broken down  many drugs are only metabolized into their therapeutic active form in the liver, while others can develop poisonous substances.  The researchers have demonstrated the basic possibilities for use of the tissue models  liver, skin, intestine and windpipe. At the moment, the test system is being examined. Within two years it could provide a safer alternative to animal experiments.
brhea@aoptix.comAOptix Technologies and NuCrypt demonstrate physical-layer quantum encryption for the Air ForceCampbell, California  AOptix Technologies, Inc., (AOptix) (www.aoptix.com), a leading edge developer of ultra-high bandwidth laser communication solutions, and NuCrypt (www.nucrypt.net), a provider of technology for ultra-high security over optical communication networks, disclosed today the recent completion of a first-of-its-kind quantum encryption test over free space optical (FSO) links for the United States Air Force Research Laboratory (AFRL) located in Rome, New York, with funding provided by the Air Force Office of Scientific Research in Arlington, Va.Flying at 10,000ft with distances of up to 20km, AOptix provided long-distance 2.5 gigabit per second lasercom links for the series of encryption tests.  AOptix applied their patented bi-directional Adaptive Optics correction to the wavefront distortions, caused by atmospheric turbulence.  This high speed closed-loop control system insured link stability so that all of the encrypted light was inserted with minimal loss back into the fiber; a key fundamental element in the success of the tests.In previous FSO link tests without AOptix terminals, NuCrypt experienced dramatic power fades due to turbulence and pointing errors, representing a significant, if not impossible, challenge.  Teaming with AOptix provided a robust link solution that preserved the integrity of the encoded signal for this type of encryption over a wireless optical connection."This new capability adds to the impressive low probability of detection, low probability of interception (LPD/LPI) that only a lasercom link can provide.  NuCrypt has transformed physical-layer security for free space optical lasercom" says Dean Senner, President and CEO of AOptix Technologies.  "Lessons learned will be applied to much longer link distances in the future".NuCrypt's proprietary "AlphaEta" quantum-noise randomized, physical-layer encryption technology represents a new paradigm in ultra-secure, high data-rate optical communications.  Not only does AlphaEta bring elements of physics and traditional cryptography together to enhance security, it does so in a way that is robust and compatible with current optical communications infrastructure whether long distance fiber or FSO links. "It is a testament to the robust nature of the AlphaEta encryption method and the capabilities of the AOptix free-space optical terminals that they can be combined to create ultra-secure Gigabit-per-second air-to-ground optical links," says Professor Prem Kumar, Founder and CEO of NuCrypt. The AOptix wireless bi-directional optical terminals utilize a unique patented, single aperture, adaptive optics method of beam control to compensate for real-time atmospheric turbulence while maintaining lock between two terminals.  Video, voice and data is transmitted through the air over a single invisible, low power, eye-safe, FSO laser link.  About the companyAOptix Technologies is a privately funded company founded in 2000.  With core technology expertise in the application of advanced adaptive optics, they develop free space optical communications and iris biometrics based identification solutions for both government and commercial markets.  For additional information, please see www.aoptix.com.
Keirissa.Lawson@csiro.auPlugging into an electric vehicle revolutionA road trial of plug-in hybrid electric vehicles (PHEVs), which could one day end up in every Australian driveway, is underway.Over the next three months, staff from Victorian energy distributor SP AusNet will use the PHEVs for their daily drive to work and for leisure as part of CSIRO and SP AusNet trial.CSIRO engineers have modified the PHEVs to carry a 30Ah NiMH battery which is capable of holding a 6kw charge, and a battery charger, to allow the cars to plug into and charge with electricity from the grid or from on-site renewable energy sources.CSIRO Energy Transformed Flagship scientist Dr Phillip Paevere said the road trial is collecting extensive information on how the existing PHEV technology could be used for a new application: using the car as a large mobile battery which can be integrated and used in the home."The PHEVs have been fitted with instruments which will monitor the travel patterns of different users, and the residual battery power left in the car at the end of the day, which could be available for other uses," Dr Paevere said."When not needed, the parked car in the driveway could potentially become a large battery store and energy source for the house, running appliances or storing off-peak or surplus electricity generated from on-site renewable generators, such as solar panels."SP AusNet spokesperson, Sean Sampson, said the trial will also allow thorough analysis of what the electricity demands are likely to be when PHEVs are connected to the network for charging."The introduction of electric vehicles into the mainstream market could have a significant impact on the electricity network," Mr Sampson said. "They may also dramatically affect the output at residential and retail outlets and the forecasted growth of peak and base demands." The transport sector accounts for 14 per cent of Australia's total greenhouse gas emissions. PHEVs have the potential to reduce our emissions and may also provide a way to manage peak demand on the electricity grid.By controlling when PHEVs are recharging from the electricity network the burden of demand can be shifted. Furthermore, the car battery can be drawn upon to provide power during peak periods of demand, prevent blackouts when there is a network supply interruption and assist in maintaining the overall stability of the network.The road trial is the first phase in understanding the potential for using PHEVs in Australian homes.The PHEV technology will also be used in the home energy system of CSIRO's Zero           Emission House (AusZEH) project.The demonstration home will be open to the public in summer 2009.CSIRO initiated the National Research Flagships to provide science-based solutions in response to Australia's major research challenges and opportunities. The 10 Flagships form multidisciplinary teams with industry and the research community to deliver impact and benefits for Australia.Image available at: http://www.scienceimage.csiro.au/mediarelease/mr09-186.htmlFurther Information: Phillip Paevere, Energy Transformed FlagshipPh:03 9252 6220E:Phillip.Paevere@csiro.auSean Sampson, SP Ausnet	Ph:03 9625 0199 Background information available at:http://www.csiro.au/resources/Plug-In-Hybrid-Electric-Vehicle.htmlMedia Assistance:Keirissa Lawson, Energy Transformed Flagship	Ph:02 4960 6109Mb:0418282055E:Keirissa.Lawson@csiro.auwww.csiro.au
daniel.parry@nrl.navy.milNRL artificial intelligence team win prestigious video awardsResearchers at NRL's Navy Center for Applied Research in Artificial Intelligence, within the laboratory's Information Technology Division (ITD), received two top awards at the 21st International Joint Conference on Artificial Intelligence (IJCAI) held in California. Selecting from a cadre of 39 competitor videos, IJCAI awarded the NRL films with top honors in the categories of "Best Overall" and "Most Informative."       Since 2006, the Artificial Intelligence research community has held this prestigiously honored competition for videos documenting exciting artificial intelligence advances in research, education and application and that are accessible to a broad on-line audience. "We are very excited to have won not just one, but two awards in our first year entering this competition," said Dr. Greg Trafton, section head, NRL Intelligent Systems Section. "Both videos are extremely entertaining and display the top-notch research currently occurring in artificial intelligence and robotics at NRL". In the category of "Best Overall," the award went to "Casey's Quest: Transfer Learning for Adversarial Environments" by Kalyan Gupta, Matthew Molineaux and Philip Moore. The video describes recent research that ITD has conducted with members of Knexus Research Corporation and the University of Central Florida on the topic of transfer learningthe ability to leverage experience gained from one task to improve performance on a different task. In transfer learning, the software first learns the intent of an adversary in a multi-agent simulation game. It then uses this experience to assist in controlling friendly agents, and was shown to significantly increase scores for this task in comparison to the non-transfer agent, which was not provided with this experience. This has practicable application to the Navy by providing more realistic training scenarios for a variety of mission tasks and creating a more intelligent adversary in training simulators involving semi-automated forces.In the category of "Most Informative," the award went to "Robotic Secrets Revealed, Episode 001" by Anthony Harrison, Ben Fransen, Magdalena Bugajska and Greg Trafton. This video highlighted recent gesture recognition work and NRL's novel cognitive architecture, ACT-R/E. While set in a popular game of skill, this video illustrates several Navy relevant issues, including: computational cognitive architecture that allows autonomous function and integrates perceptual information with higher level cognitive reasoning; gesture recognition for shoulder-to-shoulder human-robot interaction; and anticipation and learning on a robotic system. Such abilities will be critical for future Naval Autonomous systems for persistent surveillance, tactical mobile robots and other autonomous platforms.NCARAI is engaged in research and development efforts designed to address the application of artificial intelligence technology and techniques to critical Navy and national problems. Research is directed toward understanding the design and operation of systems capable of improving performance based on experience; efficient and effective interaction with other systems and with humans; sensor-based control of autonomous activity; and the integration of varieties of reasoning as necessary to support complex decision-making. Both award winning videos may found via the Internet by entering either of the below links. Casey's Quest: http://videolectures.net/ijcai09_moore_cqtlfae/ Robotic Secrets Revealed: http://www.nrl.navy.mil/aic/iss/aas/CognitiveRobotsVideos.phpThe Naval Research Laboratory is the Department of the Navy's corporate laboratory. NRL conducts a broad program of scientific research, technology, and advanced development. The Laboratory, with a total complement of nearly 2,500 personnel, is located in southwest Washington, DC, with other major sites at the Stennis Space Center, MS; and Monterey, CA.
evincen2@kent.eduKent State receives $2.7 million NSF training grant for environmental aquatic resource sensingKent State University has been awarded a training grant in the amount of $2,756,719 by the National Science Foundation under its Integrative Graduation Education and Research Training (IGERT) program. This is the first IGERT grant to be awarded to Kent State. The grant, which is funded under the American Recovery and Reinvestment Act of 2009, runs through 2014.The grant funds an IGERT project that focuses on environment aquatic resource sensing (EARS). The purpose of the program is to train doctoral students in environment sensing to learn how to protect and sense things in aquatic environments. The training provided by this project will prepare graduate students for a variety of future careers relevant to freshwater resources."The use of sensing technology allows us to monitor and understand what's going on in our environment," said Laura Leff, professor and assistant chair of Kent State's Biological Sciences department and principal investigator of the program. "Humans are dependent on freshwater resources, and there is not much freshwater on the Earth's surface. There are many diverse threats that can impact our aquatic systems, and technology, such as sensors, allows us to ask questions we couldn't ask before."The EARS project is interdisciplinary and involves Kent State and Miami University. "We want students to collaborate across disciplines to get a real hands-on experience and business experience in terms of technology transfer," Leff said. "The project provides a unique opportunity to bring together people in sciences, business and technology, serving as a catalyst of new partnerships to form not just between the sciences, but also with the colleges of business and technology."A highly competitive program, Kent State was one of more than 400 pre-proposals that got narrowed down to approximately 100 proposals. Of those, only 25 were funded."This grant brings a lot of prestige to the university since it's a highly competitive award, endorsing the quality of our students and our sciences," said James Blank, chair of Kent State's Biological Sciences department. "This is a highly coveted award that will help transform graduate programs."Currently, three doctoral students from Kent State and three doctoral students from Miami University are participating in the EARS project. The students recently conducted a workshop at Lacawac Sanctuary in Pennsylvania where they tested sensors and collected data. They also will design an education outreach project that they will implement together, using what they are learning to educate students in local schools."We hope to stimulate interest in environmental science from school kids and the general public," Leff said."It is a real honor to join the ranks of other IGERT recipients and particularly to focus our efforts on issues related to our fresh water resources," said Dr. Timothy Moerland, dean of the College of Arts and Sciences at Kent State. "Through this program, Ph.D. candidates will build upon their knowledge of environmental science by designing sensors and sensor networks that can be used to analyze and manage data more effectively. These sensors will help detect potential pathogens and hazards in our water like never before. The EARS-IGERT research conducted at Kent State University and with our partners at Miami University will feed directly into the goals and objectives of the Great Lakes Restoration Initiative, which was also recently signed into law by the federal government."IGERT is an NSF-wide program intended to meet the challenges of educating U.S. doctoral scientists and engineers with the interdisciplinary background, deep knowledge in a chosen discipline and the technical, professional and personal skills needed for the career demands of the future. The IGERT program is intended to catalyze a cultural change in graduate education by establishing innovative new models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. For more information on IGERT, visit www.IGERT.org. For more information on the IGERT Environmental Aquatic Resource Sensing, visit http://bioweb.biology.kent.edu/igert/home.html.
shar.mckenzie@eurekanetwork.orgGraphical cockpit pilots way to higher security for telecommunications servicesStudies show that 80% of security problems in data systems occur because of bad enforcement of security  leading to poor protection of services and failures in data privacy. This can be a problem both from the service provider's point of view, where the entertainment distributor for example wants to be certain that the consumer pays to download a film, to the user who wants to be sure that personal details are protected when using a credit card to buy on line.Remembering to shut the door"While it was good to have well-designed security in projects, it was difficult to manage the security in complex infrastructures. Most problems arise because people forget to put the right rules on the firewall or to close access. The idea therefore was to develop a supervision tool that could check that security was properly deployed and controlled." explains BUGYO project leader Bertrand Marquet of Paris-headquartered telecommunications giant Alcatel-Lucent."We approached EUREKA as it is more focused on industrialisation than the EU Framework Programme. And we selected the EUREKA CELTIC Cluster as it is a framework that Alcatel-Lucent helped establish and is dedicated specifically to telecommunications research." CELTIC provided assistance on various levels  from helping to replace partners that had to leave during the project, to providing guidelines for the project management."We defined an operational methodology that can be applied practically by telecommunications operators and service providers to gain confidence in the security deployed to protect their services and the associated business revenues," says Marquet. "This methodology provides the foundation for the development and deployment of the security assurance framework."Taking a model approachThe result was a robust low complexity system that is able to observe the complex functioning of the service infrastructure itself  this was demonstrated on a voice over Internet protocol (VoIP) infrastructure. The BUGYO system provided a real-time overview of the security assurance status of the network and could also provide assistance in terms of alarms and compliance monitoring."The results of the BUGYO project will certainly be beneficial for end users even though they will not see the results directly," adds Bertrand. "Consumers will have a more secure service, well-secured infrastructures are essential when consulting a bank account or paying on line for example. The outcome is a means to ensure really secured services."
pberzins@stevens.eduNSF awards Wetzel and Lechler $144,000 for 2-year information security management study2-year project combines computer science and technology managementHOBOKEN, N.J.  The Division of Information & Intelligent Systems of the National Science Foundation (NSF) has awarded a two-year grant totaling $144,038 to two Stevens Institute of Technology researchers from different disciplines to study advanced problems of managing information security in an age of massive concentrations of sensitive private information and sophisticated mining and cross-referencing of personal data.Titled "EAGER: Quantifying Information Security Risks in Complex Systems at the Interface of Users, Policies, and Technologies," the proposal's Principal Investigator is Dr. Susanne Wetzel, an Associate Professor of Computer Science in the Schaefer School of Engineering & Science at Stevens, who specializes in Cybersecurity and who is also the Director of Stevens' Center for the Advancement of Secure Systems and Information Assurance. Her Co-Principal Investigator is Dr. Thomas Lechler, an Associate Professor in the Howe School of Technology Management at Stevens, who specializes in Entrepreneurship, Project Management and Innovation Management, and who is also Director for Academic Entrepreneurship Programs as Stevens.The PIs' proposal represents an "opportunity to seed a highly innovative interdisciplinary research project that has the potential for significant practical and theoretical impact for the management of information security  an area which is receiving more and more public attention. During the past decade, research in information security has expanded from a purely technical focus to a more general technology-economic focus. Despite its expansion, a multidisciplinary approach to understand and theoretically explain the interaction of security and economy within complex systems of partners is still missing," said Wetzel and Lechler.The principal objective of this proposed research is to develop an innovative interdisciplinary information security framework to optimize and substantially advance both its system information security and system productivity. "For example," the PIs continued, "consider a hospital that exchanges data records of patients with governmental data bases that  on the other hand  are accessed by insurance companies. Furthermore, hospitals directly exchange information with these insurance companies. This may allow an insurance company to combine and deduce information from different data sources that could pose a security threat which is not addressed by traditional security considerations. From a security economics perspective, the impact of information exchange between partners on their productivity has to be considered to understand the conditions under which partners will obey or violate information security policies."The proposed project provides the potential for high impact in substantially advancing research in information security as well as in management science. Although the project will address systems information security within the health care industry, its outcomes are expected to be applicable in other industries, e.g., defense. The cross-disciplinary nature of the proposed project is also expected to identify opportunities for interdisciplinary education.To learn more about the project, please contact Professor Wetzel at swetzel@stevens.edu or Professor Lechler at tlechler@stevens.eduAbout Stevens Institute of TechnologyFounded in 1870, Stevens Institute of Technology is one of the leading technological universities in the world dedicated to learning and research. Through its broad-based curricula, nurturing of creative inventiveness, and cross disciplinary research, the Institute is at the forefront of global challenges in engineering, science, and technology management. Partnerships and collaboration between, and among, business, industry, government and other universities contribute to the enriched environment of the Institute. A new model for technology commercialization in academe, known as Technogenesis®, involves external partners in launching business enterprises to create broad opportunities and shared value. Stevens offers baccalaureates, master's and doctoral degrees in engineering, science, computer science and management, in addition to a baccalaureate degree in the humanities and liberal arts, and in business and technology. The university has a total enrollment of 2,150 undergraduate and 3,500 graduate students with about 250 full-time faculty. Stevens' graduate programs have attracted international participation from China, India, Southeast Asia, Europe and Latin America. Additional information may be obtained from its web page at www.stevens.edu.For the latest news about Stevens, please visit www.StevensNewsService.com.
smguns@rit.eduManaging disasters with high-tech imaging could save livesRIT, University at Buffalo seek to improve emergency response with real-time informationThe debacle of Hurricane Katrina proved that scrambling for information during a disaster is no way to run an emergency response effort. Quick access to information is critical to saving life and property in the precarious hours following a disaster.Improving disaster response is one of the goals of the Information Products Laboratory for Emergency Response, a partnership between Rochester Institute of Technology and the University at Buffalo. The collaboration will foster research to improve disaster mitigation planning, real-time response and recovery efforts, and to create potential business opportunities for industry.The incubator, funded with $600,000 from the National Science Foundation, will focus on technology, policy and business-development and bring together university researchers, private sector service and product providers, and emergency response decision makers."The economic benefit of this initiative will be seen in the growth of disaster-related information products and workers who know how to use them," says Donald Boyd, RIT vice president of research and lead scientist on the project.RIT and UB are leading centers in remote sensing, or the use of airborne sensors or satellite imagery to capture data over large areas. The laboratory team has extensive experience in fire and flood research, and previous collaborations with local emergency response personnel. Matching the needs of emergency responders with information products that combine remote sensing imagery with geographical information systems is central to the lab's mission. Geospatial analysis technology can be used to show crisis managers what is happening during a disaster, where events are occurring and how they might develop over time.Information products developed at the RIT-UB lab will identify priority areas in disaster events through the use of digital elevation models of ground surfaces for flood-plain mapping using radar or light detection and ranging, or LIDAR, sensors, and multi-spectral infrared detection of fire and floods through long- and short-wave infrared sensing."By working with agencies, companies and end-users, IPLER allows us to leverage the broad expertise in disaster mitigation and response that is exemplified by the UB 2020 extreme events strategic strengthUB 2020 Strategic Strength on Extreme Events," says Chris Renschler, associate professor in the UB Department of Geography, leader of the UB team and research scientist of the National Center for Geographic Information and Analysis and at UB's MCEER, (formerly known as the Multidisciplinary Center for Earthquake Engineering Research). "An important goal of this project is ensuring that the new technologies that we develop in our research labs at RIT and UB will best meet the needs of the people in agencies and industries who are charged with protecting our communities during disasters and attempting to improve their resilience in the face of natural and man-made hazards."Researchers at the Information Products Laboratory for Emergency Response will hold a series of workshops starting in the fall to bring together disaster management experts and technology and service providers. Industry partners specializing in satellite and airborne sensors and analysis software tools are Kucera International, ImageCat Inc., DigitalGlobe and Pictometry International. These team members will share data sets for research and development and existing software tools. In turn, the companies will benefit from products developed at the lab and contacts with potential clients.Public sector partners will provide consultation for information product needs and requirements, and feedback on research. The U.S. Forest Service Remote Sensing Applications Center, New York State Office of Homeland Security, New York State Foundation for Science, Technology and Innovation and Monroe County Office of Emergency Management will provide feedback."The idea is that we go directly to end-users and ask them what they need in terms of disaster management products, and then go back to the sensing system and do the research and develop the algorithms that lead to the identified products," says Jan van Aardt, RIT co-principal investigator. "It's all about open flow of communication from end-user up to the level where we integrate systems and algorithms for operational purposes. "The first set of information products will focus on flood and wild land fire mapping using existing data collected by researchers at RIT's Chester F. Carlson Center for Imaging Science and UB's MCEER. Next year, the team will define another set of disasters based on user need."We could potentially go from fires to gas leak detection, environmental disasters, terrorist attacks," says Don McKeown, RIT research scientist and Information Products Laboratory for Emergency Response team member. "A lot of the tools you have in your remote-sensing tool box are the same; it doesn't matter what the application is.""Research won't gather dust on a shelf," adds van Aardt. "Research outputs will actually be used in operational environments. I think that's really exciting."Student research is another important aspect of the laboratory. The NSF grant will support an undergraduate in imaging science and in computer science, as well as a doctoral candidate. Working under Tony Vodacek, RIT co-principal investigator on the lab team, is Bin Chen, a doctoral candidate at RIT's Center for Imaging Science, who will provide the research backbone for product development."We're educating students who know something about the emergency response community," notes Vodacek. "Their potential jobs would be working for companies like the ones we're partnering with. In essence, they'd get scientists with the expertise so that they can do their own research."The Information Products Laboratory for Emergency Response team is led by RIT's Donald Boyd. Supporting members of the laboratory include remote sensing researchers at RIT: Jan van Aardt, Don McKeown and Tony Vodacek, and Jamie Winebrake, chair of science, technology and society/public policy; and geospatial analysis experts at UB: Chris Renschler and Ronald Eguchi, an MCEER affiliate from ImageCat Inc. of Long Beach, Calif.Rochester Institute of Technology is internationally recognized for academic leadership in computing, engineering, imaging technology, and fine and applied arts, in addition to unparalleled support services for students with hearing loss. Nearly 16,450 full- and part-time students are enrolled in more than 200 career-oriented and professional programs at RIT, and its cooperative education program is one of the oldest and largest in the nation.For two decades, U.S. News & World Report has ranked RIT among the nation's leading comprehensive universities. RIT is featured in The Princeton Review's 2009 edition of The Best 368 Colleges and in Barron's Best Buys in Education. The Chronicle of Higher Education recognizes RIT as a "Great College to Work For."The University at Buffalo is a premier research-intensive public university, a flagship institution in the State University of New York system and its largest and most comprehensive campus. UB's more than 28,000 students pursue their academic interests through more than 300 undergraduate, graduate and professional degree programs. Founded in 1846, the University at Buffalo is a member of the Association of American Universities. MCEER is UB's national center of excellence in advanced technology applications dedicated to reducing losses from earthquake and other hazards nationwide.
amdi@bas.ac.ukLasers from space show thinning of Greenland and Antarctic ice sheetsThe most comprehensive picture of the rapidly thinning glaciers along the coastline of both the Antarctic and Greenland ice sheets has been created using satellite lasers.  The findings are an important step forward in the quest to make more accurate predictions for future sea level rise.Reporting this week in the journal Nature researchers from British Antarctic Survey and the University of Bristol describe how analysis of millions of NASA satellite measurements* from both of these vast ice sheets shows that the most profound ice loss is a result of glaciers speeding up where they flow into the sea.  The authors conclude that this 'dynamic thinning' of glaciers now reaches all latitudes in Greenland, has intensified on key Antarctic coastlines, is penetrating far into the ice sheets' interior and is spreading as ice shelves thin by ocean-driven melt. Ice shelf collapse has triggered particularly strong thinning that has endured for decades. Lead author Dr Hamish Pritchard from British Antarctic Survey (BAS) says, "We were surprised to see such a strong pattern of thinning glaciers across such large areas of coastline  it's widespread and in some cases thinning extends hundreds of kilometres inland. We think that warm ocean currents reaching the coast and melting the glacier front is the most likely cause of faster glacier flow. This kind of ice loss is so poorly understood that it remains the most unpredictable part of future sea level rise."The scientists compared the rates of change in elevation of both fast-flowing and slow-flowing ice. In Greenland for example they studied 111 fast-moving glaciers and found 81 thinning at rates twice that of slow-flowing ice at the same altitude.They found that ice loss from many glaciers in both Antarctica and Greenland is greater than the rate of snowfall further inland.In Antarctica some of the fastest thinning glaciers are in West Antarctica (Amundsen Sea Embayment) where Pine Island Glacier and neighbouring Smith and Thwaites Glacier are thinning by up to 9 metres per year.Issued by the British Antarctic Survey Press OfficeAthena Dinar, Tel: +44 (0)1223 221 414; mobile: 07740 822229 email: amdi@bas.ac.uk; Linda Capper, Tel: ++44 (0) 1223 221448; mobile: 07714 233744 email: lmca@bas.ac.ukAuthors:Dr Hamish Pritchard, Tel: ++44 (0) 1223 221293; email: hprit@bas.ac.ukProfessor David Vaughan, Tel: ++44 (0) 1223 221643; email: dgv@bas.ac.ukNotes for Editors:Stunning broadcast-quality footage and stills of Antarctica, as well as location maps are available from the BAS Press Office as above.  Extensive dynamic thinning on the margins of the Greenland and Antarctic ice sheets by Hamish D. Pritchard, Robert J. Arthern, David G. Vaughan & Laura A. Edwards is published online this week in the journal Nature (Advanced Online Publication).The team used data from NASA's high-resolution ICESat (Ice, Cloud and Land Elevation Satellite). Launched in January 2003, ICESat was sent into a polar orbit to examine changes in the world's ice and land masses -- particularly ice mass balance -- as well cloud and aerosol height. The satellite's lasers have measured the surface elevation of the Earth's ice sheets with unprecedented accuracy. The satellite's repeated passes over both poles create a wide net of coverage, and contribute to a long-term time series of topographic changes.For the first time scientists can see the elevation change over the whole of the Antarctic Peninsula, and that glaciers in this area have the highest rates of thinning recorded in Antarctica or Greenland. A glacier  is a 'river of ice' that is fed by the accumulation of snow. Glaciers drain ice from the mountains to lower levels, where the ice either melts, breaks away into the sea as icebergs, or feeds into an ice shelf. Ice sheet  is the huge mass of ice, up to 4 km thick that covers bedrock in Antarctica or Greenland.  It flows from the centre of the continent towards the coast where it feeds ice shelves. Ice shelf  is the floating extension of the grounded ice sheet.  It is composed of freshwater ice that originally fell as snow, either in situ or inland and brought to the ice shelf by glaciers.  As they are already floating, any disintegration will have no impact on sea level. Sea level will rise only if the ice held back by the ice shelf flows more quickly onto the sea. The Cambridge-based British Antarctic Survey (BAS) is a world leader in research into global environmental issues. With an annual budget of around £45 million, five Antarctic Research Stations, two Royal Research Ships and five aircraft, BAS undertakes an interdisciplinary research programme and plays an active and influential role in Antarctic affairs. BAS has joint research projects with over 40 UK universities and has more than 120 national and international collaborations. It is a component of the Natural Environment Research Council. More information about the work of the Survey can be found at:  www.antarctica.ac.uk* 43 million satellite measurements of the Antarctic and 7 million of Greenland over a 5 year period (2003-2007)
sokabe@nips.ac.jpBladder cells feel stretchMolecular mechanism of sensing fullness of urineJapanese research group led by Prof. Makoto Tominaga and Dr. Takaaki Sokabe (National Institute for Physiological Sciences: NIPS), and Prof. Masayuki Takeda, Dr. Isao Araki and Dr. Tsutomu Mochizuki (Yamanashi Univ.), found that bladder urothelial cells have a sensor for stretch stimulation. Their finding was reported in the Journal of Biological Chemistry published on Aug 7, 2009.Bladder is known to release ATP that activates micturition reflex pathway during urine storage. However, it has been unknown how urothelial cells sense bladder distension. The research group examined the function of 'TRPV4' protein abundantly expressed in urothelial cells. The group developed a special apparatus to measure cell responses upon stretch stimulation, which mimics bladder distension.Upon stretch stimulation, robust Ca2+ influx and following ATP release were observed in urothelial cells. These phenomena were almost completely attributed to TRPV4 activation, since such responses were eliminated by a TRPV4 inhibitor and reduced in TRPV4-deficient urothelial cells.Dr. Sokabe said, "This is the first report to show that TRPV4 is a primal stretch-detector in urothelial cells. Given that TRPV4 is critically involved in the sensing mechanism in the bladder, development of chemicals modulating TRPV4 activity may be useful for treatment of bladder disorders such as overactive bladder and pollakiuria."
iratik@elhuyar.comTECNALIA presents innovative mobile robots which are autonomous and polyvalentTECNALIA Technological Corporation has introduced innovative robots at Euskotren's station in Atxuri (Bilbao) and which are mobile, multifunctional, collaborative, autonomous and polyvalent, suitable for a wide range of work from street cleaning and rubbish collection to accompanying elderly people. This new generation of robots is part of the European DUSTBOT research project under the remit of the VI European Framework Programme and in which TECNALIA is participating.These latest generation robots are suitable for the monitoring of large spaces (open and closed), as guides for persons in large shopping areas (indicating to them where a particular shop or product is within a shopping centre), for accompanying elderly people or those with certain disabilities (both at home and outside), thanks to their functions of orientation, navigation, communications with others or tele-assistance centres, etc. They can also be used as guides in teaching spaces (museums, visitor centres), and for transport, storage and transport and goods deliveries, besides the cleaning of both open and closed surfaces which have either difficult or easy access. This last function is the one which was recently the object of a public demonstration in the Bilbao rail station of Atxuri.DUSTBOT has collaborative, multifunctional and autonomous robots that are capable of operating in partially destructured environments/situations based on information provided by a map.The robots can also facilitate working in large areas, stations, airports and other types of public buildings, without being any obstacle for the activity of these places, given its reduced size, and without being a danger for members of the public, thanks to the novel system for the detection and avoidance of obstacles.The rail station of the Euskotren company in the Bilbao neighbourhood of Atxuri was chosen for the public presentation of these devices. The demonstration of two robot models was undertaken: the DustCart and the DustClean. The DustCart robot, measuring 1.45 metres high and 70 Kg in weight, has a humanoid form and is designed to interact with the user and for the collection of low demand waste. The DustClean robot, in the form of a small vehicle and measuring 96 cm high and 250 Kg in weight, cleans streets of dirt and dust. Moreover, both control the quality of air in real time."These robots are the solution for cleaning areas of difficult access and for the collection of rubbish at the very front door of, above all, persons who have mobility problems when moving the rubbish to the communal waste containers", stated Mr Iñaki Inzunza, Director of the Business Unit at the Tecnalia Technological Corporation.Leading the Locating, Navigation and Obstacle Avoidance work package within this European project, TECNALIA has also actively worked on the planning algorithms and correction of trajectories. As the Director of the Project, Ms Mercedes Ferros explained, "combining infrared sensors, ultrasound and a laser scanner, the perimeter of the robot is controlled and thus its trajectory corrected if necessary".This sensor technology of infrared sensors, ultrasound and a laser scanner was applied to detect obstacles, GPRS and ultrasound sensors for locating and nitrogen dioxide (NO2), ozone (O3) and carbon monoxide (CO) sensors to control the quality of air. To communicate with the robots, wireless systems were used between the sensors and the various modules (quality of air, locating, navigation, obstacle avoidance) that communicate using CAN with the supervisor PC in each robot. And for communications between the robots themselves and with the Intelligent Ambience nucleus, a WLAN network via ad-hoc mode and bluetooth connection were employed.Participating in the DUSTBOT project, initiated in December 2006, apart from TECNALIA, is the Scuola Superiore Sant' Anna (Italy), RoboTech srl (Italy), Midra (Italy), Synapsis (Italy), Örebro University (Sweden), HW Communication Ltd (United Kingdom), the Lucerne School of Engineering and Architecture (Switzerland) and the Haute Ecole d'ingénieur et de Gestion Vaud (Switzerland).
knroark@lanl.govScience at the petascale: Roadrunner results unveiledLOS ALAMOS, New Mexico, October 26, 2009The world's fastest supercomputer, Roadrunner, at Los Alamos National Laboratory has completed its initial "shakedown" phase doing accelerated petascale computer modeling and simulations of a variety of unclassified, fundamental science projects.The Roadrunner system is now beginning its transition to classified computing to assure the safety, security, and reliability of the U.S. nuclear deterrent.   Capitalizing on this national security investment, 10 unclassified projects were selected for this opportunity to use Roadrunner, a hybrid-architecture, 1.105 petaflop/s computing system, during a six-month period that ended in September 2009.   These projects were also used to put a "work load" on the Roadrunner system so that scientists could optimize the way large codes are able to run on the machine.  The Roadrunner open science projects represent the best of science, and the value of enabling technologies at Los Alamos, and were selected from across the Laboratory by a special committee.A sampling of the projects include:About Roadrunner, the world's fastest supercomputer, first to break the petaflop barrierOn Memorial Day, May 26, 2008, the "Roadrunner" supercomputer exceeded a sustained speed of 1 petaflop/s, or 1 million billion calculations per second.  "Petaflop/s" is computer jargonpeta signifying the number 1 followed by 15 zeros (sometimes called a quadrillion) and flop/s meaning "floating point operation per second."  Shortly after that it was named the world's fastest supercomputer by the TOP500 organization at the June 2008 International Supercomputing Conference in Dresden, Germany.The Roadrunner supercomputer, developed by IBM in partnership with the Laboratory and the National Nuclear Security Administration, will be used to perform advanced physics and predictive simulations in a classified mode to assure the safety, security, and reliability of the U.S. nuclear deterrent.  The system will be used by scientists at the NNSA's Los Alamos, Sandia, and Lawrence Livermore national laboratories.The secret to its record-breaking performance is a unique hybrid design. Each compute node in this cluster consists of two AMD Opteron dual-core processors plus four PowerXCell 8i processors used as computational accelerators. The accelerators used in Roadrunner are a special IBM-developed variant of the Cell processor used in the Sony PlayStation 3®. The node-attached Cell accelerators are what make Roadrunner different than typical clusters.Roadrunner is still currently the world's fastest with a speed of 1.105 petaflop/s per second, according to the TOP500 announcement at the November 2008 Supercomputing Conference in Austin Texas, and it again retained the #1 position at the June ISC09 conference. About Los Alamos National Laboratory (www.lanl.gov) Los Alamos National Laboratory, a multidisciplinary research institution engaged in strategic science on behalf of national security, is operated by Los Alamos National Security, LLC, a team composed of Bechtel National, the University of California, The Babcock & Wilcox Company, and the Washington Division of URS for the Department of Energy's National Nuclear Security Administration.Los Alamos enhances national security by ensuring the safety and reliability of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.
tjv@bio.unc.eduNESCent helps to build a global digital data network for biology and the earth sciencesDURHAM, NC - The National Evolutionary Synthesis Center is now part of a major new digital data initiative that will improve the ability of scientists, policymakers and the public to monitor the status of Earth's biota and the environment.Named DataONE (Data Observation Network for Earth), the initiative aims to provide secure and permanent access to data in biology and the earth sciences, including atmospheric, ecological, evolutionary, hydrological, and oceanographic sources."The goal is to provide access to data that are currently in many different institutions and in many different formats," said Dr. Ryan Scherle, who oversees NESCent's digital data efforts.Dozens of institutions have partnered to make DataONE a reality, including academic and government data centers, research libraries, citizen science programs, and international affiliates. "We have more than ten different repositories that are planning to join the DataONE network," said Scherle. "One of our largest challenges is getting the technology up to speed so that they can all communicate." "NESCent is a natural partner in this ambitious effort," said Dr. Todd Vision, Associate Director of Informatics at NESCent. "Evolutionary research provides unique insights into modern environmental challenges. Historic range expansions can be used to forecast the impacts of invasive species, and past responses of ecosystems to climate change can help us to predict what the range of consequences will be. But first, researchers need to be able to combine data from many different sources. "The initiative is supported by the U.S. National Science Foundation DataNet program, and is headquartered at the University of New Mexico. The project is slated to cost $20M over 5 years. NSF plans to establish five DataNets at this scale, and DataONE is one of the first two that have been funded.The National Evolutionary Synthesis Center (NESCent) is an NSF-funded collaborative research center operated by Duke University, the University of North Carolina at Chapel Hill, and North Carolina State University.For more on the NSF DataNet project, seehttp://www.nsf.gov/pubs/2007/nsf07601/nsf07601.htm
ghunka@aftau.orgGoing out on a limbTel Aviv University develops a 'scaffold' to regenerate lost or damaged bones and tissuesMother Nature has provided the lizard with a unique ability to regrow body tissue that is damaged or torn ― if its tail is pulled off, it grows right back. She has not been quite so generous with human beings. But we might be able to come close, thanks to new research from Tel Aviv University.Prof. Meital Zilberman of TAU's Department of Biomedical Engineering has developed a new biologically active "scaffold" made from soluble fibers, which may help humans replace lost or missing bone. With more research, she says, it could also serve as the basic technology for regenerating other types of human tissues, including muscle, arteries, and skin."The bioactive agents that spur bone and tissue to regenerate are available to us. The problem is that no technology has been able to effectively deliver them to the tissue surrounding that missing bone," says Prof. Zilberman. Her artificial and flexible scaffolding connects tissues together as it releases growth-stimulating drugs to the place where new bone or tissue is needed ― like the scaffolding that surrounds an existing building when additions to that building are made.Scientific peer-reviewed research on this scaffold fiber has appeared in a number of journals, including Acta Biomaterialia, and is currently being licensed through Ramot, TAU's technology transfer company.Active implantsThe invention, which does not yet have a name, could be used to restore missing bone in a limb lost in an accident, or repair receded jawbones necessary to secure dental implants, says Prof. Zilberman. The scaffold can be shaped so the bone will grow into the proper form. After a period of time, the fibers can be programmed to dissolve, leaving no trace. Her technology also has potential uses in cosmetic surgery. Instead of silicon implants to square the chin or raise cheekbones, the technology can be used to "grow your own" cheekbones or puffy lips. But Prof. Zilberman says it's far too early to think of such uses. She first started her work in biomaterials at the UT Southwestern Medical Center at Dallas, Texas, and currently is concentrating on various medical applications. One of them intends to make dental implants more effective. She envisions applying the invention to organ tissue regeneration in the future.A question of structure"Our material is very special," Prof. Zilberman explains. "The fibers not only support body parts like bones and arteries. They're also specially developed to release drugs and proteins in a controlled manner. Our special 3-D matrix can hold together drugs that are particularly vulnerable to breaking down easily. The matrix gives the body shape and form, coaxing it to re-grow and strengthen missing parts," she says. Until now in vitro results on bone have been good, and some basic unpublished results from  animal models have shown excellent promise for bone regeneration, says Prof. Zilberman. "It sounds simple, but it's not. It's quite difficult to develop a process for scaffold formation for bone growth. It's a delicate balance to apply only mild conditions that will not destroy the activity of the growth factor molecules."Currently Prof. Zilberman has developed both a fibrous artificial scaffold and an organic scaffold which forms a film.  The technology could also be applied to peripheral nerve regeneration. "Our fibers provide all the advantages that clinicians in tissue regeneration are calling for," says Prof. Zilberman. "Being thin, they're ideal when delicate scaffolds are called for. But they can also be the basic building blocks of bones and tissues when bigger structures are needed."	American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
cblesch@ur.rutgers.eduRutgers computer scientists work to strengthen online securityAuthentication research aims to make it harder to crack security questionsNEW BRUNSWICK, N.J.  If you forget your password when logging into an e-mail or online shopping Web site, the site will likely ask you a security question: What is your mother's maiden name? Where were you born?The trouble is that such questions are not very secure. More people than you may think will know your answers. And if they don't, it might not be hard to search for it online or even make a lucky guess.But Rutgers computer scientists are testing a new tactic that could be both easier and more secure."We call them activity-based personal questions," said Danfeng Yao, assistant professor of computer science in the Rutgers School of Arts and Sciences. "Sites could ask you, 'When was the last time you sent an e-mail?' Or, 'What did you do yesterday at noon?'"Yao and her students have been testing how resistant these activity questions are to "attack,"  computer security lingo for when an intruder answers them correctly and gains access to personal information such as e-mails or to do online shopping or banking.Early studies suggest that questions about recent activities are easy for legitimate users to answer but harder for potential intruders to find or guess, Yao said."We want the question to be dynamic," she said. "The questions you get today will be different from the ones you would get tomorrow."Rutgers doctoral student Huijun Xiong and visiting undergraduate student Anitra Babic are presenting the group's preliminary results in a workshop at this week's Association for Computing Machinery Conference on Computer and Communications Security. Babic is a senior at Chestnut Hill College in Philadelphia and participated in a summer research program at Rutgers.Yao said she gave four students in her lab a list of questions related to network activities, physical activities and opinion questions, and then told them to "attack" each other."We found that questions related to time are more robust than others. Many guessed the answer to the question, 'Who was the last person you sent e-mail to?' But fewer were able to guess, 'What time did you send your last e-mail?'"Yao explains that it should not be difficult for an online service provider to formulate these kinds of security questions by looking at its users' e-mail, calendar activities or previous transactions. Computers would have to use natural language processing tools to synthesize understandable questions and analyze the answers for accuracy.Yao is proposing further studies to determine the practicality of the new approach and the best way to implement it.Yao's work is funded in part by grants from the National Science Foundation.
evelyn.brown@nist.govOctober IT security automation conference to highlight health care IT, cloud computingThe Fifth Annual IT Security Automation Conference, co-hosted by the National Institute of Standards and Technology (NIST), will focus on emerging technologies designed to support the security automation needs of multiple sectors. The conference will be held Oct. 26-29 at the Baltimore Convention Center.This year's expanded conference includes several new conference tracks on the use of security automation in support of healthcare IT/Health Information Portability and Accountability Act (HIPAA); how security automation tools and technologies can ease the technical burdens of policy compliance; and how the rapidly evolving cloud computing sector can integrate security automation to achieve significant benefits. The first and last days are devoted to tutorials and workshops for novices and experts.This conference with workshops and an expo presents projects and integration efforts that facilitate the automation and standardization of computer vulnerability management, security measurement and compliance checking. In the past, these common challenges have been complicated by multiple proprietary methodologies and technologies that make it difficult to collect, correlate, remediate and report on mission-critical systems and data. Security automation reduces the complexity and time necessary to manage these functions, creating a secure and trusted computing environment that frees up resources to focus on other areas of the IT infrastructure.The conference is co-hosted by NIST, the Department of Homeland Security (DHS), the National Security Agency (NSA) and the Defense Information Systems Agency (DISA). The four agencies are actively involved in work to automate computer security.The IT Security Automation Conference is geared toward public and private sector senior executives, security managers and staff, information technology professionals and developers of products and services. Additional conference tracks include:Tutorial and workshops on DoC, DoD and DHS technologies and initiatives will be offered, such as:Online registration is available at http://scap.nist.gov/events, and there is a $100 early registration discount available until Oct. 5. Reporters interested in attending should contact Evelyn Brown (301) 975-5661.
jzverina@sdsc.eduSDSC dashes forward with new flash memory computer systemLarge-memory resource first of its kind among major HPC systemsLeveraging lightning-fast technology already familiar to many from the micro storage world of digital cameras, thumb drives and laptop computers, the San Diego Supercomputer Center (SDSC) at the University of California, San Diego today unveiled a "super-sized" version  a "flash" memory-based supercomputer that accelerates investigation of a wide range of data-intensive science problems.   The new High-Performance Computing (HPC) system, dubbed "Dash," is an element of the Triton Resource, an integrated, data-intensive resource primarily designed to support UC San Diego and UC researchers that went online earlier this summer. As envisioned, this "system within a system" will help researchers looking for solutions to particularly data-intensive problems that arise in astrophysics, genomics and many other domains of science. While Dash, which already has begun trial runs, is a medium-sized system as supercomputers go with a peak speed of 5.2 teraflops (TF), it has several unique properties, including the first use of flash memory technology in an HPC system, using Intel® High-Performance SATA Solid-State Drives.  Four of its nodes are specially configured as I/O nodes each serving up 1 terabyte (TB) of flash memory to any other node, courtesy of new I/O controllers also developed by Intel Corporation and integrated by Appro International, Inc. (One terabyte equals one trillion bytes of storage capacity).The system features 68 Appro GreenBlade servers with dual-socket quad-core Intel Xeon® processor 5500 series (formerly codenamed Nehalem) nodes linked to an InfiniBand interconnect. In its current configuration, Dash has 48 gigabytes (GB) of DRAM memory on each node, and employs vSMP Foundation software from ScaleMP, Inc. that provides virtual symmetric multiprocessing capabilities and aggregates memory across 16 nodes into shared memory "supernodes", giving users access to as much as 768 GB of shared DRAM memory in addition to 1 TB of flash memory per "supernode". "Dash's use of flash memory for fast file-access and swap space  as opposed to spinning discs that have much slower latency or I/O times  along with vSMP capabilities for large shared memory will facilitate scientific research," said Michael Norman, interim director of SDSC. "Today's high-performance instruments, simulations and sensor networks are creating a deluge of data that presents formidable challenges to store and analyze; challenges that Dash helps to overcome."For example, Dash will have the capability to search sky survey data for near-earth asteroids and brown dwarfs that may help researchers better understand periodic extinctions on Earth, and it will speed up investigations to establish relationships among species based on their genes. Such research could not only yield new information regarding evolution, but help biomedical researchers mine these complex data sets for clues to develop new drugs or cures for a variety of diseases. "Dash can do random data accesses one order-of-magnitude faster than other machines," said Allan Snavely, associate director at SDSC. "This means it can solve data-mining problems that are looking for the proverbial 'needle in the haystack' more than 10 times faster than could be done on even much larger supercomputers that still rely on older 'spinning disk' technology."Dash is currently being tested but soon will be made available to users of the TeraGrid, the nation's largest open-access scientific discovery infrastructure, for evaluation and development of application codes that can take advantage of flash memory and virtual "supernodes" technology. For additional information about access and allocations, see www.teragrid.org.About SDSCAs an organized research unit of UC San Diego, SDSC is a national leader in creating and providing cyberinfrastructure for data-intensive research. Cyberinfrastructure refers to an accessible and integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. SDSC recently doubled its size to 160,000 square feet with a new, energy-efficient building and data center extension, and is a founding member of TeraGrid, the nation's largest open-access scientific discovery infrastructure.
john.verrico@dhs.govTunnel visionBorder Patrol agents to spot tunnels with advanced ground-penetrating radarCriminals of all kinds are digging tunnels along the U.S. border at a fast and furious pace. Of every tunnel ever discovered by U.S. border patrol agents, 60 percent have been found in the last three years. Agents spot a new one every month."All of them have been found by accident or human intelligence," said Ed Turner, a project manager with the U.S. Department of Homeland Security (DHS) Science and Technology Directorate (S&T). "None by technology."To battle these secret burrows in the 21st century, S&T thinks this will have to change.  In partnership with Lockheed Martin, DHS S&T is pursuing a fresh approach that uses sophisticated  ground penetrating radar.  The Tunnel Detection Project is part of the Homeland Security Advanced Research Projects Agency (HSARPA), a distinct office within S&T set up to think out-of-the-box.  HSARPA invests in concepts offering the potential for revolutionary changes in homeland security technologies.  If successful, the tunnel detection technology will help agents locate and plug tunnels almost as fast as the criminals can dig them.While most tunnels are used to move drugs or people, they could also be used to move in weapons and explosives for a terrorist attack.  Tunnels are a serious challenge for border patrol agents because they can begin and end almost anywhere; their entrances and exits are often hidden inside old warehouses or under trees;  if old ones are discovered, new ones are quickly begun. Initially, S&T explored the possibility of an unmanned aircraft equipped with radar technology that would fly along the border searching for tunnels.  While this concept remains a goal, Department scientists and agents realize that most of the existing tunnels run through large urban centers where they are difficult to spot from satellite imagery. In addition, the airborne radar's radio frequency signals pose privacy concerns if they cross into someone's home.The new design technology is to place the radar antennas in a trailer that will be towed by a Border Patrol truck.  The antennas shoot a signal directly into the ground and use it to construct a multi-colored picture of the earth.  Tunnels show up as red, yellow, and aquamarine dots against a blue background.  Border patrols agents would see these images on a monitor mounted inside their truck.Ground penetrating radar is a promising technology because it is already used by civil engineers to reconstruct underground images.  These engineers, however, are usually only interested in detecting cables or pipes that may be a few meters beneath the earth.  S&T must find tunnels that often run much deeper. To find these, the radar uses much lower frequencies that penetrate the ground much better, and a sophisticated new imaging technology that can display clear pictures of deep tunnels. The Lockheed Martin team showed off an early scale model prototype this spring, mimicking the Southern U.S. border with large box filled with sand and rocks, and using pipes as tunnels.Next, they will send the technology to the Southwest this summer, where it will be tested against the rigors of the real life border. Separating tunnels from rocks, plants, and other objects along the ground or buried shallowly will be a key test."We want to develop something that can be used with high reliability so you'll find tunnels and not other things in the ground," said Turner.
ghunka@aftau.orgMoving video to 'captcha' robot hackersTel Aviv University uses a computer weakness to 'code' security images that only humans can seeWe see the popular "captcha" security mechanism often ― wavy letters websites ask us to type into a box. It's used by web pages and newsletter sign-up forms to prevent computer robots from hacking into servers and databases. But these codes, which are becoming increasingly complicated for an average person to use, are not immune to security holes. A research project led by Prof. Danny Cohen-Or of Tel Aviv University's Blavatnik School of Computer Sciences demonstrates how a new kind of video captcha code may be harder to outsmart. The foundation of the work, presented at a recent SIGGRAPH conference, is really pure research, says Prof. Cohen-Or, but it opens the door so security researchers can think a little differently."Humans have a very special skill that computer bots have not yet been able to master," says Prof. Cohen-Or. "We can see what's called an 'emergence image' ― an object on a computer screen that becomes recognizable only when it's moving ― and identify this image in a matter of seconds. While a person can't 'see' the image as a stationary object on a mottled background, it becomes part of our gestalt as it moves, allowing us to recognize and process it."A truly "emerging" technologyIn the new research paper, co-authored with colleagues in Taiwan, Saudi Arabia and India, Prof. Cohen-Or describes a synthesis technique that generates pictures of 3-D objects, like a running man or a flying airplane. This technique, he says, will allow security developers to generate an infinite number of moving "emergence" images that will be virtually impossible for any computer algorithm to decode."Emergence," as defined by the researchers, is a unique human ability to collect fragments of seemingly useless information, then synthesize and perceive it as an identifiable whole. So far, computers don't have this skill. "Computer vision algorithms are completely incapable of effectively processing emergence images," says Prof. Cohen-Or's faculty colleague Dr. Lior Wolf, a co-author of the study.The scientists warn that it will take some time before this research can be applied in the real world, but they are currently defining parameters that identify the "perception difficulty level" of various images that might be used in future security technologies. Finding Waldo in cyberspace"We're not claiming in our research paper that we've developed a whole new captcha technology," says Prof. Cohen-Or. "But we are taking a step towards that  something that could lead to a much better captcha, to highlight the big difference between men and bots. If it were to be turned into a solution, however, we wouldn't be able to give humans a multiple choice answer or common word answer for what they see, so we'll need to develop a way to use it. We have a few ideas in the works."The researchers are also developing methods of automatically generating "hidden" images in a natural background, like a pastoral mountain setting ― a digital "Where's Waldo?" game. "We're trying to hide images like eagles or a lion in mountainscape," says Prof. Cohen-Or. Because the moving image blends into a static background, it's hard for bots to understand what the human eye perceives with only minimal training."This could be a tough thing for a robot to crack, so we're working hard to make it practical," he emphasizes. "A good captcha has to be something that's easy for people but hard for a machine."American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
sarah.l.dewitt@nasa.govHand-held aerosol sensors help fill crucial data gap over oceansSince NASA researchers began assembling the Aerosol Robotic Network (AERONET) in the 1990s, the worldwide network of ground-based aerosol sensors has grown to 400 sites across seven continents. The trouble is that two-thirds of the planet is covered by ocean. And aerosols -- the tiny atmospheric particles that can have an outsized impact on the climate -- are just as likely to be found in the air above the oceans as they are over land.Yet aerosols are scarcely measured over the oceans. Alexander Smirnov, an AERONET project scientist at NASA's Goddard Space Flight Center, Greenbelt, Md., hopes to change that. Smirnov is leading a new effort called the Maritime Aerosol Network (MAN), which will send researchers with portable photometers on oceanographic research cruises. The hand-held devices can detect the presence of aerosols in air by measuring how light scatters as it strikes the particles. Taking the measurements is relatively easy: Several times a day, a researcher stands on a ship's deck when the sun is fully visible, points the instrument at the sun, and pushes a button. The photometer performs a series of scans within a few seconds. Finding "ships of opportunity" and volunteer scientists willing to take the measurements is not so easy. And transporting the photometers between the ships and Goddard for calibration can be a lengthy process.Even so, Smirnov has arranged to have the have handheld photometers carried aboard more than 50 vesselsboth commercial and researchfrom 12 countries since November 2006. Initial results show that data from the portable photometers correspond well with permanent AERONET stations on select islands.The initial efforts have produced a tantalizing observation. "Aerosol concentrations over the oceans at the high latitudes are not as high as satellite measurements suggest they should be," said Smirnov. This could be a fluke, given the relatively small number of ocean measurements so far. Or it could mean, as researchers suspect, that the satellite instruments and measurement methodologies should be improved. "We need to figure out why we're seeing this difference," said Smirnov. Unless scientists achieve greater confidence in aerosols measurements, predicting how climate in specific regions will respond to global temperature increases will remain difficult.	http://www.nasa.gov/topics/earth/features/aeronet_sidebar.html
ghunka@aftau.orgA police woman fights quantum hacking and crackingTel Aviv University researcher stays one step ahead of new threats to online securityThe first desktop computers changed the way we managed data forever. Three decades after their introduction, we rely on them to manage our time, social life and finances ― and to keep this information safe from prying eyes and online predators.So far, so good, despite an occasional breach. But our security and our data could be compromised overnight when the first quantum computer is built, says Dr. Julia Kempe of Tel Aviv University's Blavatnik School of Computer Science. These new computers, still in the theoretical stage, will be many times more powerful than the computers that protect our data now.Laying the groundwork to keep governments, companies and individuals safe, Dr. Kempe is working to understand the power of quantum computers by designing algorithms that fit them. At the same time, she is figuring out the limits of quantum computers, something especially important so we can build safety systems against quantum hackers.  "If a very rich person worked secretly to fund the building of a quantum computer, there is no reason in principle that it couldn't be used for malevolent power within the next decade," she says. "Governments, large corporations, entrepreneurs and common everyday people will have no ability to protect themselves. So we have to plan ahead."What quanta can't do"If we know what quantum computers will not be able to do, we can find 'windows' of protection for data," says Dr. Kempe, who is working on future programs that could keep data in quantum computers safe. Dr. Kempe recently published papers in Computational Complexity, the SIAM Journal on Computing and Communications in Mathematical Physics.Quantum mechanics allows a computer built on these principles, a so-called quantum computer, to perform tasks that are currently thought impossible to do efficiently on a normal computer, such as breaking current encryption standards. Adding it all upAlthough the most powerful quantum computer today barely has the computational capacity of a 4-bit calculator, it's just a matter of time until they are as powerful as physicists and mathematicians suspect they can be, Dr. Kempe says.Today's computer operates by manipulating 0s and 1s ― that is, a piece of data can be in one state or the other, but cannot be in both states simultaneously. In quantum computing, however, photons can be in the states 0 and 1 at the same time. This will give people and institutions phenomenally more computing power, but at the same time leave their data held in binary computers vulnerable to attack. "Today if you use a credit card it's encrypted. No matter who intercepts the data it would take forever to decode the numbers  even if all the computers we have today were wired together for the job," Dr. Kempe explains. A quantum computer, however, could crack the code quickly and efficiently."My basic research helps us better plan for the future when quantum computing is a reality," says Dr. Kempe, one of 23 new handpicked faculty recruits to Tel Aviv University. American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
iratik@elhuyar.comSafer space vehicles thanks to optic fiber sensorsThis release is available in Spanish.A research team from the TECNALIA Technological Corporation, through its Aerospace Unit, together with the ITEAM Institute at the Valencia Polytechnic University, the ilicitana Emxys aerospace company and the Institute for Photonic Sciences (ICFO), have developed a new protection system for the European Space Agency (ESA) through which safety for space vehicles is enhanced.The system involves monitoring based on optic fibre sensors that enable the visualisation of the real time temperature of elements making up the heat protection shield  one of the most sensitive parts where maximum temperatures are reached in reusable space vehicles (such as space shuttles)."The incorporation of our equipment in a manned vehicle significantly increases its levels of safety and reliability. With our technology, we can measure the integrity of the highly sensitive tiles of the thermal protection system (TPS) - on re-entering the atmosphere they may have to withstand temperatures of more than 800ºC for several minutes  and, therefore enhance their safety", explained those responsible for the project.The designed protection system means an important advance, being part of what is known as HMS (Health Monitoring Systems), in space vehicle structures and helps the safety elements to be much more robust in conditions of electromagnetic perturbations on launching and take-off or during incorporation into the launch pad. The nature of monitoring based on optic fibre sensors enables the placing of a great number of sensors on the structure of the shuttle, thus increasing the number of available points for the measurement of temperature, "something which, with traditional technologies, has not been viable".Moreover, very light devices are involved, directly facilitating the possible number of sensors to be installed, as they occupy little overall volume or weight.The development of this new system is fruit of an innovation project financed by the ESA and successfully developed over the last two years.The research team officially presented the project results on the 18 and 19 of November at the ESA installations in the Netherlands.
greenb@bnl.govBrookhaven Lab and Hybridyne Imaging Technologies Inc. win R&D 100 awardDeveloped compact gamma camera for imaging prostate cancerUPTON, NY  The U.S. Department of Energy's Brookhaven National Laboratory and Hybridyne Imaging Technologies, Inc., of Toronto, Canada, have won a 2009 R&D 100 Award for developing a compact gamma camera for high-resolution imaging of prostate cancer. The camera system, called ProxiScan, is a nuclear medical instrument that can localize cancer tissue in the prostate gland in detail at an early stage, which is important for the successful diagnosis and early treatment of the potentially deadly disease.R&D Magazine gives R&D 100 Awards annually to the top 100 technological achievements of the year. Typically, these are innovations that transform basic science into useful products. The 2009 awards will be presented on November 12, in Orlando, Florida."The Department of Energy's national laboratories are incubators of innovation, and I'm proud they are being recognized once again for their remarkable work," said Energy Secretary Steven Chu. "The cutting-edge research and development being done in our national labs is vital to maintaining America's competitive edge, increasing our nation's energy security, and protecting our environment. I want to thank this year's winners for their work and congratulate them on this award."The common way to diagnose prostate cancer  the second leading cancer among men, next to lung cancer  is through a blood test that measures the levels of a protein produced by the prostate gland called prostate-specific antigen, or PSA.  Elevated PSA levels may indicate prostate cancer, but with a high number of false-positive detections. Often, then, men must have an invasive biopsy, normally guided by ultrasound imagery. Other methods for confirming a diagnosis of prostate cancer include conventional nuclear medical imaging techniques, such as positron emission spectroscopy and single photon emission computed tomography.However, the current imaging methods have limitations. Benign and cancerous tumors cannot easily be distinguished by ultrasound, and fibrous tissues can be mistakenly identified as tumors if patients have had radiation treatment of the prostate previously. Traditional nuclear imaging systems produce lower-resolution images and are less efficient than Brookhaven's compact digital camera. Also, the detectors in current systems are too large to be used in trans-rectal probes. In contrast, the new cadmium zinc telluride (CZT)-based gamma camera is small enough for trans-rectal prostate cancer diagnosis, after the patient is injected with a tracer radiopharmaceutical. The high-resolution CZT detector is the cutting-edge technology that drives the novel system. Using this new technology, the working distance between the imaging system and the prostate gland is minimized, allowing urologists to obtain better images with a smaller amount of injected radioactive tracer, compared to conventional nuclear medical systems."This project has been a great opportunity to take gamma-ray detector technology originally developed for national security and apply it toward important societal goals in the area of cancer diagnosis and treatment," said Ralph James, a senior physicist at Brookhaven who is the Laboratory's principal co-inventor of the technology together with Brookhaven associate scientist Yonggang Cui. Cui added, "The CZT material combines the best aspects of conventional nuclear imaging detectors while minimizing their weaknesses. Our experience in detector development and electronics design has been critical in delivering this high performance system in a very compact package at a competitive cost."Although the CZT-based system was designed to reveal prostate cancer, it can be modified for imaging other cancers, such as cervical, uterine, colorectal and breast cancers. It can also be optimized for surgical use as a probe to guide the removal of cancerous tumors while minimizing damage to surrounding healthy tissues.CZT detectors have fostered the development of new instruments for measuring radiation. Numerous medical, industrial, scientific, environmental and homeland-security applications exist for this technology, including handheld instruments to reduce the trafficking of nuclear materials and portable field instruments for environmental monitoring and remediation.The U.S. Department of Energy's (DOE) Office of Nonproliferation Research and Development has been the principal sponsor funding the development of CZT detectors, and Hybridyne Imaging Technologies funded the design and engineering of the new compact gamma camera. The inventors have 16 patents on the technology, ranging from detector design and fabrication to imaging. Brookhaven Science Associates, the company that manages Brookhaven Lab, has a patent pending on the advanced CZT detectors.One of ten national laboratories overseen and primarily funded by the Office of Science of the U.S. Department of Energy (DOE), Brookhaven National Laboratory conducts research in the physical, biomedical, and environmental sciences, as well as in energy technologies and national security. Brookhaven Lab also builds and operates major scientific facilities available to university, industry and government researchers. Brookhaven is operated and managed for DOE's Office of Science by Brookhaven Science Associates, a limited-liability company founded by the Research Foundation of SUNY, for and on behalf of SUNY Stony Brook, the largest academic user of Laboratory facilities, and Battelle Memorial Institute, a nonprofit, applied science and technology organization. Visit Brookhaven Lab's electronic newsroom for links, news archives, graphics, and more (http://www.bnl.gov/newsroom), or follow Brookhaven Lab on Twitter (http://twitter.com/BrookhavenLab).
tkm@ece.psgtech.ac.inModified Bluetooth speeds up telemedicineA telemedicine system based on a modified version of the Bluetooth wireless protocol can transfer patient data, such as medical images from patient to the healthcare provider's mobile device for patient assessment almost four times as fast as conventional Bluetooth and without the intermittent connectivity problems, according to a paper in the forthcoming issue of the International Journal of Medical Engineering and Informatics.Telemedicine is a rapidly developing technology of clinical medicine where medical information is transferred via telephone, the internet or other networks for the purpose of consulting as a remote medical procedure. However, there are drawbacks to using direct connections between monitoring devices and the healthcare provider, not least the intermittency of standard connections.Now, T. Kesavamurthy and Subha Rani of the PSG College of Technology Peelamedu, in Coimbatore, India, have devised a dedicated embedded system that uses the short-range Bluetooth wireless networking protocol to connect patient data to the network and then on to the healthcare provider. This avoids the problem of trying to ensure that a viable connection between monitoring devices and the internet or cellular phone network is maintained constantly.The team has demonstrated a specific application of their technology which involves the transfer of patient medical images (CT scans) to the healthcare provider's personal digital assistant (PDA) device as an example of how Bluetooth might work for telemedicine."In medical imaging, picture archiving and communication systems (PACS) are computers in networks dedicated to the storage, retrieval, distribution and presentation of images," the team explains. However, PACS, which replaces hard-copy based means of managing medical images, such as film archives, cannot circumvent the connectivity issues associated with standard internet connections.The team has developed a system that can handle the digital imaging and communications in medicine (DICOM) standard for medical images and use it to produce compressible images that can be transferred readily using Bluetooth.The embedded system used in this project is an ARM based processor (AT91SAM9263), which is a 32 bit advanced embedded processor of the type commonly used in mobile data devices. "The design and implementation of an embedded wireless communication platform using Bluetooth serial communication protocol is proposed and problems and limitations are investigated," the team explains.The team adds that tests with DICOM images of approximately 1.5 megabytes can be transferred using their modified Bluetooth system in just 120 seconds, compared with 400 seconds for standard Bluetooth."DICOM medical image transmission using Bluetooth through ARM based processor for telemedicine applications" in Int. J. Medical Engineering and Informatics, 2010, 2, 52-71
ghunka@aftau.orgOpen source DNATel Aviv University finds a new solution to guarantee privacy and freedom in scientific researchIn the chilling science fiction movie Gattaca, Ethan Hawke stars as a man with "inferior genes" who assumes another's genetic identity to escape a dead-end future.  The 1997 film illustrates the very real fear swirling around today's genome research  fear that private genetic information could be used negatively against us.Last year, after a published paper found serious security holes in the way DNA data is made publicly available, health institutes in the United States and across the world removed all genetic data from public access. "Unfortunately, that knee-jerk response stymied potential breakthrough genetic research," says Dr. Eran Halperin of Tel Aviv University's Blavatnik School of Computer Sciences and Department of Molecular Microbiology and Biotechnology. He wants to put this valuable DNA information back in circulation, and has developed the tool to do it  safely. Working with colleagues at the University of California in Berkeley, Dr. Halperin devised a mathematical formula that can be used to protect genetic privacy while giving researchers much of the raw data they need to do pioneering medical research.  Reported in this month's issue of Nature Genetics, the tool could keep millions of research dollars-worth of DNA information available to scientists. New security to restart genetic research"We've developed a mathematical formula and a software solution that ensures that malicious eyes will have a very low chance to identify individuals in any study," says Dr. Halperin, who is also affiliated with the International Computer Science Institute in Berkeley.The mathematical formula that Dr. Halperin's team devised can determine which SNPs ― or small pieces of DNA -  that differ from individual to individual in the human population ― are accessible to the public without revealing information about the participation of any individual in the study.  Using computer software that implements the formula, the National Institutes of Health and similar institutes around the world can distribute important research data, but keep individual identities private. "We've been able to determine how much of the DNA information one can reveal without compromising a person's identity," says Dr. Halperin.  "This means the substantial effort invested in collecting this data will not have been in vain." Why is this information so important?  Genome association studies can find links in our genetic code for conditions like autism and predispositions for cancer. Armed with this information, individuals can avoid environmental influences that might bring on disease, and scientists can develop new gene-based diagnosis and treatment tools. A new track for government policymakersExamining SNP positions in our genetic code, Dr. Halperin and his colleagues demonstrated the statistical improbabilities of identifying individuals even when their complete genetic sequence is known.  "We showed that even when SNPs across the entire genome are collected from several thousand people, using our solution the ability to detect the presence of any given individual is extremely limited," he says. Dr. Halperin hopes his research will reverse the NIH policy, and he will provide access to the software so that researchers can use it to decide which genetic information can be safely loaded into a public database.  He also hopes it will quell raging debates about DNA usage and privacy issues. The Tel Aviv University-Berkeley research was done while Dr. Halperin was working with the International Computer Science Institute (ICSI), a non-profit research institute with close relations to the University of California (UC) and Tel Aviv University. Other coauthors of the study include Sriram Sankararaman, and Prof. Michael Jordan from UC, and Dr. Guillaume Obozinski from Willow, a joint research team between INRIA Rocquencourt, École Normale Supérieure de Paris and Centre National de la Recherche Scientifique.American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
aem1@psu.eduPenn State receives National Intelligence awardPenn State's College of Information Sciences and Technology was recently designated an Intelligence Community Center of Academic Excellence by the Office of the Director of National Intelligence paving the way for IST students to combat cyber security threats at the national level when they graduation.With the designation comes a two-year, $1 million grant from the ODNI with a possible three-year $1.5 million extension. The funds will be used to further Penn State's research in areas related to national security and intelligence, with the goal of producing students who can become leaders in the U.S. intelligence community.The IC-CAE program is a natural fit for the College of IST, which offers a bachelor's degree in security and risk analysis and a professional master's degree that includes an information assurance and decision support track.The award proposal was prepared by Henry C. "Hank" Foley, College of IST dean; Lt. Col. Ron Madrid (Ret.), director of Penn State's Office of Military and Security Programs, and Steven Peterson, director of the School of Public Affairs at Penn State Harrisburg."This project exemplifies the partnerships that the College of IST is forming both inside and outside of Penn State," said Foley. "The award will help us further our research and teaching goals at the graduate and undergraduate levels."Madrid said the IC-CAE program will provide intelligence agencies with qualified Penn State students."Intelligence services need to recruit and retain the best and brightest employees -- those with diverse ethnic, cultural and professional backgrounds as well as regional, geographical, industry, language and technical expertise -- to protect our citizens and lead this country in the 21st century," he said.In 2008, the College of IST was designated a National Center of Academic Excellence in Information Assurance Education and Research by the U.S. National Security Agency and the Department of Homeland Security. Penn State's Office of Military and Security Programs will administer the IC-CAE program. For more information, contact Madrid at 814-865-3911 or rrm11@psu.edu.
news@nas.eduSecurity of biological select agents and toxins A new report from the National Research Council, RESPONSIBLE RESEARCH WITH BIOLOGICAL SELECT AGENTS AND TOXINS, assesses the efficacy of regulations, procedures, and oversight that have been instituted to safeguard against the deliberate misuse of biological select agents and toxins (BSAT) used in research.  The report looks at security programs designed to protect against external threats, as well as internal threats from laboratory personnel.  The report also makes recommendations on refining security programs and procedures with the aim of informing policy discussions in the U.S. on balancing the security risks and benefits of BSAT research.Reporters may obtain copies of the report by contacting the National Academies' Office of News and Public Information, tel. 202-334-2138 or e-mail news@nas.edu. Advance copies will be available to reporters only starting at noon EDT on Tuesday, Sept. 29.  THE REPORT IS EMBARGOED AND NOT FOR PUBLIC RELEASE BEFORE 1:30 P.M. EDT ON WEDNESDAY, SEPT. 30.
ghunka@aftau.orgA real-time diagnosis for a treatable cancerTel Aviv University advances colorectal cancer screening with 'lab-on-a-chip' technologyAccording to the American Cancer Society, colorectal cancer, America's third leading type of cancer, is also one of the most preventable. One-third of all colorectal cancer deaths could be avoided by simple screening, they say. But colonoscopies, though highly effective, can also be painful, and current diagnostic techniques are time-consuming and sometimes inaccurate.Sefi Vernick, a doctoral student of the Department of Physical Electronics at Tel Aviv University, believes he has an answer that may lead to earlier diagnosis - and to saving lives. Utilizing the "lab-on-a-chip" technology first developed by his supervisor Prof. Yossi Shacham, Vernick attached a functioning miniature laboratory the size of a common computer chip to the end of the common endoscope used in colonoscopy examinations, providing a highly-accurate (and far less painful) biopsy done in real time. "What we're talking about is taking tiny little samples from polyps as the colonoscopy is being done, and getting the answer right away," says Vernick. "This tool allows us to both visualize and remove polyps and screen for cancer in real time. It's point-of-care diagnostics - we can do it in a physician's office, which is much more convenient than a hospital visit." "Bio"-Marks the Spot Colorectal cancer is especially difficult to diagnose in its early stages - usually, people are in advanced stages when the cancer is discovered, and the diagnostic process itself requires the removal of entire polyps as well as a laboratory assessment that may take weeks.Vernick's lab-on-a-chip solution works by recognizing tell-tale biomarkers that lab technicians cannot see with the naked eye. Cancer biomarkers are molecular changes detectable in the tumor or in the blood, urine, or other body fluids of cancer patients. These biomarkers are produced either by the tumor itself or by the body in response to the presence of cancer. The most commonly-used biomarker tests used today are the off-the-shelf pregnancy test and the test used by diabetics to monitor blood-sugar levels. With his tool, Vernick can scan up to four different biomarkers for colon cancer, an extraordinarily effective method for finding elusive colon cancer malignancies. The chip is essentially an electrochemical biosensor programmed to recognize and bind to colorectal cancer biomarkers with high specificity. "Following this bio-recognition event, the electrodes on the chip transduce the signal it receives into an electric current, which can be easily measured and quantified by us," says Vernick.Testing for Colon Cancer in the Living RoomIn addition to the lab-on-a-chip technology, Vernick and his fellow researchers believe they are well on the way to establishing a blood test for colon cancer, which, when used together with colonoscopies, offers a comprehensive package of colon cancer detection.  "When you combine all these methods together, you increase the level of confidence in the results, eliminating false positives and negatives which are dominant today in tests for colorectal cancer," says Vernick. This research, which is funded in part by American-Israeli businessman and philanthropist Lester Crown, is to be commercialized as a complete method of cancer detection, combining blood screening and biopsy. The ultimate goal would be for patients to have the ability to test themselves at home. "Glucose sensors used by diabetics are the best example today of a hand-held home biosensor test," says Vernick. In the future, he would like to offer patients a similar technology for colorectal cancer detection, in partnership with their physicians. "A person could submit the results of a home test directly online or to their doctor. This is my ultimate goal," he says. American Friends of Tel Aviv University (www.aftau.org) supports Israel's leading and most comprehensive center of higher learning.  In independent rankings, TAU's innovations and discoveries are cited more often by the global scientific community than all but 20 other universities worldwide.Internationally recognized for the scope and groundbreaking nature of its research programs, Tel Aviv University consistently produces work with profound implications for the future.
Robert.J.Gutro@nasa.govWarnings up for Philippines as Parma powers up to a super typhoon3 NASA satellites analyzing ParmaWarnings have been posted in the extreme northeastern Philippines as Parma has powered up into a Super Typhoon, and its new forecast track takes it over the northeastern tip of the Philippines, and three NASA satellites are keeping tabs on it.Public storm warning signal 1 is in force in Camarines Norte & Sur and Catanduanes, the Philippines. Overnight, Parma's sustained winds increased to 149 mph (130 knots), just 7 mph shy of a Category 5 typhoon. Right now, Thursday, October 1 at 0900 UTC (5 p.m. local Asia/Manila Time or 5 a.m. EDT) Parma is a strong Category Four Typhoon.The U.S. Navy's Joint Typhoon Warning Center forecast noted "Parma located approximately 520 nautical miles east-southeast of Manila, Philippines, near 12.7 North and 129.0 East. Parma has tracked west-northwestward at 18 mph (16 knots) over the past six hours.NASA's Aqua, Terra and Tropical Rainfall Measuring Mission satellites have been flying over Parma from space, providing valuable information on the storms clouds, temperature, rainfall, and more.NASA's Aqua satellite flew over Parma on September 30 at 12:59 p.m. EDT (16:59 UTC or 12:59 a.m. local Asia/Manila Time on October 1). Both infrared and microwave images were created from the Atmospheric Infrared Sounder (AIRS) instrument, and both showed very high, powerful thunderstorms, a sign that the storm was intensifying. Amicrowave image was created combining AIRS and Advanced Microwave Sounding Unit (AMSU) data. AMSU is another instrument that flies on NASA's Aqua satellite.The microwave image revealed cold areas in the storm that indicate ice in cloudtops, and heavy precipitation. Around the eye are the coldest cloud temperatures, as cold as -63F. Microwave data suggests cloud heights to the 200 millibar level, near the tropopause.NASA's Tropical Rainfall Measuring Mission (TRMM) satellite also flew over Parma one hour before Aqua, to get an idea of the rainfall that the storm is generating, and what it may be bringing to the Luzon area of the Philippines. TRMM is a joint mission between NASA and the Japanese space agency JAXA that can estimate rainfall in a tropical cyclone from its vantage point in space.TRMM has been providing valuable images and information on tropical cyclones around the tropics for almost 12 years since its launch back in November of 1997. Armed with a combination of passive microwave and active radar sensors, TRMM provides unique images of tropical cyclones, and they are created by the TRMM Team at NASA's Goddard Space Flight Center in Greenbelt, Md. It's a complicated process to make those images as rain rates in the center of the image are from the TRMM Precipitation Radar (PR), the only spaceborne radar of its kind, while those in the outer portion are from the TRMM Microwave Imager (TMI). The rain rates are then overlaid on infrared (IR) data from the TRMM Visible Infrared Scanner (VIRS) to create the entire image.When TRMM flew over Parma on September 30 at 1551 UTC (11:51 p.m. local Asia/Manila Time) it captured areas of heavy rainfall around the storm's eye. Most of the storm was seen dumping moderate rainfall between .78 to 1.57 inches per hour, however, in several areas around the eye heavy rain was falling at almost 2 inches per hour.NASA's Terra satellite flew over Parma on October 1 at 2:25 UTC (10:25 a.m. local Asia/Manila Time) and its Moderate Imaging Spectroradiometer (MODIS) instrument captured a visible image of Parma's clouds as the storm's center was approaching the Philippines, part of which are already under a part of Parma's clouds.Parma maintained intensity as a super typhoon over six hours (from 0300 UTC to 0900 UTC), and is forecast to further intensify prior to landfall on the northeastern coast of Luzon near 48 hours. After landfalling on the Luzon coast, computer forecast models take Parma toward China.Images: http://www.nasa.gov/mission_pages/hurricanes/archives/2009/h2009_Parma.html
m.gumadiaz@umiami.eduImproving security with face recognition technologyUniversity of Miami engineer presents novel methods for 3-D face recognition and for ear and face biometric systems at the 2009 IEEE International Conference on Image Processing in Cairo, Egypt, on Saturday, Nov. 7-Tuesday, Nov. 10CORAL GABLES-(November 10, 2009)-- A number of U.S. states now use facial recognition technology when issuing drivers licenses. Similar methods are also used to grant access to buildings and to verify the identities of international travelers. Historically, obtaining accurate results with this type of technology has been a time intensive activity. Now, a researcher from the University of Miami College of Engineering and his collaborators have developed ways to make the technology more efficient while improving accuracy.Mohamed Abdel-Mottaleb, professor and chair in the UM Department of Electrical and Computer Engineering has developed state-of-the-art systems capable of photographing an image of someone's face and ear and comparing it against pre-stored images of the same person, with 95-100 percent accuracy. Abdel-Mottaleb presented his findings at the 2009 IEEE International Conference on Image Processing in Cairo, Egypt on Saturday, November 7 - Tuesday, November 10. He describes his research as "satisfying, especially when you know that what you're doing has real-world applications that will benefit people and enhance personal security." The systems the researchers have designed can use 3-D facial images, or combine 2-D images of the face with 3-D models of the ear, which they construct from a sequence of video frames, to identify people by unique facial features and ear shapes.In the first method, the researchers use 3-D facial images with over 95 percent recognition rate, in the lab setting. Conventional shape matching methods commonly used in 3-D face recognition are time consuming. Abdel-Mottaleb uses a method that effectively increases computational efficiency while maintaining an acceptable recognition rate. He reduces the number of vertices (distinguishable landmarks of each face) considered when matching 3-D facial data, by automatically selecting the most discriminative facial regions. These automatically selected landmarks were found to be primarily within the regions of the nose, eye brows, mouth, and chin.The second method called "Multi-Modal Ear and Face Modeling and Recognition" obtains a set of facial landmarks from frontal facial images and combines this data with a 3-D ear recognition component-- a much more difficult identification process given the technique's sensitivity to lighting conditions. Fusing the scores of these two modalities, the researchers achieved an identification rate of 100 percent in the lab. "No single approach can give you 100 percent accuracy," Abdel-Mottaleb says. "One way to increase the accuracy is to use different biometrics and then combine them."These high-tech identification tools help fight crime, and enforce border security. In the future, the researchers hope to expand their techniques to faces demonstrating facial expressions and to recognize faces using only profile images.The University of Miami's mission is to educate and nurture students, to create knowledge, and to provide service to our community and beyond. Committed to excellence and proud of the diversity of our University family, we strive to develop future leaders of our nation and the world.  www.miami.edu
gabrielledemarco@gmail.comRensselaer to lead multimillion-dollar research center for social and cognitive networksTroy, N.Y.  With $16.75 million in funding from the Army Research Laboratory (ARL), Rensselaer Polytechnic Institute will launch a new interdisciplinary research center devoted to the study of social and cognitive networks. The Center for Social and Cognitive Networks is part of the newly created Collaborative Technology Alliance (CTA) of the ARL, which includes a total of four nationwide centers focused on different aspects of the emerging field of network science. The Rensselaer center will be headed by Boleslaw Szymanski, Rensselaer's Claire & Roland Schmitt Distinguished Professor of Computer Science. Rensselaer will receive $8.6 million of the $16.75 million in total funding to lead the new center for its first five years. An additional $18.75 million is anticipated from the ARL for a second phase, which would bring the total funding for the interdisciplinary center to $35.5 million over 10 years.Rensselaer will be joined by corporate and academic partners from IBM Corp., Northeastern University, and the City University of New York, and collaborators from Harvard University, Massachusetts Institute of Technology, New York University, Northwestern University, the University of Notre Dame, the University of Maryland, and Indiana University.  "Rensselaer offers a unique research environment to lead this important new network science center," said Rensselaer President Shirley Ann Jackson. "We have assembled an outstanding team of researchers, and built powerful new research platforms. The team will work with one of the largest academic supercomputing centers in the world  the Rensselaer Computational Center for Nanotechnology Innovations  and the leading visualization and simulation capabilities within our new Experimental Media and Performing Arts Center. The Center for Social and Cognitive Networks will bring together our world-class scientists in the areas of computer science, cognitive science, physics, Web science, and mathematics in an unprecedented collaboration to investigate all aspects of the ever-changing and global social climate of today." "A unique feature of this center will be its interdisciplinary approach," said Rensselaer Provost Robert Palazzo. "It has been a pleasure to work with Professor Szymanski and the faculty in drawing upon the strengths of the Rensselaer academic disciplines that will contribute to building knowledge in this important emerging field. The center will capitalize on the platforms President Jackson has put in place to support new levels of interdisciplinary research.""Together with other centers of the CTA, we are creating the new discipline of network science," said Szymanski. "The centers will be in the leading position to define this new discipline in all its complexity. Rensselaer researchers are very pleased to be a leading part of this transformation."The Center for Social and Cognitive Networks will link together top social scientists, neuroscientists, and cognitive scientists with leading physicists, computer scientists, mathematicians, and engineers in the search to uncover, model, understand, and foresee the complex social interactions that take place in today's society. All aspects of social networks, from the origins of adversarial networks to gauging the level of trust within vast social networks, will be investigated within the center.The center will enable stronger and more closely integrated collaborations among the team of top interdisciplinary researchers in the emerging field of network science that already existed informally, according to Szymanski. "I explored those earlier links and collaboration when organizing the team for the center," he said. "The impact of our work will be far-reaching. We are in an entirely new world where Twitter, cell phones, and wireless communication change the way we interact with each other. Together and with the support of the ARL, the researchers in the center will be able to investigate how technology enhances social interactions and how those technologies and relationships can be used to better measure and understand people's interactions with each other." Several Rensselaer faculty will take part in the center research. Szymanski will be leading the interdisciplinary team that includes Senior Professor of the Tetherless World Research Constellation and head of Information Technology James Hendler; Professor of Cognitive Science and Acting Head of the School of Humanities, Arts, and Social Sciences Wayne Gray; Associate Professor of Computer Science Adali Sibel; Associate Professor of Computer Science Malik Magdon-Ismail; Professor of Computer Science Mark Goldberg; Professor of Mathematical Sciences Chjan Lim; Professor of Decision Sciences & Engineering Systems William Wallace; Associate Professor of Physics, Applied Physics, and Astronomy Gyorgy Korniss; and Research Associate Professor of Cognitive Science Michael Schoelles.The center will study the fundamentals of social and cognitive networks and their roles in today's society and organizations, including the U.S. Army. The goal will be to gain a deeper understanding of these networks and build a firm scientific basis in the field of network science. The work will include research on large social networks, with a focus on networks with mobile agents. An example of a mobile agent is someone who is interacting (e.g., communicating, observing, helping, distracting, interrupting, etc.) with others while moving around the environment. The U.S. Army and the societies within which it operates are primary examples of such networks, according to Szymanski.Five topics will be the focus of the center's research. One will be dynamic processes in networks. Today's modern societies are supported by organically evolving network structures, which contribute to the transport and storage of various entities, including materials, energy, information, and people across vast time and space. At the same time, technological advances provide tools to better monitor social interactions and also influence social networks by providing novel ways for humans to interact, Szymanski said. With this in mind, the researchers will work to understand both the human interactions and the underlying technological infrastructure they utilize. To do this, the researchers will combine theoretical and computational tools from the disciplines of sociology, political science, computer science, mathematics, and physics.A second area will study organizational networks and how knowledge, particularly in the Army, is spread from peer to peer in the modern military. Researchers will search for digital traces of collaboration and communication within an organization at all levels to understand how information flows. The third focus area will be the study of adversary networks. This research has important implications for the Army in dealing with terrorists and other hidden groups within a society, according to Szymanski. The research will seek ways to monitor the activities of adversary networks, to map the composition and hierarchy of the network, and to understand their dynamics and evolution over time. The work will bring together expertise ranging from computer science to game theory."Adversary networks can be discovered very early in their development by careful social network analysis," Szymanski said. "Studying the technologies they use and how they use them will allow us to act well before the adversary network has reached maturity. This will greatly minimize their impact within their society as well as our own."A fourth focus examines trust in social networks. The researchers will seek to measure the level of trust within a network and understand how the impacts of trust move information through a network. For example, researchers will use mathematical and computational modeling to understand how different types of social interactions impact an individual's thoughts and behaviors. This work will also rely heavily on collaborations with computer scientists that will model and compute how actors in a social network build trust and use that trust to spread information. Finally, the center will look at the impacts of human error in social networks. This research will utilize computational systems that predict how human error or bias will influence their judgment. "As the diversification of nations and societies progresses, understanding of social and cognitive networks and their impacts on people's behavior and operation will become increasingly important," Szymanski said. "These networks impact the Army in all aspects of its operations, from internal cohesiveness to their ability to perform complex missions in increasingly complex international social environments. Equally important is that these networks impact our society in a very similar way, as the complexity of social interactions grows and the influence of other societies on our lives increases."
evelyn.brown@nist.govNIST, DOD, intelligence agencies join forces to secure US cyber infrastructureThe National Institute of Standards and Technology (NIST), in partnership with the Department of Defense (DOD), the Intelligence Community (IC), and the Committee on National Security Systems (CNSS), has released the first installment of a three-year effort to build a unified information security framework for the entire federal government. Historically, information systems at civilian agencies have operated under different security controls than military and intelligence information systems. This installment is titled NIST Special Publication 800-53, Revision 3, Recommended Security Controls for Federal Information Systems and Organizations.The common security control catalog is a critical step that effectively marshals our resources, says Ron Ross, NIST project leader for the joint task force. It also focuses our security initiatives to operate effectively in the face of changing threats and vulnerabilities. The unified framework standardizes the information security process that will also produce significant cost savings through standardized risk management policies, procedures, technologies, tools and techniques.This publication is a revised version of the security control catalog that was previously published in response to the Federal Information Security Management Act (FISMA) of 2002. This special publication contains the catalog of security controls and technical guidelines that federal agencies use to protect their information and technology infrastructure.When complete, the unified framework will result in the defense, intelligence and civil communities using a common strategy to protect critical federal information systems and associated infrastructure. This ongoing effort is consistent with President Obamas call for integrating all cybersecurity policies for the government in his May 29 speech on securing the U.S. cybersecurity infrastructure.The revised security control catalog in SP 800-53 provides the most state-of-the-practice set of safeguards and countermeasures for information systems ever developed. The updated security controlsmany addressing advanced cyber threatswere developed by a joint task force that included NIST, DOD, the IC and the CNSS with specific information from databases of known cyber attacks and threat information.Additional updates to key NIST publications that will serve the entire federal government are under way. These will include the newly revised SP 800-37, which will transform the current certification and accreditation process into a near real-time risk management process that focuses on monitoring the security state of federal information systems, and SP 800-39, which is an enterprise-wide risk management guideline that will expand the risk management process.NIST Special Publication 800-53, Revision 3, is open for public comment through July 1, 2009. The document is available online at http://csrc.nist.gov/publications/PubsDrafts.html#800-53_Rev3. Comments should be sent to sec-cert@nist.gov.
astark@osa.orgPowerful lasers, futuristic digital cameras, 3-D television and moreHighlights of Frontiers in Optics Meeting in San Jose, Oct. 11-15WASHINGTON, Oct. 1The latest technology in optics and lasers will be on display at the Optical Society's (OSA) Annual Meeting, Frontiers in Optics (FiO), which takes place Oct. 11-15 at the Fairmont San Jose Hotel and the Sainte Claire Hotel in San Jose, Calif.Information on free registration for reporters is contained at the end of this release. Research highlights of the meeting include:SPECIAL SYMPOSIUM: THE FUTURE OF 3-D TELEVISIONWith 3-D movies helping to drive record box office revenues this spring and companies like Sony and Panasonic rolling out the first 3-D-enabled televisions, a timely special symposium titled "The Future of 3-D Display: The Marketplace and the Technology" will feature presentations on current and future technologies driving the 3-D revolution.  Some highlights:The symposium is being organized by Hong Hua of the University of Arizona.  For more information on the special symposium, see: http://www.frontiersinoptics.com/ConferenceProgram/SpecialSymposium/default.aspx#Futureof3DDisplay. LASER FUSION AND EXAWATT LASERSIn the recent past, producing lasers with terawatt (a trillion watts) beams was impressive. Now petawatt (a thousand trillion watts, or 10^15 watts) lasers are the forefront of laser research.  Some labs are even undertaking work toward achieving exawatt (10^18 watts) levels. Todd Ditmire at the University of Texas currently produces petawatt power through a process of chirping, in which a short light pulse (150 femtoseconds in duration) is stretched out in time. This longer pulse is amplified to higher energy and then re-compressed to its shorter duration, thus providing a modest amount of energy, 190 joules in a very tiny bundle.  Ditmire claims that his petawatt device has the highest power of any laser system now operating, even the one at the National Ignition Facility at the Lawrence Livermore National Lab, owing to the very short pulse-compression he and his colleagues use.The main research use for the Texas Petawatt Laser, as it is called, has been to produce thermonuclear fusion; the laser light strikes a target where fusion of light nuclei occurs, releasing neutrons into the vicinity. These neutrons can themselves be used for doing research. The first results of this fusion experiment will be presented at this meeting. Other applications include the study of hot dense plasmas at pressures billions of time higher than atmospheric pressure and the creation of conditions for accelerating electrons to energies of billions of electron-volts.Another figure of merit for a laser, in addition to power, is power density. The Texas device is capable of producing power densities exceeding 10^21 watts per square centimeter. At this level many novel interactions might become possible.To get to exawatt powers, Ditmire hopes to combine largely-existing laser technology and his already-tested 100-femtosecond pulses with new laser glass materials that would allow amplification up to energies of 100 kilo-joules. Ditmire's current energy level, approximately 100 joules, is typical of laser labs at or near the petawatt level, such as those in Oxford, England, Osaka, Japan and Rochester, N.Y. With support from the government and the research community, building an exawatt laser might take 10 years to achieve, Ditmire estimates. (Paper FTuK2, "The Texas Petawatt Laser and Technology Development toward an Exawatt Laser" is at 11 a.m. Tuesday, Oct. 13). 1,001 CAMERAS SEE IN GIGAPIXELSAs manufacturers of consumer digital cameras compete in increments, adding one or two megapixels to their latest models, David Brady of Duke University is thinking much bigger. Working with the U.S. Department of Defense's Defense Advanced Research Projects Agency, he is designing and building a camera that could achieve resolutions 1,000 or even 1 million times greater than the technology on the market today.The goal of reaching giga- or terapixels, says Brady, is currently being held back by the difficulty of designing a spherical lens that will not distort small areas of a scene. His idea is not only to modify the shape of the camera lens -- making it aspherical -- but to link together thousands of microcameras behind the main lens. Each of these cameras would have its own lens optimized for a small portion of the field of view."Now, when you use a camera, you're looking through a narrow soda straw," says Brady. "These new cameras will be able to capture the full view of human vision."The final result of the three-year project should be a device about the size of a breadbox, though Brady hopes to scale the technology down to create a single-lens reflex camera with a resolution of 50 gigapixels. (Paper CWB2, "Multiscale Optical Systems" is at 2 p.m. Wednesday, Oct. 14).ALL THAT GLITTERS IS NOW GOLD In full sunlight at mid-day, gold objects are brilliant and richly colored. Put those same objects in a dark interior room with only fluorescent lamps, however, and they will look pale and slightly greenish -- a problem arising from the inability of fluorescent lamps to render the optimal color temperature to reveal gold in its warmest light. That's why museums and jewelry stores typically illuminate the gold objects in display cases with small incandescent bulbs, the only commercially-available lights that can emit soft yellow tones and warm color temperatures and render a true gold appearance. Incandescent bulbs are a poor choice for other reasons, however. They are notoriously hot and can alter the temperature and humidity in display cases, potentially damaging priceless museum pieces. Besides that, the European Union is phasing out the sale of incandescent bulbs starting this fall (a similar phase-out will go into effect in the United States beginning in 2012). Now Paul Michael Petersen and his colleagues at the Technical University of Denmark have designed an alternative, energy efficient and non-heating light source for gold objects. After they were contacted by curators at Rosenborg Castle in Copenhagen, which houses the Royal Danish Collection, Petersen and his colleagues created a novel LED designed specifically to illuminate gold. Combining commercially-available red, green, and blue LEDs with holographic diffusion, the new light can achieve a temperature and color rendering akin to incandescent bulbs -- with 70 percent energy savings and without emitting excess heat. They have been tested in a few display cases, says Petersen, and the lights will soon be installed throughout the museum.  (Paper JWC3, "A New LED Light Source for Display Cases" is at 12 p.m. Wednesday, Oct. 14).PREHISTORIC BEAR DIET REVEALED BY LASER ARCHAEOLOGYTwenty-six thousand years ago, a brown bear living in what is now the Czech Republic died, leaving behind a tooth that has since become a fossil. Now a team of engineers has developed a way to figure out not only what it ate but its migration patterns using a laser instrument that could be modified to take out into the field.The technique, called laser-induced breakdown spectroscopy (LIBS), is able to identify the chemical composition of a material -- such a tooth -- by penetrating miniscule samples with high-energy pulses of laser light. This laser turns each sample into plasma many times hotter than the surface of the sun. In this experiment, the light released as the plasma cooled revealed the composition of each part of the tooth. By checking the ratio of different elements in the root of the tooth, the team determined that the bear ate mostly plants during the hotter parts of the year. The changes in these ratios over time revealed the bear's migration patterns and a gradual shift in its living territory in one direction.It's a simple and fast technique, say the authors, with an unusually high resolution and the ability to scan a wide area of a sample. "The device could be modified to be taken out into the field," says Josef Kaiser of the Brno University of Technology in the Czech Republic.Next, the team hopes to use LIBS to solve the mystery of a cave full of dead snakes that died more than 1 million years ago -- possibly from a disease -- by analyzing the vertebrae left behind. (Paper JWC18, "Multielemental Mapping of Archaeological Samples by Laser-Induced Breakdown Spectroscopy (LIBS)" is at 12 p.m. Wednesday, Oct. 12).ILLUMINATION-AWARE IMAGINGConventional imaging systems incorporate a light source for illuminating an object and a separate sensing device for recording the light rays scattered by the object. By using lenses and software, the recorded information can be turned into a proper image. Human vision is an ordinary process: the use of two eyes (and a powerful brain that processes visual information) provides human observers with a sense of depth perception. But how does a video camera attached to a robot "see" in three dimensions? Carnegie Mellon scientist Srinivasa Narasimhan believes that efficiently producing 3-D images for computer vision can best be addressed by thinking of a light source and sensor device as being equivalent. That is, they are dual parts of a single vision process.For example, when a light illuminates a complicated subject, such as a fully-branching tree, many views of the object must be captured. This requires the camera to be moved, making it hard to find corresponding locations in different views.  In Narasimhan's approach, the camera and light constitute a single system. Since the light source can be moved without changing the corresponding points in the images, complex reconstruction problems can be solved easily for the first time. Another approach is to use a pixilated mask interposed at the light or camera to selectively remove certain light rays from the imaging process. With proper software, the resulting series of images can more efficiently render detailed 3-D vision information, especially when the object itself is moving.Narasimhan calls this process alternatively illumination-aware imaging or imaging-aware illumination.  He predicts it will be valuable for producing better robotic vision and rendering 3-D shapes in computer graphics. (Paper CtuD5, "Illuminating Cameras" is at 5:15 p.m. Tuesday, Oct. 13).ABOUT THE MEETINGFiO 2009 is OSA's 93rd Annual Meeting and is being held together with Laser Science XXV, the annual meeting of the American Physical Society (APS) Division of Laser Science (DLS). The two meetings unite the OSA and APS communities for five days of quality, cutting-edge presentations, fascinating invited speakers and a variety of special events spanning a broad range of topics in physics, biology and chemistry. The FiO 2009 conference will also offer a number of Short Courses designed to increase participants' knowledge of a specific subject while offering the experience of insightful teachers. An exhibit floor featuring leading optics companies will further enhance the meeting.Useful Links:EDITOR'S NOTE: A Press Room for credentialed press and analysts will be located in the Fairmont Hotel, Sunday through Thursday. Those interested in obtaining a press badge for FiO should contact OSA's Angela Stark, astark@osa.org or 202.416.1443.About OSAUniting more than 106,000 professionals from 134 countries, the Optical Society (OSA) brings together the global optics community through its programs and initiatives. Since 1916 OSA has worked to advance the common interests of the field, providing educational resources to the scientists, engineers and business leaders who work in the field by promoting the science of light and the advanced technologies made possible by optics and photonics. OSA publications, events, technical groups and programs foster optics knowledge and scientific collaboration among all those with an interest in optics and photonics. For more information, visit: www.osa.org. 
